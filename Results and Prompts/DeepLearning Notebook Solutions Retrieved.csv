competition_slug,competition_problem_type,competition_problem_subtype,competition_problem_description,competition_dataset_type,evaluation_metrics,dataset_metadata,target_column,preprocessing_steps,notebook_model_layers_code,used_technique,library,kernel_ref,kernel_link
playground-series-s4e8,classification,binary-classification,The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics.,Tabular,MCC – Matthews Correlation Coefficient,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the UCI Mushroom dataset. The feature distributions are similar but not identical to the original dataset. The dataset contains categorical values that may not be present in the original dataset, and competitors must decide how to handle these. The training dataset includes a binary target indicating whether the mushroom is edible or poisonous, while the test dataset requires predictions for the same target class.",['class'],"[""drop the 'id' column from both train and test datasets"", ""fill missing values in categorical features with 'Missing'"", ""apply count encoding to categorical features"", ""scale numerical features using standard scaling"", ""replace target values with numeric labels (0 for edible, 1 for poisonous)""]","ann.add(Dense(288, input_dim=input_dim, kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.4))
ann.add(Dense(80, kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.1))
ann.add(Dense(32, kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.0))
ann.add(Dense(num_classes, kernel_initializer='he_uniform', activation='sigmoid'))
ann.compile(loss='binary_crossentropy', optimizer=nadam, metrics=[matthews_correlation_coefficient])
model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=4, epochs=5, verbose=verbose)",DL,Tensorflow,arunklenin/ps4e8-binary-class-mathews-correlation-coeff,https://www.kaggle.com/arunklenin/ps4e8-binary-class-mathews-correlation-coeff
playground-series-s4e8,classification,binary-classification,The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics.,Tabular,MCC – Matthews Correlation Coefficient,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the UCI Mushroom dataset. The feature distributions are similar but not identical to the original dataset. Competitors can use the original dataset to explore differences and potentially improve model performance. The training dataset contains a binary target class indicating whether the mushroom is edible or poisonous, while the test dataset requires predictions for the target class. Data artifacts have not been cleaned, and there are categorical values present that are not found in the original dataset.",['class'],"[""filter out corrupted feature data"", ""apply one-hot encoding for categorical features"", ""handle missing data as a separate class"", ""convert numerical features to one-hot encoding based on specified boundaries""]","model = keras.Sequential()
model.add(keras.Input(shape=(X_train_np.shape[1],)))
model.add(Normalization(axis=-1))
model.add(Dense(units=64, activation=""relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=32, activation=""relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=16, activation=""relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=8, activation=""relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=4, activation=""relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=1, activation=tf.nn.sigmoid))

little_adam = tf.keras.optimizers.Adam(learning_rate=0.003687)
model.compile(optimizer=little_adam,
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[keras.metrics.BinaryAccuracy(),
                       mcc,
                       keras.metrics.FalsePositives(name='FP'),
                       keras.metrics.FalseNegatives(name='FN')])

model.fit(x=X_train_fold, y=y_train_fold, epochs=epoch_switch, batch_size=409, validation_data=(X_validation_fold, y_validation_fold))",DL,Tensorflow,fredericnicholson/mushroom-soup-with-polars-one-hot-encoding-and-nn,https://www.kaggle.com/fredericnicholson/mushroom-soup-with-polars-one-hot-encoding-and-nn
playground-series-s4e8,classification,binary-classification,The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics.,Tabular,MCC – Matthews Correlation Coefficient,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the UCI Mushroom dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target indicating whether a mushroom is edible or poisonous, while the test dataset is used to predict the target class for each observation. The dataset includes categorical values that may not be present in the original dataset, and competitors can choose how to handle these values.",['class'],"[""clean and categorize values"", ""handle low frequency and unexpected values"", ""encode categorical variables"", ""scale numerical features""]","model = Sequential([\n    Dense(128, activation = 'relu', input_shape = (X_train.shape[1],)),\n    Dropout(0.3),\n    Dense(64, activation = 'relu'),\n    Dropout(0.3),\n    Dense(32, activation = 'relu'),\n    Dropout(0.2),\n    Dense(16, activation = 'relu'),\n    Dropout(0.3),\n    Dense(4, activation = 'relu'),\n    Dropout(0.2),\n    Dense(1, activation = 'sigmoid')])\n\nmodel.compile(optimizer = Adam(learning_rate = 0.001),\n              loss = 'binary_crossentropy',\n              metrics = ['accuracy'])\n\nhistory = model.fit(\n    X_train_tensor, y_train_tensor,\n    validation_data = (X_val_tensor, y_val_tensor),\n    epochs = 30, batch_size = 32, verbose = 1,\n    callbacks = [early_stopping, lr_scheduler])",DL,Tensorflow,nastaranzandi/mushroom-neuralnetwork-dslanders,https://www.kaggle.com/nastaranzandi/mushroom-neuralnetwork-dslanders
playground-series-s4e8,classification,binary-classification,The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics.,Tabular,MCC – Matthews Correlation Coefficient,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the UCI Mushroom dataset. The feature distributions are similar but not identical to the original dataset. Competitors can use the original dataset to explore differences and potentially improve model performance. The training dataset contains a binary target class indicating whether a mushroom is edible or poisonous, while the test dataset requires predictions for the target class. The dataset includes categorical values that may not be found in the original dataset, and competitors must decide how to handle these artifacts.",['class'],"[""drop unnecessary columns"", ""fill missing values with mode"", ""scale numerical features using standard scaler"", ""one-hot encode categorical features"", ""label encode target variable""]","model = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(512, activation='linear'),
    BatchNormalization(),
    Dropout(0.3), 
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.3), 
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.2),  
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.2),  
    Dense(32, activation='relu'),
    BatchNormalization(),
    Dropout(0.1),  
    Dense(16, activation='relu'), 
    BatchNormalization(),
    Dropout(0.06),  
    Dense(8, activation='relu'), 
    BatchNormalization(),
    Dense(1, activation='sigmoid')
])

little_adam = tf.keras.optimizers.Adam(learning_rate=0.0009)
model.compile(loss='binary_crossentropy',optimizer=little_adam, metrics=['accuracy'])
history = model.fit(X_train, y_train,
                    epochs=100,
                    batch_size=128,
                    validation_data=(X_val, y_val),
                    callbacks=[early_stopping, reduce_lr])",DL,Tensorflow,danishyousuf19/using-neural-networks-for-classifying-mushrooms,https://www.kaggle.com/danishyousuf19/using-neural-networks-for-classifying-mushrooms
playground-series-s4e8,classification,binary-classification,The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics.,Tabular,MCC – Matthews Correlation Coefficient,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the UCI Mushroom dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target indicating whether the mushroom is edible or poisonous, while the test dataset is used to predict the target class. The dataset includes categorical values that may not be found in the original dataset, and competitors can choose how to handle these artifacts.",['class'],"[""drop duplicates from train and test datasets"", ""impute missing values in numerical columns using median"", ""impute missing values in categorical columns using most frequent value"", ""encode categorical features using label encoding"", ""split combined dataset back into training and test sets after encoding"", ""standardize features to have zero mean and unit variance""]","model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping])",DL,Tensorflow,manaswiyahello/s4e8-binary-prediction-of-poisonous-mushrooms,https://www.kaggle.com/manaswiyahello/s4e8-binary-prediction-of-poisonous-mushrooms
playground-series-s4e8,classification,binary-classification,The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics.,Tabular,MCC – Matthews Correlation Coefficient,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the UCI Mushroom dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target indicating whether a mushroom is edible or poisonous. The test dataset requires predictions for the target class. Data artifacts have not been cleaned, and there are categorical values present that are not found in the original dataset.",['class'],"[""drop the 'id' column from train and test datasets"", ""merge training dataset with original mushroom dataset"", ""drop duplicates from the training dataset"", ""encode the target class using label encoding"", ""fill missing values in categorical columns with 'missing'"", ""fill missing values in numerical columns with the mean"", ""apply ordinal encoding to categorical variables"", ""separate the combined dataset back into training and test datasets""]","model = CustomModel(numerical_cols, categorical_cols, input_shape)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
criterion = nn.BCELoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)
for epoch in range(40):
    model.train()
    for numerical, categorical, labels in train_loader:
        numerical = numerical.to(device)
        categorical = categorical.to(device)
        labels = labels.to(device).unsqueeze(1)
        optimizer.zero_grad()
        outputs = model(numerical, categorical)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()",DL,Pytorch,mobinapoulaei/nn-t8-dslanders,https://www.kaggle.com/mobinapoulaei/nn-t8-dslanders
playground-series-s4e8,classification,binary-classification,The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics.,Tabular,MCC – Matthews Correlation Coefficient,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the UCI Mushroom dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target indicating whether a mushroom is edible or poisonous. The test dataset requires predictions for the target class. The dataset includes categorical values that may not be found in the original dataset, and competitors must decide how to handle these artifacts.",['class'],"[""drop the 'id' column from train and test datasets"", ""merge training dataset with original mushroom dataset"", ""drop duplicates from the training dataset"", ""encode the target class using label encoding"", ""fill missing values in categorical columns with 'missing'"", ""fill missing values in numerical columns with the mean"", ""apply ordinal encoding to categorical variables"", ""separate the combined dataset back into training and test datasets""]","model = CustomModel(numerical_cols, categorical_cols, input_shape)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
criterion = nn.BCELoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)
for epoch in range(20):
    model.train()
    for numerical, categorical, labels in train_loader:
        numerical = numerical.to(device)
        categorical = categorical.to(device)
        labels = labels.to(device).unsqueeze(1)
        optimizer.zero_grad()
        outputs = model(numerical, categorical)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()",DL,Pytorch,mobinapoulaei/neural-network-with-embedding-layer,https://www.kaggle.com/mobinapoulaei/neural-network-with-embedding-layer
playground-series-s4e8,classification,binary-classification,The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics.,Tabular,MCC – Matthews Correlation Coefficient,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the UCI Mushroom dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target indicating whether a mushroom is edible or poisonous, while the test dataset requires predictions for the target class. The dataset includes categorical values that may not be present in the original dataset, and competitors must decide how to handle these artifacts.",['class'],"[""drop columns with excessive missing data"", ""impute missing values in categorical columns with mode"", ""impute missing values in numerical columns with mean or median"", ""map categorical values to known values and replace unknowns with 'unknown'"", ""scale numerical features using standard scaling""]","self.fc1 = nn.Linear(X_train.shape[1], 50)
self.dropout1 = nn.Dropout(0.7)
self.fc2 = nn.Linear(50, 40)
self.dropout2 = nn.Dropout(0.65)
self.fc3 = nn.Linear(40, 20)
self.dropout3 = nn.Dropout(0.6)
self.fc4 = nn.Linear(20, 10)
self.dropout4 = nn.Dropout(0.55)
self.fc5 = nn.Linear(10, 5)
self.dropout5 = nn.Dropout(0.5)
self.fc6 = nn.Linear(5, 1)
self.sigmoid = nn.Sigmoid()
criterion = nn.BCELoss()
optimizer = optim.Adam(net_1.parameters(), lr=0.001)
for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = net_1(batch_x).squeeze()
    loss = criterion(outputs, batch_y)
    loss.backward()
    optimizer.step()",DL,Pytorch,teranekerimova/nn-pytorch-mushrooms,https://www.kaggle.com/teranekerimova/nn-pytorch-mushrooms
playground-series-s4e8,classification,binary-classification,The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics.,Tabular,MCC – Matthews Correlation Coefficient,"The dataset for this competition includes both training and test data generated from a deep learning model trained on the UCI Mushroom dataset. The feature distributions are similar but not identical to the original dataset. Competitors can use the original dataset to explore differences and potentially improve model performance. The training dataset contains a binary target indicating whether a mushroom is edible or poisonous, while the test dataset requires predictions for the target class. The dataset includes categorical values that may not be found in the original dataset, and competitors must decide how to handle these artifacts.",['class'],"[""drop columns with mostly NaN values"", ""fill NaN values with the mode"", ""replace rare categories with 'rare'"", ""convert categorical features to integers""]","model = Deep().to(device)
model_train(model, X_train, y_train, X_test, y_test)",DL,Pytorch,mohammedragabsaad/playground-s4-e8-pytorch-98-accuracy-97-mcc,https://www.kaggle.com/mohammedragabsaad/playground-s4-e8-pytorch-98-accuracy-97-mcc
playground-series-s4e8,classification,binary-classification,The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics.,Tabular,MCC – Matthews Correlation Coefficient,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the UCI Mushroom dataset. The feature distributions are similar but not identical to the original dataset. Data artifacts have not been cleaned, and there are categorical values present that are not found in the original dataset. Participants can use the original dataset to explore differences and potentially improve model performance. The training dataset contains a binary target class indicating whether the mushroom is edible or poisonous, while the test dataset requires predictions for the target class.",['class'],"[""fill null values with 'NN' for categorical columns"", ""fill null values with mean for numerical columns"", ""replace target class values with integers"", ""cast categorical columns to enum type"", ""apply ordinal encoding to categorical columns"", ""apply robust scaling to numerical columns"", ""apply power transformation to numerical columns""]","model = BinaryClassificationModel(config={'lr': 1e-3,'layer_1_size': 128,'layer_1_dropout': 0.25,'layer_2_size': 32,'emb_root': 1})
trainer.fit(model, datamodule=datamodule)",DL,Pytorch,tomkrger/lightning-shrooms-on-ray,https://www.kaggle.com/tomkrger/lightning-shrooms-on-ray
playground-series-s4e7,classification,binary-classification,The objective of this competition is to predict which customers respond positively to an automobile insurance offer.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target column named Response, while the test dataset requires predicting the probability of this target. The dataset is in CSV format and has a size of 1.2 GB, licensed under Attribution 4.0 International (CC BY 4.0).",['Response'],"[""reduce memory usage"", ""ordinal encode categorical features""]","inputs = []
flat_embeddings = []
for feature, input_dim in input_shape.items():
    output_dim = min(64, round(1.6 * (input_dim + 1) ** 0.56))  
    input_layer = keras.Input(shape=(1,), name=feature)
    embedding_layer = layers.Embedding(input_dim=input_dim + 1, output_dim=output_dim)(input_layer)  
    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)
    embedding_layer = layers.Flatten()(embedding_layer)
    inputs.append(input_layer)
    flat_embeddings.append(embedding_layer)

numerical_input = keras.Input(shape=(len(numerical_features),), name='numerical')
inputs.append(numerical_input)
concatenated_inputs = layers.Concatenate()(flat_embeddings + [numerical_input])
concatenated_inputs_bn = layers.BatchNormalization()(concatenated_inputs)
x = layers.Dense(256, activation='mish')(concatenated_inputs_bn)
x = layers.BatchNormalization()(x)
x = layers.Concatenate()([x, concatenated_inputs_bn])
x = layers.Dense(128, activation='mish')(x)
x = layers.Dropout(0.3)(x)
x = layers.BatchNormalization()(x)
outputs = layers.Dense(1, activation='sigmoid')(x)
model = keras.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=[keras.metrics.AUC(name='auc')])
model.fit(X_train_inputs, y_train, epochs=4, batch_size=1024, validation_data=(X_val_inputs, y_val), verbose=0)",DL,Tensorflow,rzatemizel/stacking-xgb-lgbm-catb-ann,https://www.kaggle.com/rzatemizel/stacking-xgb-lgbm-catb-ann
playground-series-s4e7,classification,binary-classification,The objective of this competition is to predict which customers respond positively to an automobile insurance offer.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target column named Response, while the test dataset requires predicting the probability of this target for each row. The dataset is in CSV format and has a size of 1.2 GB, licensed under Attribution 4.0 International (CC BY 4.0).",['Response'],"[""use categorical data types instead of strings"", ""apply min-max scaling to Age, Annual_Premium, and Vintage"", ""perform one-hot encoding on categorical features"", ""shuffle the training data""]","model = keras.Sequential()
model.add(keras.Input(shape=(215,)))
model.add(Normalization(axis=-1))
model.add(Dense(units=64, activation=""leaky_relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=32, activation=""leaky_relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=16, activation=""leaky_relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=8, activation=""leaky_relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=16, activation=""leaky_relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=32, activation=""leaky_relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=64, activation=""leaky_relu""))
model.add(Normalization(axis=-1))
model.add(Dense(units=1, activation=tf.nn.sigmoid))

model.compile(optimizer=little_adam,
              loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2, alpha=0.5, apply_class_balancing=True),
              metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.AUC(num_thresholds=200, curve=""ROC"", summation_method=""interpolation"")])

history = model.fit(x=X_train, y=y_train, epochs=150, batch_size=1024, callbacks=[callback_1, callback_2, callback_0, callback_3], verbose=1, class_weight=class_weight_from_results, validation_data=(X_val, y_val))",DL,Tensorflow,fredericnicholson/polars-data-processing-nn,https://www.kaggle.com/fredericnicholson/polars-data-processing-nn
playground-series-s4e7,classification,binary-classification,The objective of this competition is to predict which customers respond positively to an automobile insurance offer.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target column named Response, while the test dataset requires predicting the probability of Response for each row. The dataset is in CSV format and has a size of 1.2 GB, licensed under Attribution 4.0 International (CC BY 4.0).",['Response'],"[""convert categorical features to ordinal encoding"", ""combine train and test datasets for preprocessing"", ""separate features into categorical and continuous groups""]","cat_inputs = [layers.Input(shape=(1,), name=f'cat{i}') for i in range(len(cat_features))]
cont_inputs = layers.Input(shape=(len(cont_features),))

flat_embeddings = []
for i, f in enumerate(cat_features):
    input_dim = int(cat_features_card[f])
    output_dim = int(min(64, round(1.6 * input_dim ** .56)))
    embedding = layers.Embedding(input_dim=input_dim, output_dim=output_dim)(cat_inputs[i])
    embedding = layers.SpatialDropout1D(.3)(embedding)
    flat_embeddings.append(layers.Flatten()(embedding))

concatenated_inputs = layers.Concatenate()(flat_embeddings + [cont_inputs, ])
concatenated_inputs_bn = layers.BatchNormalization()(concatenated_inputs)

x = layers.Dense(256, activation='mish')(concatenated_inputs_bn)
x = layers.BatchNormalization()(x)
for units in (128,):
    inp = layers.Concatenate()([x, concatenated_inputs_bn])
    x = layers.Dense(units=units, activation='mish')(inp)
    x = layers.Dropout(.3)(x)
    x = layers.BatchNormalization()(x)

outputs = layers.Dense(1, activation='sigmoid')(x)
model.compile(optimizer=keras.optimizers.AdamW(learning_rate=1E-4), loss='binary_crossentropy', metrics=['auc'])
model.fit(to_nn_feed(tr_and_or), tr_and_or[TARGET_NAME], validation_data=(to_nn_feed(vl), vl[TARGET_NAME]), batch_size=BS, epochs=epochs, callbacks=callbacks, verbose=0)",DL,Tensorflow,paddykb/ps-s4e7-keras-haz-insurance-losses,https://www.kaggle.com/paddykb/ps-s4e7-keras-haz-insurance-losses
playground-series-s4e7,classification,binary-classification,The objective of this competition is to predict which customers respond positively to an automobile insurance offer.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target column named Response, while the test dataset requires predicting the probability of this target for each row. The dataset is in CSV format and has a size of 1.2 GB, licensed under Attribution 4.0 International (CC BY 4.0).",['Response'],"[""transform categorical features"", ""adjust data types"", ""create additional features"", ""optimize memory usage"", ""apply scaling""]","scaled_model = Sequential()
scaled_model.add(Dense(input_dim = scaled_inputs, activation = 'relu', units = 128))
scaled_model.add(BatchNormalization())
scaled_model.add(Dense(activation = 'relu', units = 128))
scaled_model.add(BatchNormalization())
scaled_model.add(Dense(activation = 'relu', units = 64))
scaled_model.add(BatchNormalization())
scaled_model.add(Dense(activation = 'relu', units = 32))
scaled_model.add(BatchNormalization())
scaled_model.add(Dense(activation = 'sigmoid', units = 1))
scaled_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [AUC (name = 'auroc')])
history_scaled = scaled_model.fit(scaled_x_train, scaled_y_train, validation_data = (scaled_x_val, scaled_y_val), epochs = 100, callbacks = [early_stopping])",DL,Tensorflow,younusmohamed/18-18-hybrid-of-xgb-and-dnn,https://www.kaggle.com/younusmohamed/18-18-hybrid-of-xgb-and-dnn
playground-series-s4e7,classification,binary-classification,The objective of this competition is to predict which customers respond positively to an automobile insurance offer.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target column named Response, while the test dataset requires predicting the probability of this target for each row. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['Response'],"[""drop the 'id' column from train and test datasets"", ""split the training data into features and target"", ""apply one-hot encoding to categorical columns"", ""apply ordinal encoding to specific categorical columns"", ""scale the features using standard scaling""]","model = Sequential()
model.add(Dense(10 , activation = 'relu',input_dim = 10 ))
model.add(Dense(16 ,activation = 'relu'))
model.add(Dense(32,activation = 'relu'))
model.add(Dense(64 ,activation = 'relu'))
model.add(Dense(32 ,activation = 'relu'))
model.add(Dense(8 ,activation = 'relu'))
model.add(Dense(2 ,activation = 'relu'))
model.add(Dense(1 ,activation = 'sigmoid'))

model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics=['accuracy','auc'])

history = model.fit(X_train,y_train,validation_split = 0.2,epochs = 50,batch_size = 2048)",DL,Tensorflow,satyajeetrai/xgb-ann,https://www.kaggle.com/satyajeetrai/xgb-ann
playground-series-s4e7,classification,binary-classification,The objective of this competition is to predict which customers respond positively to an automobile insurance offer.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target column named Response, while the test dataset requires predicting the probability of this Response for each row. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['Response'],"[""convert categorical gender to binary values"", ""convert vehicle age categories to numerical values"", ""convert vehicle damage boolean to binary values"", ""drop the id column from features"", ""scale features using standard scaling"", ""split the data into training and testing sets""]","net_1 = NN()
criterion = nn.BCELoss()
optimizer = optim.Adam(net_1.parameters(), lr=0.001)
epochs = 6
batch_size = 1500
for epoch in range(epochs):
    permutation = torch.randperm(X_train_tensor.size()[0])
    for i in range(0, X_train_tensor.size()[0], batch_size):
        optimizer.zero_grad()
        indices = permutation[i:i+batch_size]
        batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]
        outputs = net_1(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')",DL,Pytorch,teranekerimova/nn-pytorch,https://www.kaggle.com/teranekerimova/nn-pytorch
playground-series-s4e7,classification,binary-classification,The objective is to predict which customers respond positively to an automobile insurance offer.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data. It includes features that are similar but not identical to the original dataset. The training dataset contains a binary target variable named Response, while the test dataset requires predicting the probability of this target. The dataset is in CSV format and has a size of 1.2 GB.",['Response'],"[""drop the id column"", ""impute missing values in numeric features with median"", ""scale numeric features using standard scaling"", ""impute missing values in categorical features with most frequent value"", ""one-hot encode categorical features"", ""combine preprocessed numeric and categorical features""]","model = KANModel(input_dim, hidden_dim, output_dim, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)",DL,Pytorch,bhaveshg20/insurance-cross-selling-via-kan,https://www.kaggle.com/bhaveshg20/insurance-cross-selling-via-kan
playground-series-s4e7,classification,binary-classification,The objective of this competition is to predict which customers respond positively to an automobile insurance offer.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target column named Response, while the test dataset requires predicting the probability of this target. The dataset files include train.csv, test.csv, and a sample_submission.csv file for submission formatting.",['Response'],"[""concat training data with external dataset"", ""one-hot encode categorical variables"", ""replace boolean values with integers"", ""rename columns for consistency""]","net = Classifier(X_train.shape[1])
net.to(device);
trainer(net, n_epochs, train_loader, val_loader, device)",DL,Pytorch,yichuanhuang/ps4e7-xgb-pytorch,https://www.kaggle.com/yichuanhuang/ps4e7-xgb-pytorch
playground-series-s4e7,classification,binary-classification,The objective is to predict which customers respond positively to an automobile insurance offer.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target column named 'Response', while the test dataset requires predicting the probability of this target. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['Response'],"[""label encode categorical features"", ""normalize features using StandardScaler""]","model = SimpleNN(input_size=features.shape[1], output_size=1).to(device)
loss_function = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)
epochs = 20
for epoch in range(epochs):
    for inputs, targets in dataloader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_function(outputs.squeeze(), targets)
        loss.backward()
        optimizer.step()",DL,Pytorch,cyberia/gpu-pytorch,https://www.kaggle.com/cyberia/gpu-pytorch
playground-series-s4e7,classification,binary-classification,The objective is to predict which customers respond positively to an automobile insurance offer.,Tabular,AUCROC,"The dataset for this competition includes both training and test sets generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target column named Response, while the test dataset requires predicting the probability of Response for each row. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['Response'],"[""drop unnecessary columns"", ""one-hot encode categorical variables"", ""scale numerical features""]","model = InsuranceNN(input_dim).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)",DL,Pytorch,syednazmussakib/neural-network-based-solution,https://www.kaggle.com/syednazmussakib/neural-network-based-solution
playground-series-s4e6,classification,multiclass-classification,The goal of this competition is to predict academic risk of students in higher education based on various features.,Tabular,Accuracy,"The dataset for this competition includes a training dataset with a categorical target variable indicating academic risk, and a test dataset for which predictions are to be made. The features in the dataset were generated from a deep learning model trained on a related dataset, and while the distributions are similar, they are not identical. The dataset is provided in CSV format and includes files for training, testing, and a sample submission.",['Target'],"[""min-max scale numerical features"", ""label encode categorical target""]","inputs = keras.Input(shape=(36,))
x = keras.layers.Dense(64, activation='leaky_relu')(inputs)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Dropout(0.2)(x)
concatenated = keras.layers.Concatenate()([x, inputs])
y = keras.layers.Dense(128, activation='leaky_relu')(concatenated)
y = keras.layers.BatchNormalization()(y)
y = keras.layers.Dropout(0.2)(y)
concatenated = keras.layers.Concatenate()([y, concatenated])
z = keras.layers.Dense(128, activation='leaky_relu')(concatenated)
z = keras.layers.BatchNormalization()(z)
z = keras.layers.Dropout(0.2)(z)
concatenated = keras.layers.Concatenate()([z, concatenated])
t = keras.layers.Dense(64, activation='leaky_relu')(concatenated)
t = keras.layers.BatchNormalization()(t)
t = keras.layers.Dropout(0.2)(t)
outputs = keras.layers.Dense(units=3, activation='softmax')(t)
model = keras.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
nn_classifier.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=1024, verbose=0, callbacks=[keras.callbacks.ReduceLROnPlateau, keras.callbacks.EarlyStopping])",DL,Tensorflow,rzatemizel/lgbm-catb-xgb-nn-voting-or-stacking,https://www.kaggle.com/rzatemizel/lgbm-catb-xgb-nn-voting-or-stacking
playground-series-s4e6,classification,multiclass-classification,"The goal is to predict the academic risk of students in higher education, classifying them into categories such as dropout, enrolled, or graduate.",Tabular,Accuracy,"The dataset consists of training and test data generated from a deep learning model trained on a dataset focused on predicting students' dropout and academic success. It includes features related to academic history, demographic details, and socio-economic factors. The training dataset contains a target variable that is categorical, while the test dataset is used to predict the target for each row. The dataset is in CSV format and has a size of 16.2 MB.",['Target'],"[""drop unnecessary columns"", ""fill missing values with median"", ""apply one-hot encoding for categorical variables"", ""scale numerical features using standard scaling""]","ann = Sequential()
ann.add(Dense(288, input_dim=input_dim, kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.4))
ann.add(Dense(80, kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.1))
ann.add(Dense(32, kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.0))
ann.add(Dense(num_classes, kernel_initializer='he_uniform', activation='softmax'))
ann.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])
",DL,Tensorflow,arunklenin/ps4e6-academic-success-classification-ensemble,https://www.kaggle.com/arunklenin/ps4e6-academic-success-classification-ensemble
playground-series-s4e6,classification,multiclass-classification,The goal of this competition is to predict academic risk of students in higher education based on various features.,Tabular,Accuracy,"The dataset for this competition includes both training and test sets generated from a deep learning model trained on a dataset focused on predicting students' dropout and academic success. The feature distributions are similar to the original dataset, which can also be used for exploration and model improvement. The training dataset contains a categorical target variable, while the test dataset is used to predict the target for each row. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['Target'],"[""median-impute missing values"", ""standardize numerical features"", ""encode categorical variables using one-hot encoding""]","model = tf.keras.Sequential([tf.keras.layers.Dense(128, activation='relu', input_shape=(input_shape,)),tf.keras.layers.Dense(64, activation='relu'),tf.keras.layers.Dense(num_classes, activation='softmax')])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))",DL,Tensorflow,arabidopsisthalian/ps4-e6-fttransformer,https://www.kaggle.com/arabidopsisthalian/ps4-e6-fttransformer
playground-series-s4e6,classification,multiclass-classification,"The goal of this competition is to predict the academic risk of students in higher education, categorizing them into classes such as Graduate, Dropout, or Enrolled.",Tabular,Accuracy,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a related dataset. The training dataset contains features and a categorical target variable indicating academic risk, while the test dataset is used to predict the target variable for each row. The dataset is provided in CSV format and is approximately 16.2 MB in size.",['Target'],"[""drop duplicates"", ""drop missing values"", ""encode categorical variables"", ""scale numerical features""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(3, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))",DL,Tensorflow,pratyushojha12/voting-classifier-of-xgb-catboost-and-naive-bias,https://www.kaggle.com/pratyushojha12/voting-classifier-of-xgb-catboost-and-naive-bias
playground-series-s4e6,classification,multiclass-classification,The goal is to predict the academic risk of students in higher education based on various features.,Tabular,Accuracy,"The dataset consists of training and test data generated from a deep learning model trained on a dataset focused on predicting students' dropout and academic success. It includes features related to tuition fees and curricular units, with the target being a categorical assessment of academic risk. The training dataset contains the target variable, while the test dataset is used to predict the target for each row. The dataset is in CSV format and is approximately 16.2 MB in size.",['target'],"[""rename columns for consistency"", ""create prompt column for model input""]","gemma_lm.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=keras.optimizers.Adam(learning_rate=8e-5),
    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
)
gemma_lm.fit(data, epochs=CFG.epochs, batch_size=CFG.batch_size)",DL,Tensorflow,mpwolke/gemmakeras-on-academic-success,https://www.kaggle.com/mpwolke/gemmakeras-on-academic-success
playground-series-s4e6,classification,multiclass-classification,The goal of this competition is to predict academic risk of students in higher education based on various features.,Tabular,Accuracy,"The dataset for this competition includes both training and test sets generated from a deep learning model trained on a dataset focused on predicting students' dropout and academic success. The feature distributions are similar to the original dataset, which can also be used for exploration and model improvement. The training dataset contains a categorical target variable, while the test dataset is used to predict the target for each row. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['Target'],"[""oversample using SMOTE"", ""apply robust scaling and PCA to specific columns"", ""standard scale other numeric columns"", ""label encode the target variable""]","model = MultiClassNN(INPUT_SIZE, CLASSES).to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
history = {""train_loss"": [], ""val_loss"": [], ""train_accuracy"": [], ""val_accuracy"": []}
for epoch in range(EPOCHS):
    model.train()
    running_loss = 0.0
    running_accuracy = 0.0
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        running_accuracy += accuracy(outputs, targets)
    running_loss /= len(train_loader)
    running_accuracy /= len(train_loader)
history[""train_loss""].append(running_loss)
history[""train_accuracy""].append(running_accuracy)
model.eval()
    running_loss = 0.0
    running_accuracy = 0.0
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item()
            running_accuracy += accuracy(outputs, targets)
    running_loss /= len(val_loader)
    running_accuracy /= len(val_loader)
history[""val_loss""].append(running_loss)
history[""val_accuracy""].append(running_accuracy)",DL,Pytorch,atharva0577/s4e6-pytorch-xgb-lgbm-optuna,https://www.kaggle.com/atharva0577/s4e6-pytorch-xgb-lgbm-optuna
playground-series-s4e6,classification,multiclass-classification,The goal of this competition is to predict academic risk of students in higher education based on various features.,Tabular,Accuracy,"The dataset for this competition includes a training dataset with a categorical target indicating academic risk and a test dataset for which predictions are to be made. The features in the dataset were generated from a deep learning model trained on a related dataset, and the distributions of these features are similar but not identical to the original dataset. The training dataset is provided in a CSV format, along with a sample submission file.",['Target'],"[""remove unnecessary fields"", ""apply z-score normalization to features"", ""convert categorical target to numerical labels""]","self.input = nn.Linear(featuresT.shape[1], 512)
self.fc1 = nn.Linear(512, 512)
self.bn1 = nn.BatchNorm1d(512)
self.fc2 = nn.Linear(512, 512)
self.bn2 = nn.BatchNorm1d(512)
self.output = nn.Linear(512, 3)
lossfun = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr = .0001, weight_decay = .001)",DL,Pytorch,manas28/academic-success-classification-pytorch-ann,https://www.kaggle.com/manas28/academic-success-classification-pytorch-ann
playground-series-s4e6,classification,multiclass-classification,The goal of this competition is to predict the academic risk of students in higher education based on provided features.,Tabular,Accuracy,"The dataset for this competition includes a training set and a test set generated from a deep learning model trained on a related dataset. The training dataset contains features and a categorical target indicating academic risk, while the test dataset is used to predict the target class for each row. The dataset is in CSV format and is approximately 16.2 MB in size.",['Target'],"[""label-encode categorical target variable"", ""standardize feature values"", ""split dataset into training and testing sets""]","model_v1 = ClassificationModel()
model_v1.to(device)
loss_fn = torch.nn.CrossEntropyLoss()
optimizer =torch.optim.SGD(model_v1.parameters(), lr=0.01)
train_model(3501, model_v1, loss_fn, optimizer, metric)",DL,Pytorch,burakarslan1/pytorch-classification,https://www.kaggle.com/burakarslan1/pytorch-classification
playground-series-s4e6,classification,multiclass-classification,The goal is to predict the academic risk of students in higher education based on provided features.,Tabular,Accuracy,"The dataset consists of training and test data generated from a deep learning model trained on a dataset related to predicting students' dropout and academic success. The feature distributions are similar but not identical to the original dataset. The training dataset includes a categorical target variable, while the test dataset is used to predict the target for each row. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['Target'],"[""drop the 'id' column"", ""apply log transformation to 'Nacionality'"", ""remove features with high multicollinearity based on correlation analysis""]","tabnet_classier = TabNetClassifier()
tabnet_classier.fit(X_train.values, y_train,max_epochs=1,patience=10,batch_size=1024,virtual_batch_size=128,num_workers=1,drop_last=False)",DL,Pytorch,grooper/tabnet-eda-try-nn-algorithm-for-table-data,https://www.kaggle.com/grooper/tabnet-eda-try-nn-algorithm-for-table-data
playground-series-s4e6,classification,multiclass-classification,The goal of this competition is to predict academic risk of students in higher education based on provided features.,Tabular,Accuracy,"The dataset for this competition includes a training dataset with a categorical target variable indicating academic risk, and a test dataset for which predictions are to be made. The features in the dataset were generated from a deep learning model trained on a related dataset, and the distributions of these features are similar but not identical to the original dataset. The training dataset is provided in a CSV format, along with a sample submission file to guide the format of predictions.",['Target'],"[""label encode categorical target variable"", ""drop id column"", ""split data into training and testing sets"", ""standardize features using z-normalization"", ""convert data to PyTorch tensors"", ""create TensorDataset for training and testing"", ""generate DataLoader for training and testing batches""]","model = Model()
loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
model.train()
for X,y in X_train_loader:
  y_pred = model(X)
  loss = loss_function(y_pred,y)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()",DL,Pytorch,harshitarya003/academic-success-deep-learning-easy,https://www.kaggle.com/harshitarya003/academic-success-deep-learning-easy
playground-series-s4e5,regression,continuous-regression,The goal is to predict the probability of a region flooding based on various environmental and infrastructural factors.,Tabular,R2 – Coefficient of Determination,"The dataset for this competition includes training and test data generated from a deep learning model trained on the Flood Prediction Factors dataset. It contains various features related to flood risk, such as monsoon intensity, urbanization, and climate change, with the target variable being FloodProbability. The dataset is well-suited for visualizations and exploratory data analysis, providing insights into the factors influencing flood occurrences.",['FloodProbability'],"[""drop unnecessary columns"", ""scale numerical features using standardization"", ""split data into training and testing sets""]","model = Sequential()
model.add(Dense(256, activation='relu', input_shape=input_shape))
model.add(Dropout(rate=0.2))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=32, verbose=1, callbacks=[model_checkpoint, early_stopping])",DL,Tensorflow,satyaprakashshukl/neural-network-prediction-analysis,https://www.kaggle.com/satyaprakashshukl/neural-network-prediction-analysis
playground-series-s4e5,regression,continuous-regression,The goal is to predict the probability of a region flooding based on various environmental and geographical factors.,Tabular,R2 – Coefficient of Determination,"The dataset consists of training and test data generated from a deep learning model trained on the Flood Prediction Factors dataset. It includes various features related to environmental conditions and flood risks. The training dataset contains 1,117,957 records and 22 columns, while the test dataset has 745,305 records and 21 columns. The target variable is FloodProbability, which represents the likelihood of flooding in a given area.",['FloodProbability'],"[""drop irrelevant columns"", ""concatenate additional datasets"", ""check for duplicates"", ""compute missing values"", ""scale features using MinMaxScaler"", ""split data into training and test sets""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_dim,)))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(x_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(x_test_scaled, y_test))",DL,Tensorflow,getanmolgupta01/floodprobability-xgb-lgbm-catboost-ensemble,https://www.kaggle.com/getanmolgupta01/floodprobability-xgb-lgbm-catboost-ensemble
playground-series-s4e5,regression,continuous-regression,The goal is to predict the probability of a region flooding based on various factors.,Tabular,R2 – Coefficient of Determination,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Flood Prediction Factors dataset. The feature distributions are similar to the original dataset. The training dataset contains a target column named FloodProbability, while the test dataset is used to predict this target. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['FloodProbability'],"[""standardize feature values""]","model = Sequential()
model.add(Dense(24,activation='relu',input_dim=21))
model.add(Dropout(0.2))
model.add(Dense(16,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10,activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(5,activation='relu'))
model.add(Dense(3,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])
history = model.fit(X_train_s,y_train,epochs=10,validation_split=0.2, callbacks=callback)",DL,Tensorflow,mpwolke/flooding-rivers-through-my-kaggler-mind,https://www.kaggle.com/mpwolke/flooding-rivers-through-my-kaggler-mind
playground-series-s4e5,regression,continuous-regression,The goal of this competition is to predict the probability of a region flooding based on various factors.,Tabular,R2 – Coefficient of Determination,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Flood Prediction Factors dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains the target variable FloodProbability, while the test dataset requires predictions for this target. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['FloodProbability'],"[""standardize numerical features"", ""scale features using MinMaxScaler""]","inputs = keras.Input(shape=(29,))
x = keras.layers.Dense(64, activation='relu')(inputs)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Dropout(0.1)(x)
concatenated = keras.layers.Concatenate()([x, inputs])
y = keras.layers.Dense(units=128, activation=""relu"")(concatenated)
y = keras.layers.BatchNormalization()(y)
y = keras.layers.Dropout(0.1)(y)
concatenated = keras.layers.Concatenate()([y, concatenated])
z = keras.layers.Dense(units=128, activation=""relu"")(concatenated)
z = keras.layers.BatchNormalization()(z)
z = keras.layers.Dropout(0.1)(z)
concatenated = keras.layers.Concatenate()([ z, concatenated])
t = keras.layers.Dense(units=64, activation=""relu"")(concatenated)
t = keras.layers.BatchNormalization()(t)
t = keras.layers.Dropout(0.1)(t)
outputs = keras.layers.Dense(units=1)(t)",DL,Tensorflow,rzatemizel/ensemble-ann-boosters-linear-model,https://www.kaggle.com/rzatemizel/ensemble-ann-boosters-linear-model
playground-series-s4e5,regression,continuous-regression,The goal of this competition is to predict the probability of a region flooding based on various factors.,Tabular,R2 – Coefficient of Determination,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Flood Prediction Factors dataset. The feature distributions are similar to the original dataset. The training dataset contains the target variable FloodProbability, which represents the probability of flooding in the region, ranging from 0 to 1. The test dataset is used to predict FloodProbability for each row. The dataset is well-suited for visualizations, clustering, and exploratory data analysis.",['FloodProbability'],"[""drop unnecessary columns"", ""apply feature engineering"", ""scale features using robust scaler""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_dim,)))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)",DL,Tensorflow,yorkyong/regression-flood-prediction-ensemble,https://www.kaggle.com/yorkyong/regression-flood-prediction-ensemble
playground-series-s4e5,regression,continuous-regression,The goal of this competition is to predict the probability of a region flooding based on various factors.,Tabular,R2 – Coefficient of Determination,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Flood Prediction Factors dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains the target variable FloodProbability, while the test dataset requires predictions for this variable. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['FloodProbability'],"[""standardize features using StandardScaler"", ""generate polynomial and interaction features""]","model = DeepNeuralNetwork(X_train.shape[1])
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

model.fit(X_train, y_train)",DL,Pytorch,liaoguoying/ml-dl,https://www.kaggle.com/liaoguoying/ml-dl
playground-series-s4e5,regression,continuous-regression,The goal is to predict the probability of a region flooding based on various factors.,Tabular,R2 – Coefficient of Determination,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Flood Prediction Factors dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains the target variable FloodProbability, while the test dataset requires predictions for this variable. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['FloodProbability'],"[""create new features by summing, multiplying, finding min/max, variance, and skewness of columns"", ""standardize features using StandardScaler"", ""convert features and target to PyTorch tensors""]","model = MLP(X.shape[1])
criterion = nn.MSELoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
for epoch in range(100000):
    model.train()
    optimizer.zero_grad()
    outputs = model(X)
    loss = criterion(outputs.squeeze(), y)
    loss.backward()
    optimizer.step()",DL,Pytorch,rmehedi/ps4e5-flood-prediction-eda-mlp-pytorch,https://www.kaggle.com/rmehedi/ps4e5-flood-prediction-eda-mlp-pytorch
playground-series-s4e5,regression,continuous-regression,The goal is to predict the probability of a region flooding based on various factors.,Tabular,R2 – Coefficient of Determination,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Flood Prediction Factors dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains the target variable FloodProbability, while the test dataset requires predictions for this variable. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['FloodProbability'],"[""drop the 'id' column"", ""standardize the features using StandardScaler"", ""split the data into training and testing sets""]","self.lin1 = nn.Linear(size_size, size_hidden1)
self.act1 = nn.ReLU()
self.lin2 = nn.Linear(size_hidden1, size_hidden2)
self.act2 = nn.ReLU()
self.lin3 = nn.Linear(size_hidden2, size_hidden3)
self.act3 = nn.ReLU()
self.lin4 = nn.Linear(size_hidden3, size_hidden4)
self.act4 = nn.ReLU()
self.lin5 = nn.Linear(size_hidden4, size_hidden5)

model.compile(optimizer='adam', loss='mse')
trainer.fit(model, train_iter, val_iter)",DL,Pytorch,skytao1314/prediction-by-pytorch,https://www.kaggle.com/skytao1314/prediction-by-pytorch
playground-series-s4e4,regression,continuous-regression,"The goal of this competition is to predict the age of abalone from various physical measurements, specifically the number of rings on their shells, which correlates with their age.",Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset for this competition includes a training set and a test set generated from a deep learning model trained on the Abalone dataset. The training dataset contains various physical measurements of abalones, and the target variable is the number of rings, which indicates their age. The test dataset is used to predict the target variable for each row. The dataset is provided in CSV format and is approximately 8.4 MB in size.",['Rings'],"[""drop unnecessary columns"", ""rename columns for consistency"", ""concatenate original dataset with training data"", ""check for missing values"", ""apply min-max scaling to continuous features"", ""create new features based on existing measurements"", ""apply log transformation to the target variable""]","ann = Sequential()
ann.add(Dense(64, input_dim=X_test.shape[1], kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.1))
ann.add(Dense(16,  kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.1))
ann.add(Dense(4,  kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.1))
ann.add(Dense(1,  kernel_initializer='he_uniform'))
ann.compile(loss=root_mean_squared_log_error, optimizer=adamW,metrics=['mse'])",DL,Tensorflow,arunklenin/ps4e4-abalone-age-prediction-regression,https://www.kaggle.com/arunklenin/ps4e4-abalone-age-prediction-regression
playground-series-s4e4,regression,continuous-regression,"The goal of this competition is to predict the age of abalone from various physical measurements, specifically estimating the number of rings on the abalone shell.",Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset for this competition includes training and test datasets generated from a deep learning model trained on the Abalone dataset. It contains various physical measurements of abalones, such as length, diameter, height, and weight, along with the target variable Rings, which represents the age of the abalone. The training dataset is used to build the model, while the test dataset is used for evaluation. The dataset is in CSV format and has a size of 8.4 MB.",['Rings'],"[""drop the 'id' column from both train and test datasets"", ""encode the 'Sex' categorical variable using label encoding"", ""split the dataset into training and testing sets"", ""apply mean imputation for missing values"", ""scale numerical features using standard scaling""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(rate=0.2))
model.add(Dense(64, activation='relu'))
model.add(Dense(1))
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mse')
simple_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)",DL,Tensorflow,satyaprakashshukl/neural-network-optimization-analysis,https://www.kaggle.com/satyaprakashshukl/neural-network-optimization-analysis
playground-series-s4e4,regression,continuous-regression,The goal of this competition is to predict the age of abalone from various physical measurements.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset for this competition includes a training dataset with 90615 records and 10 columns, where the target variable is the number of rings on the shell of the abalone, which serves as a proxy for its age. The test dataset contains 60411 records and no columns. The dataset was generated from a deep learning model trained on the Abalone dataset, with feature distributions that are close to but not exactly the same as the original.",['Rings'],"[""drop unnecessary columns"", ""log-transform skewed features"", ""map categorical variables to numerical values"", ""scale features using MinMaxScaler"", ""split data into training and testing sets""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_logarithmic_error')
model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2)",DL,Tensorflow,getanmolgupta01/abalone-age-xgb-gb-mlp-lgbm-ensemble,https://www.kaggle.com/getanmolgupta01/abalone-age-xgb-gb-mlp-lgbm-ensemble
playground-series-s4e4,regression,continuous-regression,The goal of this competition is to predict the age of abalone from various physical measurements.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset for this competition includes training and test data generated from a deep learning model trained on the Abalone dataset. It contains features related to the physical measurements of abalone, such as length, diameter, height, and weights, with the target variable being the number of rings, which correlates with the age of the abalone. The training dataset is provided in train.csv, while the test dataset is in test.csv, and a sample submission format is included in sample_submission.csv.",['Rings'],"[""remove id column"", ""replace categorical values in Sex column with numeric values"", ""standardize features by subtracting the mean and dividing by the standard deviation"", ""split the dataset into training and validation sets""]","abalone_model = tf.keras.Sequential([layers.Dense(64), layers.Dense(1)])
abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(), optimizer = tf.keras.optimizers.Adam())
abalone_model.fit(X_train, y_train, epochs=30)",DL,Tensorflow,javedali92/regression-for-abalone-age,https://www.kaggle.com/javedali92/regression-for-abalone-age
playground-series-s4e4,regression,continuous-regression,The goal is to predict the age of abalone based on various physical measurements.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset for this competition includes a training set and a test set generated from a deep learning model trained on the Abalone dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains the target variable Rings, which is an integer representing the age of the abalone. The test dataset requires predictions for the Rings variable. A sample submission file is also provided in the correct format.",['Rings'],"[""one-hot encode categorical variables""]","model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.2),
    BatchNormalization(),
    Dense(256, activation='relu'),
    Dropout(0.3),
    BatchNormalization(),
    Dense(256, activation='relu'),
    Dropout(0.3),
    BatchNormalization(),
    Dense(128, activation='relu'),
    Dropout(0.2),
    BatchNormalization(),
    Dense(1) 
])

model.compile(optimizer='adam',
              loss='mean_squared_error', 
              metrics=['mean_squared_error'])

history = model.fit(X_trainable, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid),callbacks=[lr_scheduler])",DL,Tensorflow,maheshmani13/neural-net-regression,https://www.kaggle.com/maheshmani13/neural-net-regression
playground-series-s4e4,regression,continuous-regression,The goal of this competition is to predict the age of abalone from various physical measurements.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset for this competition includes training and test data generated from a deep learning model trained on the Abalone dataset. The training dataset contains features and the target variable, Rings, which is the integer representing the age of the abalone. The test dataset is used to predict the Rings value for each row. The dataset is in CSV format and has a size of 8.4 MB, with an Attribution 4.0 International (CC BY 4.0) license.",['Rings'],"[""convert 'Sex' column to numerical values"", ""scale the features using MinMaxScaler"", ""apply log transformation to the target variable""]","model = NODEModel(input_dim, num_trees, depth)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
model.fit(train_loader)",DL,Pytorch,trupologhelper/neural-networks-for-tabular-data,https://www.kaggle.com/trupologhelper/neural-networks-for-tabular-data
playground-series-s4e4,regression,continuous-regression,The goal of this competition is to predict the age of abalone from various physical measurements.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset for this competition includes a training dataset and a test dataset generated from a deep learning model trained on the Abalone dataset. The feature distributions are similar to the original dataset. The training dataset contains the target variable Rings, which is an integer representing the age of the abalone. The test dataset requires predictions for the Rings variable. The dataset is provided in CSV format and is approximately 8.4 MB in size.",['Rings'],"[""set index to 'id'"", ""one-hot encode 'Sex' feature"", ""drop 'Sex' from original data"", ""normalize features using StandardScaler""]","model = MyRegressor()
opt = torch.optim.Adam(model.parameters(), lr)
model.train()
for data in data_loader:
    output = model(data[0])
    loss = mse(output, data[1].unsqueeze(1))
    loss.backward()
    opt.step()",DL,Pytorch,sachinsd/learn-pytorch-regression-bonus-xgbregressor,https://www.kaggle.com/sachinsd/learn-pytorch-regression-bonus-xgbregressor
playground-series-s4e4,regression,continuous-regression,The goal is to predict the age of abalone based on various physical measurements.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and test data generated from a deep learning model trained on the Abalone dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes an integer target column named Rings, while the test dataset requires predictions for this target. A sample submission file is also provided in the correct format.",['Rings'],"[""map categorical values to integers""]","model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=N_CLASSES)
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=LEARNING_RATE)
train_network(model, optimizer, criterion, X_train, y_train, X_test, y_test, EPOCHS, train_losses, test_losses)",DL,Pytorch,risakashiwabara/pytorch,https://www.kaggle.com/risakashiwabara/pytorch
playground-series-s4e4,regression,continuous-regression,The goal of this competition is to predict the age of abalone from various physical measurements.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset for this competition includes a training dataset and a test dataset generated from a deep learning model trained on the Abalone dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains the target variable Rings, which is an integer representing the age of the abalone. The test dataset requires predictions for the Rings variable. A sample submission file is also provided in the correct format.",['Rings'],"[""one-hot encode categorical variable 'Sex'"", ""drop 'id' column from training and test datasets"", ""scale continuous features using StandardScaler"", ""split training data into training and validation sets""]","clf = TabNetRegressor(n_d=8,n_a=8,cat_idxs=[7,8,9],cat_dims=[2,2,2])
clf.fit(
  X_train, Y_train,
  eval_set=[(X_valid, Y_valid)],
  eval_metric=['rmsle']
)",DL,Pytorch,shwetalishimangaud/pytorch-with-tabnetregressor-0-15,https://www.kaggle.com/shwetalishimangaud/pytorch-with-tabnetregressor-0-15
playground-series-s4e4,regression,continuous-regression,The goal is to predict the age of abalone based on various physical measurements.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and test data generated from a deep learning model trained on the Abalone dataset. It includes features such as sex, length, diameter, height, and various weight measurements. The target variable is the integer Rings, which represents the age of the abalone. The training dataset is provided in train.csv, while the test dataset is in test.csv, with a sample submission format in sample_submission.csv.",['Rings'],"[""remove records with height equal to zero"", ""label encode categorical variable 'Sex' to float"", ""normalize numerical features using StandardScaler""]","self.model = smp.Unet(encoder_name=""mit_b1"", in_channels=3, classes=1, decoder_attention_type='scse')
self.regressor = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(1, 1))
trainer.fit(model, train_loader, val_loader)",DL,Pytorch,liaoguoying/unethology-predict-age-with-transunet,https://www.kaggle.com/liaoguoying/unethology-predict-age-with-transunet
playground-series-s4e3,classification,multi-label-classification,Predict the probability of various defects on steel plates across multiple categories.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. The training dataset contains 19219 records and 35 columns, while the test dataset has 12814 records and 28 columns. The target variables include seven binary categories representing different types of defects: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults. The dataset is provided in CSV format and is approximately 5.49 MB in size.","['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']","[""drop irrelevant columns"", ""log-transform skewed features"", ""scale features using robust scaler"", ""encode categorical target variable using label encoding""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_shape,)))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(7, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])
model.fit(x_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(x_test_scaled, y_test))",DL,Tensorflow,getanmolgupta01/defect-pred-eda-xgboost-lgbm-catboost,https://www.kaggle.com/getanmolgupta01/defect-pred-eda-xgboost-lgbm-catboost
playground-series-s4e3,classification,multi-label-classification,Predict the probability of various defects on steel plates across multiple categories.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. The training dataset contains features and seven binary target columns representing different defect categories: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults. The test dataset is used to predict the probabilities for these defect categories. The dataset is in CSV format and has a size of 5.49 MB.","['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']","[""handle missing values using mean imputation"", ""scale features using standard scaling"", ""reduce dimensionality while preserving variance"", ""select top features based on statistical tests"", ""oversample minority classes to handle imbalance"", ""split dataset into training and validation sets""]","tf.keras.Sequential([tf.keras.layers.Dense(10, activation='relu'),tf.keras.layers.Dense(2, activation='softmax')]).compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']).fit(X_train_scaled_dummy, y_train_one_hot, epochs=50, batch_size=32)",DL,Tensorflow,josiagiven/plate-defect-detection,https://www.kaggle.com/josiagiven/plate-defect-detection
playground-series-s4e3,classification,multi-label-classification,Predict the probability of various defects on steel plates across seven categories.,Tabular,AUCROC,"The dataset for this competition includes a training set and a test set generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. It contains features that are similar but not identical to the original dataset. The training dataset has seven binary targets: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults. The test dataset is used to predict the probabilities of these seven defects. The dataset is in CSV format and is approximately 5.49 MB in size.","['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']","[""load the dataset"", ""convert categorical variables to numerical"", ""normalize feature values""]","model = tfdf.keras.GradientBoostedTreesModel(
    multitask=[tfdf.keras.MultiTaskItem(label=l, task=tfdf.keras.Task.CLASSIFICATION) for l in primary_labels],
    verbose=1,
)
model.fit(train_tf)",DL,Tensorflow,mpwolke/seven-targets-tf-decision-forests,https://www.kaggle.com/mpwolke/seven-targets-tf-decision-forests
playground-series-s4e3,classification,multi-label-classification,Predict the probability of various defects on steel plates across seven categories.,Tabular,AUCROC,"The dataset for this competition includes training and test data generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. It contains seven binary targets: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults. The training dataset is provided in train.csv, while the test dataset is in test.csv. A sample submission file is also included to guide the format of predictions.","['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']","[""drop unnecessary columns"", ""handle missing values"", ""scale numerical features"", ""drop duplicates""]","model = Model(X_train_,X_test)
scores,preds_1 = model.fit(params_1)
model = Model(X_train_,X_test)
scores,preds_2 = model.fit(params_2)
model = Model(X_train_,X_test)
scores,preds_3 = model.fit(params_3)
model = Model(X_train_,X_test)
scores,preds_4 = model.fit(params_4)",DL,Tensorflow,satyajeetbedi/easy-steel-plate-defect-prediction,https://www.kaggle.com/satyajeetbedi/easy-steel-plate-defect-prediction
playground-series-s4e3,classification,multi-label-classification,Predict the probability of various defects on steel plates across multiple categories.,Tabular,AUCROC,"The dataset for this competition includes training and test data generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. It contains 7 binary targets representing different defect categories: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults. The training dataset is provided in train.csv, while the test dataset is in test.csv. A sample submission file is also included to demonstrate the required format for predictions.","['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']","[""set index to id"", ""convert DataFrame to TensorFlow dataset""]","model = tfdf.keras.RandomForestModel(
    multitask=[tfdf.keras.MultiTaskItem(label=l, task=tfdf.keras.Task.CLASSIFICATION) for l in target_labels],
    verbose=1,
)
model.fit(train_tf)",DL,Tensorflow,aditya1220/tf-decisionforest-modelcomparison,https://www.kaggle.com/aditya1220/tf-decisionforest-modelcomparison
playground-series-s4e3,classification,multi-label-classification,Predict the probability of various defects on steel plates across multiple categories.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. It contains features related to steel plate characteristics and seven binary targets representing different defect categories: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults. The training dataset is used to train the model, while the test dataset is used to evaluate the model's performance by predicting the probabilities of each defect category.","['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']","[""standardize features using StandardScaler"", ""apply custom preprocessing function to create new features"", ""remove irrelevant features from the dataset""]","model = MLP(len(selected_features), hidden_size1, hidden_size2, output_size)
criterion = CustomLoss(feature_num[target])
optimizer = optim.Adam(model.parameters())

for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = model(x_train_tensor)
    loss = criterion(outputs[:, feature_num[target]], y_train_tensor[:, feature_num[target]])
    loss.backward()
    optimizer.step()",DL,Pytorch,ddxclbri/pgs4e3-nn-pytorch-loss-and-feature,https://www.kaggle.com/ddxclbri/pgs4e3-nn-pytorch-loss-and-feature
playground-series-s4e3,classification,multi-label-classification,Predict the probability of various defects on steel plates across multiple categories.,Tabular,AUCROC,"The dataset for this competition includes a training set with features derived from a deep learning model trained on the Steel Plates Faults dataset from UCI. It contains 7 binary target columns representing different defect categories: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults. The test set is used to predict the probabilities for these defects. The dataset is provided in CSV format, with a total size of 5.49 MB.","['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']","[""standardize features using StandardScaler"", ""feature engineering to create new features""]","model = TabularNetDeep(p=0.3, input_shape=48, output_shape=1)
optimizer = optim.Adam(model.parameters(), lr=1.01e-2)
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)
loss_fn = nn.BCELoss(reduction='mean')
AnuwazNet = Goodclass(model, loss_fn, optimizer, es_patience=10)
AnuwazNet.train(100)",DL,Pytorch,anuwaz/s4e3-7-dnn-binary-classifiers,https://www.kaggle.com/anuwaz/s4e3-7-dnn-binary-classifiers
playground-series-s4e3,classification,multi-label-classification,Predict the probability of various defects on steel plates across seven categories.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. The training dataset contains features and seven binary target columns representing different defect categories: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults. The test dataset is used to predict the probabilities for these defect categories. The dataset is in CSV format and has a size of 5.49 MB.","['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']","[""drop the 'id' column"", ""split features and targets"", ""apply logarithmic transformation to specific features"", ""standardize numerical features"", ""convert data to PyTorch tensors"", ""create DataLoader for training and testing datasets""]","model=NeuralNets().to(device)
epoch=100
loss_fn=torch.nn.BCELoss()
trainer=Trainer(model, loss_fn)
trainer.fit(train_loader,test_loader,epoch,verbose=1,patience=10)",DL,Pytorch,habilamar/baseline-neural-network-s4e3-kaggle-playground,https://www.kaggle.com/habilamar/baseline-neural-network-s4e3-kaggle-playground
playground-series-s4e3,classification,multi-label-classification,Predict the probability of various defects on steel plates across seven categories.,Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. The training dataset contains seven binary targets representing different defect categories: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults. The test dataset is used to predict the probabilities for these defect categories. The dataset is in CSV format and has a size of 5.49 MB.","['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']","[""scale features using standard scaler"", ""split data into training and validation sets""]","self.classifier = nn.Sequential(
            nn.Linear(input_dim, 250),
            nn.BatchNorm1d(250),
            nn.Tanh(),
            nn.Dropout(0.05),
            nn.Linear(250, 700),
            nn.BatchNorm1d(700),
            nn.Tanh(),
            nn.Dropout(0.15),
            nn.Linear(700, 700),
            nn.BatchNorm1d(700),
            nn.Tanh(),
            nn.Dropout(0.45),
            nn.Linear(700, output_dim),
            nn.Sigmoid()
        )

        self.criterion = nn.BCELoss()

        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 
            mode=""max"", 
            factor=0.3, 
            patience=5, 
            threshold=0.001,
            verbose=False
        )

        return {
            ""optimizer"": optimizer,
            ""lr_scheduler"": {
                ""scheduler"": scheduler,
                ""monitor"": ""val_auc""
            }
        }",DL,Pytorch,ravaghi/classifying-tabular-data-with-lightning-mlp,https://www.kaggle.com/ravaghi/classifying-tabular-data-with-lightning-mlp
playground-series-s4e3,classification,multi-label-classification,Predict the probability of various defects on steel plates across seven categories.,Tabular,AUCROC,"The dataset for this competition includes training and test data generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. It contains 7 binary targets representing different defect categories: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, and Other_Faults. The training dataset is provided in train.csv, while the test dataset is in test.csv. A sample submission file is also included to demonstrate the required format for predictions.","['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']","[""normalize features"", ""split dataset into training and testing sets"", ""create custom dataset class for loading data""]","model = neural_network_v2(27, 256, 7, 0.5)
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
loss_fn = nn.CrossEntropyLoss()
for e in range(EPOCHS):
    train(model, train_loader, optimizer, loss_fn)
test_loss = test(model, test_loader, loss_fn)",DL,Pytorch,anrenk/steel-plate-defect-neural-net,https://www.kaggle.com/anrenk/steel-plate-defect-neural-net
playground-series-s4e2,classification,multiclass-classification,The goal is to predict obesity risk categories based on individual characteristics and lifestyle factors.,Tabular,Accuracy,"The dataset consists of various attributes related to individuals' characteristics and lifestyle factors, collected to predict the risk of obesity. It includes demographic information, dietary habits, physical activity levels, and other relevant factors. The training dataset contains a categorical target variable, NObeyesdad, which represents the obesity risk category of individuals. The dataset is well-suited for visualizations, clustering, and exploratory data analysis.",['NObeyesdad'],"[""label-encode categorical variables""]","model.fit(X_train, y_train)",DL,Tensorflow,muhammadfurqan0/ps4e2-obesity-risk-gb-0-90,https://www.kaggle.com/muhammadfurqan0/ps4e2-obesity-risk-gb-0-90
playground-series-s4e2,classification,multiclass-classification,The goal of this competition is to predict obesity risk in individuals based on various factors related to cardiovascular disease.,Tabular,Accuracy,"The dataset consists of training and test data generated from a deep learning model trained on an obesity or cardiovascular disease risk dataset. It includes features such as gender, age, height, weight, family history with overweight, and dietary habits. The target variable is a categorical label indicating the level of obesity. The training dataset contains 20758 records and 18 columns, while the test dataset has 13840 records and 17 columns. The dataset is suitable for visualizations and exploratory data analysis.",['NObeyesdad'],"[""drop the id column"", ""encode categorical variables"", ""generate new features such as BMI and water intake per kg"", ""perform one-hot encoding on CALC column""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_dim,)))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_val, y_val))",DL,Tensorflow,getanmolgupta01/obese-pred-92-eda-catboost-lgbm-xgboost,https://www.kaggle.com/getanmolgupta01/obese-pred-92-eda-catboost-lgbm-xgboost
playground-series-s4e2,classification,multiclass-classification,The goal of this competition is to predict obesity risk in individuals based on various factors related to cardiovascular disease.,Tabular,Accuracy,"The dataset for this competition includes both training and test sets generated from a deep learning model trained on an obesity or cardiovascular disease risk dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a categorical target variable, NObeyesdad, which represents different obesity risk categories. The test dataset is used to predict the class of NObeyesdad for each row. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['NObeyesdad'],"[""median-impute missing values"", ""one-hot encode categorical variables"", ""scale numerical features using standard scaling""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_dim,)))
model.add(Dense(64, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32)",DL,Tensorflow,chinmayadatt/obesity-risk-prediction-multi-class-0-92160,https://www.kaggle.com/chinmayadatt/obesity-risk-prediction-multi-class-0-92160
playground-series-s4e2,classification,multiclass-classification,"The goal of this competition is to use various factors to predict obesity risk in individuals, which is related to cardiovascular disease.",Tabular,Accuracy,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the obesity or cardiovascular disease risk dataset. The feature distributions are similar to the original dataset. The training dataset contains a categorical target variable, NObeyesdad, which represents different obesity categories. The test dataset is used to predict the class of NObeyesdad for each row. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['NObeyesdad'],"[""median-impute missing values"", ""one-hot encode categorical variables"", ""scale numerical features""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_shape,)))
model.add(Dense(64, activation='relu'))
model.add(Dense(7, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)",DL,Tensorflow,divyam6969/best-solution-multiclass-obesity-prediction,https://www.kaggle.com/divyam6969/best-solution-multiclass-obesity-prediction
playground-series-s4e2,classification,multiclass-classification,"The goal of this competition is to use various factors to predict obesity risk in individuals, which is related to cardiovascular disease.",Tabular,Accuracy,"The dataset for this competition includes training and test datasets generated from a deep learning model trained on the obesity or cardiovascular disease risk dataset. The feature distributions are similar to the original dataset. The training dataset contains a categorical target variable, NObeyesdad, while the test dataset is used to predict the class of NObeyesdad for each row. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['NObeyesdad'],"[""drop the 'id' column from training and test datasets"", ""combine training dataset with original obesity dataset"", ""drop duplicates from the combined dataset"", ""handle missing values in the training and test datasets"", ""scale numerical features using square root transformation"", ""encode categorical variables using label encoding""]","lgbm_clf = LGBMClassifier(
    ""objective"": ""multiclass"",
    ""metric"": ""multi_logloss"",
    ""verbosity"": -1,
    ""boosting_type"": ""gbdt"",
    ""random_state"": 42,
    ""num_class"": 7,
    'learning_rate': 0.031,
    'n_estimators': 550,
    'lambda_l1': 0.010,
    'lambda_l2': 0.040,
    'max_depth': 20,
    'colsample_bytree': 0.413,
    'subsample': 0.97,
    'min_child_samples': 25,
    'class_weight':'balanced'
}
lgbm_clf.fit(X_train_, y_train_)
y_pred_test = lgbm_clf.predict(X_test)",DL,Tensorflow,divyam6969/beginners-92-accuracy-obesity-lgbm,https://www.kaggle.com/divyam6969/beginners-92-accuracy-obesity-lgbm
playground-series-s4e2,classification,multiclass-classification,The goal of this competition is to predict obesity risk in individuals based on various factors related to cardiovascular disease.,Tabular,Accuracy,"The dataset for this competition includes both training and test sets generated from a deep learning model trained on an obesity or cardiovascular disease risk dataset. It contains 17 features, including personal information and lifestyle habits, with the target variable being NObeyesdad, which categorizes obesity risk levels. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['NObeyesdad'],"[""map binary categorical features to 0 and 1"", ""apply ranking transformation to ordinal features"", ""one-hot encode gender feature"", ""map transportation modes to physical activity levels"", ""label encode target variable""]","model = MultiClassNet(NUM_FEATURES, N_CLASSES)
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=0.0001)

for epoch in range(epochs):
    model.train()
    for n, (X, y) in enumerate(trainloader):
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        y_pred = model(X)
        loss = criterion(y_pred, y)
        loss.backward()
        optimizer.step()
    model.eval()
    with torch.no_grad():
        for n, (X, y) in enumerate(valloader):
            X, y = X.to(device), y.to(device)
            y_val_pred = model(X)
            val_loss = criterion(y_val_pred, y)",DL,Pytorch,kattat/obesity-prediction-eda-fe-pytorch,https://www.kaggle.com/kattat/obesity-prediction-eda-fe-pytorch
playground-series-s4e2,classification,multiclass-classification,The goal is to predict obesity risk in individuals based on various factors related to cardiovascular disease.,Tabular,Accuracy,"The dataset consists of training and test data generated from a deep learning model trained on an obesity or cardiovascular disease risk dataset. It includes features that are close to the original dataset, allowing for exploration of differences and potential improvements in model performance. The training dataset contains a categorical target variable, NObeyesdad, while the test dataset is used to predict this target. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['NObeyesdad'],"[""calculate BMI from weight and height"", ""categorize BMI based on gender"", ""create age and height categories"", ""normalize selected columns using MinMaxScaler"", ""apply one-hot encoding to categorical variables"", ""map target variable to numerical values"", ""drop unnecessary columns""]","model = NN(input_size, output_size, dropout_rate)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
scheduler = StepLR(optimizer, step_size=20, gamma=0.5)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    average_loss = total_loss / len(train_loader)
    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_loss:.4f}')",DL,Pytorch,guedes5132/multi-class-prediction-of-obesity-risk,https://www.kaggle.com/guedes5132/multi-class-prediction-of-obesity-risk
playground-series-s4e2,classification,multiclass-classification,"The goal of this competition is to use various factors to predict obesity risk in individuals, which is related to cardiovascular disease.",Tabular,Accuracy,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the obesity or cardiovascular disease risk dataset. The feature distributions are similar to the original dataset. The training dataset contains a categorical target variable, NObeyesdad, while the test dataset is used to predict the class of NObeyesdad for each row. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['NObeyesdad'],"[""map binary columns to integers"", ""map categorical variables to integers"", ""one-hot encode gender column"", ""calculate BMI from weight and height"", ""calculate bmioncp from BMI and NCP"", ""bin BMI into categories""]","model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=N_CLASSES)
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=LEARNING_RATE)
train_network(model, optimizer, criterion, X_train, y_train, X_test, y_test, EPOCHS, train_losses, test_losses)",DL,Pytorch,risakashiwabara/obesity-risk-pytorch,https://www.kaggle.com/risakashiwabara/obesity-risk-pytorch
playground-series-s4e2,classification,multiclass-classification,The goal is to predict obesity risk in individuals based on various factors related to cardiovascular disease.,Tabular,Accuracy,"The dataset consists of training and test data generated from a deep learning model trained on an obesity or cardiovascular disease risk dataset. It includes features that are close to the original dataset, allowing for exploration of differences and potential improvements in model performance. The training dataset contains a categorical target variable, NObeyesdad, while the test dataset is used to predict this target. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['NObeyesdad'],"[""combine training data with original dataset"", ""encode categorical variables into numerical format"", ""standardize the feature values using a scaler"", ""perform feature engineering to create new features""]","model=nn.Sequential()
model.add_module('hidden1',nn.Linear(27,98))
model.add_module('activation1',nn.ReLU())
model.add_module('batchNorm1',nn.BatchNorm1d(98))
model.add_module('dropout1',nn.Dropout(p=0.2))
model.add_module('hidden2',nn.Linear(98,136))
model.add_module('activation2',nn.ReLU())
model.add_module('batchNorm2',nn.BatchNorm1d(136))
model.add_module('dropout2',nn.Dropout(p=0.2))
model.add_module('hidden3',nn.Linear(136,24))
model.add_module('activation3',nn.ReLU())
model.add_module('batchNorm3',nn.BatchNorm1d(24))
model.add_module('dropout3',nn.Dropout(p=0.2))
model.add_module('output',nn.Linear(24,7))
model.add_module('softmax',nn.Softmax(dim=1))
optimizer=optim.Adam(model.parameters(),lr=learning_rate)
loss_fn=nn.CrossEntropyLoss() 
n_epochs=241
ann=DNN(model,loss_fn,optimizer,es_patience)
ann.set_loaders(train_loader,val_loader)
ann.set_scheduler(patience=5,factor=0.3)
ann.train(n_epochs)",DL,Pytorch,anuwaz/s4-e2-pytorch-deep-neural-network,https://www.kaggle.com/anuwaz/s4-e2-pytorch-deep-neural-network
playground-series-s4e2,classification,multiclass-classification,"The goal of this competition is to use various factors to predict obesity risk in individuals, which is related to cardiovascular disease.",Tabular,Accuracy,"The dataset for this competition includes both training and test sets generated from a deep learning model trained on the Obesity or CVD risk dataset. The feature distributions are similar to the original dataset. The training dataset contains a categorical target variable, NObeyesdad, while the test dataset is used to predict this target. The dataset is suitable for visualizations, clustering, and exploratory data analysis.",['NObeyesdad'],"[""concatenate original dataset with training data"", ""encode categorical variables using manual encoding"", ""perform feature engineering to create new features"", ""standardize the data using StandardScaler""]","model.add_module('hidden0', nn.Linear(27, 128))
model.add_module('activation0', nn.ReLU())
model.add_module('batchNorm0', nn.BatchNorm1d(128))
model.add_module('dropout0', nn.Dropout(p=0.3))
model.add_module('hidden1', nn.Linear(128, 88))
model.add_module('activation1', nn.ReLU())
model.add_module('batchNorm1', nn.BatchNorm1d(88))
model.add_module('dropout1', nn.Dropout(p=0.2))
model.add_module('hidden2', nn.Linear(88, 32))
model.add_module('activation2',nn.ReLU())
model.add_module('batchNorm2', nn.BatchNorm1d(32))
model.add_module('dropout2', nn.Dropout(p=0.1))
model.add_module('output', nn.Linear(32, 7))
model.add_module('softmax', nn.Softmax(dim=1))
optimizer=optim.Adam(model.parameters(),lr=learning_rate)
loss_fn=nn.CrossEntropyLoss() 
DeepNN.train(n_epochs)",DL,Pytorch,anuwaz/class-imbalance-fixed-deep-neural-network,https://www.kaggle.com/anuwaz/class-imbalance-fixed-deep-neural-network
playground-series-s4e1,classification,binary-classification,"The task is to predict whether a customer continues with their account or closes it, indicating customer churn.",Tabular,AUCROC,"The dataset consists of training and testing data generated from a deep learning model trained on a bank customer churn prediction dataset. It includes features such as customer demographics, account details, and whether the customer has exited or not. The training dataset contains 165034 rows and 14 columns, with the target variable being binary, indicating customer churn.",['Exited'],"[""standardize numerical features"", ""apply power transformation to numerical features"", ""apply quantile transformation to numerical features"", ""encode categorical features using label encoding""]","model=tf.keras.models.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_test, y_test), callbacks=[early_stopping])",DL,Tensorflow,marianadeem755/bank-churn-classification-neural-network-xgboost,https://www.kaggle.com/marianadeem755/bank-churn-classification-neural-network-xgboost
playground-series-s4e1,classification,binary-classification,"The task is to predict whether a customer continues with their account or closes it, indicating customer churn.",Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on a bank customer churn prediction dataset. It includes features such as customer ID, surname, credit score, geography, gender, age, tenure, balance, number of products, credit card ownership, active membership status, estimated salary, and a binary target variable indicating whether the customer has churned.",['Exited'],"[""drop unnecessary columns"", ""combine datasets for encoding"", ""apply one-hot encoding"", ""min-max scale numerical features"", ""apply transformations to numerical features"", ""handle categorical features with encoding techniques"", ""apply TFIDF and PCA on text features"", ""create new features based on arithmetic operations"", ""remove unimportant features""]","ann = Sequential()
ann.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer='he_uniform', activation=lrelu))
ann.add(Dropout(0.1))
ann.add(Dense(16,  kernel_initializer='he_uniform', activation=lrelu))
ann.add(Dropout(0.1))
ann.add(Dense(4,  kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.1))
ann.add(Dense(1,  kernel_initializer='he_uniform', activation='sigmoid'))
ann.compile(loss=""binary_crossentropy"", optimizer=sgd,metrics=['accuracy'])",DL,Tensorflow,arunklenin/ps4e1-advanced-feature-engineering-ensemble,https://www.kaggle.com/arunklenin/ps4e1-advanced-feature-engineering-ensemble
playground-series-s4e1,classification,binary-classification,"The task is to predict whether a customer continues with their account or closes it, indicating churn.",Tabular,AUCROC,"The dataset for this competition includes a training dataset with a binary target variable 'Exited' and a test dataset where the objective is to predict the probability of 'Exited'. The dataset was generated from a deep learning model trained on a bank customer churn prediction dataset, with feature distributions similar to the original.",['Exited'],"[""encode categorical features using label encoding"", ""split the dataset into training and testing sets"", ""scale features using MinMaxScaler""]","model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=100, batch_size=128, callbacks=[early_stopping, reduce_lr])",DL,Tensorflow,danishammar/bank-churn-165034-dl,https://www.kaggle.com/danishammar/bank-churn-165034-dl
playground-series-s4e1,classification,binary-classification,"The task is to predict whether a customer continues with their account or closes it, indicating customer churn.",Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on the Bank Customer Churn Prediction dataset. It includes 13 columns with features such as Customer ID, Credit Score, Geography, Gender, Age, Tenure, Balance, and the binary target variable Exited, which indicates whether a customer has churned. The training dataset contains 165,000 rows, while the test dataset has 110,000 rows, with no missing values in either set.",['Exited'],"[""convert categorical columns to category type"", ""replace zero balance with NaN"", ""round estimated salary"", ""round age"", ""round balance"", ""generate new features based on existing ones"", ""drop unnecessary features""]","inputs = tf.keras.Input((x.shape[1]))
inputs_norm = tf.keras.layers.BatchNormalization()(inputs)
z = tf.keras.layers.Dense(32)(inputs_norm)
z = tf.keras.layers.BatchNormalization()(z)
z = tf.keras.layers.LeakyReLU()(z)
z = tf.keras.layers.Dense(64)(z)
z = tf.keras.layers.BatchNormalization()(z)
z = tf.keras.layers.LeakyReLU()(z)
z = tf.keras.layers.Dense(16)(z)
z = tf.keras.layers.BatchNormalization()(z)
z = tf.keras.layers.LeakyReLU()(z)
z = tf.keras.layers.Dense(4)(z)
z = tf.keras.layers.BatchNormalization()(z)
z = tf.keras.layers.LeakyReLU()(z)
z = tf.keras.layers.Dense(1)(z)
z = tf.keras.layers.BatchNormalization()(z)
outputs = tf.keras.activations.sigmoid(z)
self.model = tf.keras.Model(inputs, outputs)
self.model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.AdamW(1e-4))
self.model.fit(x.to_numpy(), y, epochs = 10, verbose = 0)",DL,Tensorflow,iqbalsyahakbar/ps4e1-3rd-place-solution,https://www.kaggle.com/iqbalsyahakbar/ps4e1-3rd-place-solution
playground-series-s4e1,classification,binary-classification,"The task is to predict whether a customer continues with their account or closes it, indicating customer churn.",Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on a bank customer churn prediction dataset. It includes features that are similar to the original dataset, with a binary target variable 'Exited' indicating whether a customer has churned. The training dataset contains 165034 records and 14 columns, while the test dataset is used to predict the probability of churn. The dataset is provided in CSV format and is approximately 21.65 MB in size.",['Exited'],"[""drop irrelevant columns"", ""merge datasets"", ""drop duplicates"", ""drop missing values"", ""one-hot encode categorical variables"", ""convert categorical columns to numerical"", ""apply feature engineering""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_dim,)))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])
model.fit(x_smote, y_smote, epochs=50, batch_size=32, validation_split=0.2)",DL,Tensorflow,getanmolgupta01/bank-churn-eda-catboost-lgbm-xgboost,https://www.kaggle.com/getanmolgupta01/bank-churn-eda-catboost-lgbm-xgboost
playground-series-s4e1,classification,binary-classification,"The task is to predict whether a customer continues with their account or closes it, indicating customer churn.",Tabular,AUCROC,"The dataset for this competition includes a training dataset with a binary target variable 'Exited' indicating customer churn, and a test dataset where the goal is to predict the probability of 'Exited'. The dataset was generated from a deep learning model trained on a bank customer churn prediction dataset, with feature distributions similar to the original.",['Exited'],"[""drop unnecessary columns"", ""standardize numerical features"", ""one-hot encode categorical features""]","model = Net(X_train.shape[1])
criterion = nn.BCELoss()
optimizer = Prodigy(model.parameters(), lr=1.0, weight_decay=1e-2)
best_model_state = train_nn(model, criterion, optimizer, train_loader, val_loader)",DL,Pytorch,anthonytherrien/simple-customer-churn-rate-nn,https://www.kaggle.com/anthonytherrien/simple-customer-churn-rate-nn
playground-series-s4e1,classification,binary-classification,"The task is to predict whether a customer continues with their account or closes it, indicating customer churn.",Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Bank Customer Churn Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target variable 'Exited', while the test dataset is used to predict the probability of 'Exited'.",['Exited'],"[""feature engineering to create new features"", ""drop unnecessary columns"", ""one-hot encode categorical variables"", ""scale numerical features""]","self.initial = nn.Sequential(nn.Linear(input_size, hidden_block_sizes[0]), nn.ReLU())
self.output = nn.Linear(hidden_block_sizes[-1], output_size)
model.compile(optimizer=optim.AdamW(model.parameters(), lr=lr_initial, weight_decay=8e-3), loss=nn.BCELoss())
model.fit(train_loader)",DL,Pytorch,anthonytherrien/advanced-model-residual-block-2x-ml,https://www.kaggle.com/anthonytherrien/advanced-model-residual-block-2x-ml
playground-series-s4e1,classification,binary-classification,"The task is to predict whether a customer continues with their account or closes it, indicating customer churn.",Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Bank Customer Churn Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target variable 'Exited', while the test dataset is used to predict the probability of 'Exited'.",['Exited'],"[""median-impute missing values"", ""standardize numerical features"", ""one-hot encode categorical features"", ""create new features based on existing data"", ""remove unnecessary columns""]","model = ANNnet()
lossfun = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
model.to(device)",DL,Pytorch,divyanshu2000/bank-churn-binary-classification-ml-pytorch,https://www.kaggle.com/divyanshu2000/bank-churn-binary-classification-ml-pytorch
playground-series-s4e1,classification,binary-classification,"The task is to predict whether a customer continues with their account or closes it, indicating churn.",Tabular,AUCROC,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on the Bank Customer Churn Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a binary target variable 'Exited', while the test dataset is used to predict the probability of 'Exited'.",['Exited'],"[""drop unnecessary columns"", ""label encode categorical features"", ""scale features using standard scaler""]","clf = TabNetClassifier()
clf.fit(X_scaled, y.values,
        max_epochs=25,
        eval_set=[(X_scaled, y.values)],
        eval_name =['train'], eval_metric=['auc', 'balanced_accuracy'],
        patience=10)",DL,Pytorch,shayalvaghasiya/binary-classification-bank-churn,https://www.kaggle.com/shayalvaghasiya/binary-classification-bank-churn
playground-series-s4e1,classification,binary-classification,"The task is to predict whether a customer continues with their account or closes it, indicating customer churn.",Tabular,AUCROC,"The dataset consists of generated data from a deep learning model trained on the Bank Customer Churn Prediction dataset. It includes both training and test datasets, with the target variable 'Exited' being binary. The training dataset is in train.csv and the test dataset is in test.csv, while sample_submission.csv provides the format for submission.",['Exited'],"[""load data into PyTorch Frame"", ""materialize datasets for column stats and embeddings""]","model = FTTransformer(channels=128, num_layers=8, out_channels=1, col_stats=dataset.col_stats, col_names_dict=dataset.tensor_frame.col_names_dict, stype_encoder_dict=stype_encoder_dict).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.00026224846167802537)
lr_scheduler = ExponentialLR(optimizer, gamma=0.9337593802894704)
model.fit(train_loader)",DL,Pytorch,yiwenyuan1998/tabular-deep-learning-with-pytorch-frame,https://www.kaggle.com/yiwenyuan1998/tabular-deep-learning-with-pytorch-frame
playground-series-s3e26,classification,multiclass-classification,"The task is to predict the outcomes of patients with cirrhosis using a multi-class approach, specifically predicting the probabilities of three possible statuses: C, CL, and D.",Tabular,Multi-class Log Loss,"The dataset consists of training and test data generated from a deep learning model based on the Cirrhosis Patient Survival Prediction dataset. The training dataset includes features related to patient demographics and health indicators, with the target variable being the categorical status of patients. The test dataset is used to predict the probabilities of each status. The dataset is in CSV format and contains various features such as age, sex, and laboratory results, with some missing values present.",['Status'],"[""drop the 'id' column from the training dataset"", ""merge supplementary data with the training dataset"", ""impute missing categorical values"", ""impute missing numerical values"", ""encode categorical features using one-hot and ordinal encoding"", ""create additional features based on existing data"", ""remove outliers based on standard deviations from the mean""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_shape,)))
model.add(Dense(64, activation='relu'))
model.add(Dense(3, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)",DL,Tensorflow,markuslill/s3e26-xgbclassifer,https://www.kaggle.com/markuslill/s3e26-xgbclassifer
playground-series-s3e26,classification,multiclass-classification,"The task is to predict the outcomes of patients with cirrhosis using a multi-class approach, specifically predicting probabilities for three possible outcomes: Status_C, Status_CL, and Status_D.",Tabular,Multi-class Log Loss,"The dataset consists of training and test data generated from a deep learning model trained on the Cirrhosis Patient Survival Prediction dataset. The training dataset includes features related to patient health and a categorical target variable, Status, which indicates the patient's outcome. The test dataset is used to predict the probabilities of the three outcomes. The dataset is in CSV format and has a size of 1.39 MB.",['Status'],"[""label-encode categorical variables"", ""standardize numerical features"", ""drop unnecessary columns""]","model.add(keras.layers.Dense(units=128, activation='relu'))
model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dropout(rate=0.2))
model.add(keras.layers.Dense(3, activation='softmax'))
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])
model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=callbacks)",DL,Tensorflow,endofnight17j03/final,https://www.kaggle.com/endofnight17j03/final
playground-series-s3e26,classification,multiclass-classification,"The task is to predict the outcomes of patients with cirrhosis using a multi-class approach, specifically predicting probabilities for three possible outcomes: Status_C, Status_CL, and Status_D.",Tabular,Multi-class Log Loss,"The dataset consists of training and test data generated from a deep learning model trained on the Cirrhosis Patient Survival Prediction dataset. The training dataset includes a categorical target variable, Status, which indicates the patient's outcome: C (censored), CL (alive due to liver transplant), and D (deceased). The test dataset requires predicting the probabilities of these three outcomes. The dataset is in CSV format and has a size of 1.39 MB.",['Status'],"[""drop unnecessary columns"", ""impute missing values in numerical columns using KNN"", ""impute missing values in categorical columns using mode"", ""encode categorical variables using label encoding"", ""standard scale the features""]","model = Sequential([
    Dense(hidden_units1, activation='relu', input_dim=x_train_scaled.shape[1]),
    Dropout(0.1),
    Dense(hidden_units2, activation='relu'),
    Dropout(0.1),
    Dense(hidden_units3, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(x_train_scaled, y_train_encoded, epochs=50, validation_data=(x_test_scaled, y_test_encoded), batch_size=32)",DL,Tensorflow,satyaprakashshukl/simple-neural-network-classification,https://www.kaggle.com/satyaprakashshukl/simple-neural-network-classification
playground-series-s3e26,classification,multiclass-classification,"The task is to predict the outcomes of patients with cirrhosis, specifically three possible statuses: C (censored), CL (censored due to liver transplantation), and D (deceased).",Tabular,Multi-class Log Loss,"The dataset consists of training and test data generated from a deep learning model based on the Cirrhosis Patient Survival Prediction dataset. It includes 17 features derived from a Mayo Clinic study on primary biliary cirrhosis conducted between 1974 and 1984. The training dataset contains a categorical target variable, Status, which indicates the patient's outcome, while the test dataset is used to predict the probabilities of each outcome.",['Status'],"[""remove outliers based on z-score"", ""normalize features for skewed distributions"", ""one-hot encode categorical features"", ""ordinal encode ordinal features"", ""scale numerical features using standard scaling""]","model = Sequential()
model.add(BatchNormalization(momentum=0.99, epsilon=0.00001))
model.add(Dense(32, input_dim=X_train3.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(8, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(3, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(X_train3_scaled, y_train_scaled, epochs=500, callbacks=callbacks_list, validation_split=0.1, batch_size=16)",DL,Tensorflow,valerybonneau/ps3e26-eda-automl-nn-and-ensemble,https://www.kaggle.com/valerybonneau/ps3e26-eda-automl-nn-and-ensemble
playground-series-s3e26,classification,multiclass-classification,"The task is to predict the outcomes of patients with cirrhosis using a multi-class approach, specifically predicting probabilities for three possible outcomes: Status_C, Status_CL, and Status_D.",Tabular,Multi-class Log Loss,"The dataset consists of training and test data generated from a deep learning model trained on the Cirrhosis Patient Survival Prediction dataset. The training dataset includes a categorical target variable, Status, which indicates the patient's outcome: C (censored), CL (alive due to liver transplant), and D (deceased). The test dataset requires predicting the probabilities of these three outcomes. The dataset is provided in CSV format and is approximately 1.39 MB in size.",['Status'],"[""concatenate training dataset with additional data"", ""drop unnecessary columns"", ""impute missing values in numerical columns using KNN"", ""impute missing values in categorical columns using mode"", ""encode categorical variables using label encoding"", ""apply one-hot encoding to categorical variables"", ""scale numerical features using standard scaling""]","model = Sequential([
    Dense(32, activation='relu', input_dim=x_train_scaled.shape[1]),
    Dropout(0.1),
    Dense(16, activation='relu'),
    Dropout(0.1),
    Dense(8, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(x_train_scaled, y_train_encoded, epochs=50, validation_data=(x_test_scaled, y_test_encoded), batch_size=32)",DL,Tensorflow,satyaprakashshukl/graph-neural-network,https://www.kaggle.com/satyaprakashshukl/graph-neural-network
playground-series-s3e26,classification,multiclass-classification,"The task is to predict the outcomes of patients with cirrhosis using a multi-class approach, specifically predicting probabilities for three possible outcomes: Status_C, Status_CL, and Status_D.",Tabular,Multi-class Log Loss,"The dataset consists of training and test data generated from a deep learning model trained on the Cirrhosis Patient Survival Prediction dataset. The training dataset includes features related to patient health and a categorical target variable, Status, which indicates the patient's outcome. The test dataset is used to predict the probabilities of the three outcomes. The dataset is in CSV format and is approximately 1.39 MB in size.",['Status'],"[""one-hot encode categorical features"", ""scale numerical features"", ""convert categorical target to numeric labels""]","self.layer1 = nn.Linear(input_size, 32)
self.batch_norm1 = nn.BatchNorm1d(32)
self.relu = nn.ReLU()
self.dropout1 = nn.Dropout(dropout_rate)
self.layer2 = nn.Linear(32, 16)
self.batch_norm2 = nn.BatchNorm1d(16)
self.dropout2 = nn.Dropout(dropout_rate)
self.output_layer = nn.Linear(16, 3)
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)",DL,Pytorch,anthonytherrien/simple-neural-network-benchmark,https://www.kaggle.com/anthonytherrien/simple-neural-network-benchmark
playground-series-s3e26,classification,multiclass-classification,The task is to predict the outcomes of patients with cirrhosis using a multi-class approach.,Tabular,Multi-class Log Loss,"The dataset for this competition includes a training dataset where the target variable is the categorical status of patients with cirrhosis, indicating whether they are alive, alive due to a liver transplant, or deceased. The test dataset requires predicting the probabilities of these three outcomes. The dataset was generated from a deep learning model trained on a related dataset, and feature distributions are similar but not identical to the original.",['Status'],"[""encode categorical variables"", ""drop the id column"", ""split the dataset into training and validation sets"", ""convert data to tensor format""]","model = NeuralNetwork()
crit = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.004695479567227815)
for epoch in range(5000):
    outputs = model(X_train)
    loss = crit(outputs, y_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()",DL,Pytorch,hridaym25/pytorch-nn-from-scratch-optuna,https://www.kaggle.com/hridaym25/pytorch-nn-from-scratch-optuna
playground-series-s3e26,classification,multiclass-classification,"The task is to predict the outcomes of patients with cirrhosis using a multi-class approach, specifically predicting the probabilities of three possible outcomes: Status_C, Status_CL, and Status_D.",Tabular,Multi-class Log Loss,"The dataset consists of a training set and a test set generated from a deep learning model trained on the Cirrhosis Patient Survival Prediction dataset. The training dataset includes a categorical target variable, Status, which indicates the patient's outcome: C (censored), CL (alive due to liver transplant), and D (deceased). The test dataset requires predicting the probabilities for each of these outcomes. The dataset is in CSV format and has a size of 1.39 MB.","['Status_C', 'Status_CL', 'Status_D']","[""drop duplicates"", ""drop missing values"", ""one-hot encode categorical variables"", ""map boolean values to integers"", ""replace categorical values with numerical equivalents""]","model_nn = TabularModel(input_size, output_size)
criterion = nn.BCELoss()
optimizer = optim.Adam(model_nn.parameters(), lr=0.001)
model_nn.train()
for epoch in range(epochs):
    total_loss = 0.0
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model_nn(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    average_loss = total_loss / len(train_loader)
    print(f""Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}"")
model_nn.eval()
with torch.no_grad():
    outputs_val = model_nn(X_val)
    loss_val = criterion(outputs_val, y_val)
    print(f""Validation Loss: {loss_val.item():.4f}"")",DL,Pytorch,umar47/cirrhosis-outcome-eda-modelling,https://www.kaggle.com/umar47/cirrhosis-outcome-eda-modelling
playground-series-s3e26,classification,multiclass-classification,"The task is to predict the outcomes of patients with cirrhosis using a multi-class approach, specifically predicting probabilities for three possible outcomes: Status_C, Status_CL, and Status_D.",Tabular,Multi-class Log Loss,"The dataset consists of training and test data generated from a deep learning model trained on the Cirrhosis Patient Survival Prediction dataset. The training dataset includes a categorical target variable, Status, which indicates the patient's outcome: C (censored), CL (alive due to liver transplant), or D (deceased). The test dataset requires predicting the probabilities of these three outcomes. The dataset is in CSV format and is approximately 1.39 MB in size.","['Status_C', 'Status_CL', 'Status_D']","[""one-hot encode categorical variables"", ""scale numerical variables using Min-Max scaling"", ""apply PCA to numerical variables after scaling""]","model_1 = create_dense_network(input_dim, hidden_dims_1, activations_1, output_dim, dropout, batch_norm).to(device=device)
train_model(model_1, train_data_loader, validation_data_loader, epochs=20, learning_rate=0.003, device=device)",DL,Pytorch,artemzysko/cirrhosis-prediction-pytorch-ensemble-models,https://www.kaggle.com/artemzysko/cirrhosis-prediction-pytorch-ensemble-models
playground-series-s3e26,classification,multiclass-classification,"The task is to predict the outcomes of patients with cirrhosis using a multi-class approach, specifically the probabilities of three possible outcomes: Status_C, Status_CL, and Status_D.",Tabular,Multi-class Log Loss,"The dataset consists of training and test data generated from a deep learning model trained on the Cirrhosis Patient Survival Prediction dataset. The training dataset includes a categorical target variable, Status, which indicates the patient's outcome: C (censored), CL (alive due to liver transplant), and D (deceased). The test dataset requires predicting the probabilities of these three outcomes. The dataset is in CSV format and has a size of 1.39 MB.",['Status'],"[""label-encode categorical variables"", ""standardize features using MinMaxScaler"", ""split dataset into training and testing sets""]","model = NN(input_size, num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=0.00009, weight_decay=0.3)
for epoch in range(epochs):
    outputs = model(X_train_tensor[i:i+batch_size])
    loss = criterion(outputs, y_train_tensor[i:i+batch_size])
    loss.backward()
    optimizer.step()",DL,Pytorch,anannoasif/multiclass,https://www.kaggle.com/anannoasif/multiclass
playground-series-s3e25,regression,continuous-regression,The task is to predict the Mohs hardness of a mineral based on its properties.,Tabular,MedAE – Median Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the prediction of Mohs hardness. The feature distributions are similar to the original dataset, which can also be used for exploration and model improvement. The training dataset includes a continuous target variable, Hardness, while the test dataset is used for making predictions. The dataset files are in CSV format and the total size is 2.11 MB.",['Hardness'],"[""drop the 'id' column from both train and test datasets"", ""calculate 'n_elements' as 'allelectrons_Total' divided by 'allelectrons_Average' plus a small constant"", ""calculate 'total_weight' as 'n_elements' multiplied by 'atomicweight_Average'"", ""drop the 'n_elements' column after calculation"", ""scale features using RobustScaler""]","model.fit(X, y)",DL,Tensorflow,larjeck/regression-with-a-mohs-hardness-dataset-optimal,https://www.kaggle.com/larjeck/regression-with-a-mohs-hardness-dataset-optimal
playground-series-s3e25,regression,continuous-regression,The task is to predict the Mohs hardness of a mineral based on its properties using regression techniques.,Tabular,MedAE – Median Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the prediction of Mohs hardness. The training dataset includes a continuous target variable, Hardness, while the test dataset is used for making predictions. The dataset is in CSV format and has no missing values.",['Hardness'],"[""drop unnecessary columns"", ""scale features using MinMaxScaler""]","input_layer = tf.keras.Input(shape = (12, ))
x = tf.keras.layers.BatchNormalization(epsilon = 0.00001)(input_layer)
x = tf.keras.layers.Dense(16, activation = 'relu')(x)
x = tf.keras.layers.Dense(32, activation = 'relu')(x)
output_layer = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs = input_layer, outputs = output_layer)
model.compile(optimizer = tf.keras.optimizers.Adam(0.013, beta_1 = 0.5), loss = loss_fn)",DL,Tensorflow,oscarm524/ps-s3-ep25-eda-modeling-submission,https://www.kaggle.com/oscarm524/ps-s3-ep25-eda-modeling-submission
playground-series-s3e25,regression,continuous-regression,The task is to predict the Mohs hardness of a mineral based on its properties.,Tabular,MedAE – Median Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the prediction of Mohs hardness. The features are derived from mineral properties, and the target variable is the continuous hardness value. The training dataset includes a column for hardness, while the test dataset is used for making predictions. The dataset is in CSV format and has a size of 2.11 MB.",['Hardness'],"[""drop the target column from features"", ""split the dataset into training and validation sets""]","input_layer = tf.keras.Input(shape=(len(features), ))
x = tf.keras.layers.BatchNormalization(epsilon=0.00001)(input_layer)
x = tf.keras.layers.Dense(16, activation='relu')(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)
output_layer = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(hp_learning_rate, beta_1=0.5),loss=loss_fn,metrics=metric_fn)
history1 = model1.fit(X_new.astype('float32'), y.astype('float32'),epochs=100,class_weight=model_pre.class_weight,callbacks=callbacks_list1,validation_split=0.1)",DL,Tensorflow,yutodennou/competition-lightgbm-and-nn,https://www.kaggle.com/yutodennou/competition-lightgbm-and-nn
playground-series-s3e25,regression,continuous-regression,The task is to predict the Mohs hardness of a mineral based on its properties.,Tabular,MedAE – Median Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the prediction of Mohs hardness. It includes features such as the number of electrons, atomic weight, ionization energy, and electronegativity. The training dataset contains a continuous target variable, Hardness, while the test dataset is used for making predictions. The dataset is in CSV format and has a size of 2.11 MB.",['Hardness'],"[""drop rows with missing values"", ""fill missing values with maximum of each column"", ""create new features based on existing ones""]","input_layer = tf.keras.Input(shape=(len(features+new_features), ))
x = tf.keras.layers.BatchNormalization()(input_layer)
x = tf.keras.layers.Dense(16, activation='relu')(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
output_layer = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(),loss=loss_fn,metrics=metric_fn)
history = model.fit(X_new, y,epochs=1000,validation_split=0.2)",DL,Tensorflow,enricomanosperti/regression-with-a-mohs-hardness-dataset,https://www.kaggle.com/enricomanosperti/regression-with-a-mohs-hardness-dataset
playground-series-s3e25,regression,continuous-regression,The task is to predict the Mohs hardness of a mineral based on its properties.,Tabular,MedAE – Median Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the prediction of Mohs hardness. The training dataset includes a continuous target variable, Hardness, while the test dataset is used to predict the same target. The dataset is in CSV format and has a size of 2.11 MB.",['Hardness'],"[""drop unnecessary columns"", ""check for null values"", ""split data into training and validation sets""]","input_layer = tf.keras.Input(shape=(len(features), ))
x = tf.keras.layers.BatchNormalization(epsilon=0.00001)(input_layer)
x = tf.keras.layers.Dense(16, activation='relu')(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)
output_layer = tf.keras.layers.Dense(1)(x)
model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(0.013, beta_1=0.5),loss=loss_fn,metrics=metric_fn)
history = model.fit(X_new.astype('float32'), y.astype('float32'),epochs=100,class_weight=model_pre.class_weight,callbacks=callbacks_list,validation_split=0.1)",DL,Tensorflow,imnandini/regression-with-mohs-hardnesss,https://www.kaggle.com/imnandini/regression-with-mohs-hardnesss
playground-series-s3e25,regression,continuous-regression,The task is to predict the Mohs hardness of a mineral based on its properties.,Tabular,MedAE – Median Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on a dataset for predicting Mohs hardness. It includes features that describe various properties of minerals, and the target variable is the continuous hardness value. The training dataset contains the target variable 'Hardness', while the test dataset is used for making predictions. The dataset is in CSV format and has a size of 2.11 MB.",['Hardness'],"[""drop unnecessary columns"", ""fill missing values with median"", ""apply Min-Max scaling to features""]","model = torch.nn.Sequential(
    OrderedDict([
        ('Linear_layer_1', torch.nn.Linear(in_features=10, out_features=16)),
        ('ReLU_activation_1', torch.nn.ReLU()),
        ('Linear_layer_2', torch.nn.Linear(in_features=16, out_features=32)),
        ('ReLU_activation_2', torch.nn.ReLU()),
        ('Linear_layer_3', torch.nn.Linear(in_features=32, out_features=64)),
        ('ReLU_activation_3', torch.nn.ReLU()),
        ('Linear_layer_4', torch.nn.Linear(in_features=64, out_features=1)),
    ])
)

loss_fn = MedianAbsoluteError()

optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.001
)

for epoch in range(1, num_epochs + 1):
    optimizer.zero_grad()
    pred = model(X)
    loss = loss_fn(pred, y)
    loss.backward()
    optimizer.step()",DL,Pytorch,kapturovalexander/kapturov-s-solution-of-ps-s3e25,https://www.kaggle.com/kapturovalexander/kapturov-s-solution-of-ps-s3e25
playground-series-s3e25,regression,continuous-regression,The task is to predict the Mohs hardness of a mineral based on its properties.,Tabular,MedAE – Median Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on a related dataset. The training dataset includes a continuous target variable, Hardness, while the test dataset is used for making predictions. The dataset is in CSV format and has a size of 2.11 MB.",['Hardness'],"[""drop the 'id' column from the training dataset"", ""drop invalid columns with over 99% identical values"", ""apply log transformation to right-skewed features"", ""apply square transformation to left-skewed features"", ""standardize the features using StandardScaler""]","model = nn.Sequential(
    nn.Linear(in_channels, out_channels),
    nn.ReLU(),
    nn.Linear(out_channels, C_OUT)
)
optimizer = torch.optim.Adam(model.parameters())
criterion = nn.L1Loss()
model.fit()",DL,Pytorch,ucas0v0zhuoqunli/eda-pytorch-optuna,https://www.kaggle.com/ucas0v0zhuoqunli/eda-pytorch-optuna
playground-series-s3e25,regression,continuous-regression,The task is to predict the Mohs hardness of a mineral based on its properties.,Tabular,MedAE – Median Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the prediction of Mohs hardness. The feature distributions are similar but not identical to the original dataset. The training dataset includes a continuous target variable, Hardness, while the test dataset is used for making predictions. A sample submission file is also provided in the correct format.",['Hardness'],"[""scale features using standard scaling"", ""label encode categorical variables""]","self.model = nn.Sequential(\n    nn.Linear(11, 256),\n    nn.ReLU(), \n    nn.Linear(256, 128),\n    nn.Dropout(0.2),\n    nn.Linear(128, 512),\n    nn.ReLU(), \n    nn.Linear(512, 128),\n    nn.Dropout(0.2),\n    nn.Linear(128, 256),\n    nn.ReLU(), \n    nn.Linear(256, 64),\n    nn.Dropout(0.2),\n    nn.Linear(64, 1)\n)\n\noptimizer = optim.Adam(self.parameters(), lr=1e-3)\ntrainer.fit(model, datamodule)",DL,Pytorch,stpeteishii/pgs-s3e25-pytorch-lightning-linear2,https://www.kaggle.com/stpeteishii/pgs-s3e25-pytorch-lightning-linear2
playground-series-s3e25,regression,continuous-regression,The task is to predict the Mohs hardness of a mineral based on its properties.,Tabular,MedAE – Median Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on a related dataset. It includes features such as total number of electrons, elemental density, and atomic weights, with the target variable being the Mohs hardness scale value ranging from 1 to 10. The training dataset contains 10407 rows and does not have any missing values.",['Hardness'],"[""drop the id column"", ""apply power transformation to stabilize variance and minimize skewness""]","model_config = FTTransformerConfig(
    task=""regression"",
    head = ""LinearHead"",
    head_config = head_config,
    target_range = [(1.0, 10.0)],
    **params
)
tabular_model = TabularModel(
    data_config=data_config,
    model_config=model_config,
    optimizer_config=optimizer_config,
    trainer_config=trainer_config,
)
loss = nn.L1Loss()
tabular_model.fit(train=train_df,
                  loss=loss,
                  metrics=[median_absolute_error_torch],
                  metrics_prob_inputs=[False])",DL,Pytorch,luistalavera/pg-s3-e25-ft-transformer,https://www.kaggle.com/luistalavera/pg-s3-e25-ft-transformer
playground-series-s3e25,regression,continuous-regression,The task is to predict the Mohs hardness of a mineral based on its properties using regression techniques.,Tabular,MedAE – Median Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on a related dataset. The training dataset includes various mineral properties, with the target variable being the continuous measure of Mohs hardness. The test dataset is used to evaluate the model's predictions. The dataset is in CSV format and has a size of 2.11 MB.",['Hardness'],"[""drop the 'id' column from train and test datasets"", ""merge original and playground data"", ""check for and drop duplicates"", ""apply skew transformation to features"", ""scale features using RobustScaler""]","self.bn = nn.BatchNorm1d(in_shape)
self.linear_stack = nn.Sequential(*layers)
self.linear_out = nn.Linear(hidden_units[-1], 1)
optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)
trainer.fit(lnn, oof_train_dataloader, oof_valid_dataloader)",DL,Pytorch,seanandrie/ps-s3-e25-regression-with-pytorch-lightning,https://www.kaggle.com/seanandrie/ps-s3-e25-regression-with-pytorch-lightning
playground-series-s3e24,classification,binary-classification,The task is to predict a patient's smoking status based on various health indicators using binary classification.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. It includes features such as age, height, weight, and various health metrics, with the target variable being the binary smoking status. The training dataset is provided in train.csv, while the test dataset is in test.csv, and a sample submission format is included in sample_submission.csv.",['smoking'],"[""drop unnecessary columns"", ""combine training and original datasets"", ""create additional features based on existing data"", ""apply min-max scaling to numerical features"", ""one-hot encode categorical features"", ""fill missing values iteratively"", ""apply various transformations to numerical features"", ""encode categorical features using target-guided mean encoding"", ""apply arithmetic operations to create new features"", ""eliminate unimportant features""]","ann = Sequential()
ann.add(Dense(64, input_dim=X_train.shape[1], kernel_initializer='he_uniform', activation=lrelu))
ann.add(Dropout(0.1))
ann.add(Dense(16, kernel_initializer='he_uniform', activation=lrelu))
ann.add(Dropout(0.1))
ann.add(Dense(4, kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.1))
ann.add(Dense(1, kernel_initializer='he_uniform', activation='sigmoid'))
ann.compile(loss=""binary_crossentropy"", optimizer=sgd,metrics=['accuracy'])",DL,Tensorflow,arunklenin/ps3e24-smoking-cessation-prediction-binary,https://www.kaggle.com/arunklenin/ps3e24-smoking-cessation-prediction-binary
playground-series-s3e24,classification,binary-classification,The task is to predict a patient's smoking status based on various health indicators using binary classification.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. The training dataset includes a binary target variable 'smoking', while the test dataset is used to predict the probability of this target. The dataset is in CSV format and has a size of 22.79 MB.",['smoking'],"[""convert boolean columns to integers"", ""remove outliers using IQR method"", ""split dataset into training and validation sets"", ""convert dataset from Pandas DataFrame to TensorFlow Dataset format""]","rf = tfdf.keras.RandomForestModel(task=tfdf.keras.Task.REGRESSION)
rf.compile(metrics=[RocAucMetric()])
rf.fit(x=train_ds)",DL,Tensorflow,rukenmissonnier/predict-using-tfdf,https://www.kaggle.com/rukenmissonnier/predict-using-tfdf
playground-series-s3e24,classification,binary-classification,The task is to predict a patient's smoking status based on various health indicators using binary classification.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a binary target variable 'smoking', while the test dataset is used to predict the probability of smoking status. The dataset is in CSV format and has a size of 22.79 MB.",['smoking'],"[""remove outliers for specific models"", ""apply log transformation for right-skewed features"", ""apply power transformation for left-skewed features"", ""scale features using MinMaxScaler""]","model = Sequential()
model.add(Dense(256, input_dim=22, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])
model.fit(X_train, y_train)",DL,Tensorflow,valerybonneau/ps-s3e24-eda-automl-and-ensemble-models,https://www.kaggle.com/valerybonneau/ps-s3e24-eda-automl-and-ensemble-models
playground-series-s3e24,classification,binary-classification,The task is to predict a patient's smoking status based on various health indicators using binary classification.,Tabular,AUCROC,The dataset consists of training and test data generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. The training dataset includes a binary target variable 'smoking' and various health indicators. The test dataset is used to predict the probability of the target variable. The dataset is in CSV format and has a size of 22.79 MB.,['smoking'],"[""combine training dataset with original dataset"", ""calculate Body Mass Index (BMI)"", ""calculate waist to height ratio"", ""calculate average eyesight"", ""calculate average hearing"", ""categorize blood pressure into ranges"", ""calculate cholesterol ratio""]","model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(num_features,)))
model.add(Dense(320, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dropout(.3))
model.add(Dense(128, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dropout(.3))
model.add(Dense(64, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dropout(.3))
model.add(Dense(32, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer=tf.keras.optimizers.AdamW(), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])
model.fit(X_train, y_train, batch_size=512, epochs=120, callbacks=[save_best_callback], validation_data=(X_val, y_val),verbose=2)",DL,Tensorflow,hikmatullahmohammadi/smoking-status-feature-engineering-tf-keras,https://www.kaggle.com/hikmatullahmohammadi/smoking-status-feature-engineering-tf-keras
playground-series-s3e24,classification,binary-classification,The task is to predict a patient's smoking status based on various health indicators.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a binary target variable 'smoking', while the test dataset is used to predict the probability of this target. A sample submission file is also provided in the correct format.",['smoking'],"[""median-impute missing values"", ""normalize health indicators""]","input_layer = tf.keras.Input(shape=(X_train.shape[1]+2, ))
x = tf.keras.layers.BatchNormalization()(input_layer)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(32, activation='relu')(x)
x = tf.keras.layers.Dense(16, activation='relu')(x)
output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(x)
model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.BinaryCrossentropy(),metrics=tf.keras.metrics.AUC())
model.fit(X, y,epochs=20,callbacks=callbacks_list,validation_split=0.2)",DL,Tensorflow,enricomanosperti/binary-prediction-of-smoker-status-using-bio-signa,https://www.kaggle.com/enricomanosperti/binary-prediction-of-smoker-status-using-bio-signa
playground-series-s3e24,classification,binary-classification,The task is to predict a patient's smoking status based on various health indicators using binary classification.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. It includes features such as age, height, weight, waist circumference, eyesight, hearing ability, blood pressure measurements, cholesterol levels, and the target variable smoking, which indicates if the individual is a smoker or not. The training dataset is provided in train.csv, while the test dataset is in test.csv, and a sample submission file is included as well.",['smoking'],"[""drop the 'id' column"", ""combine training data with original dataset"", ""create new features such as BMI"", ""clip outlier values in certain features"", ""normalize or scale features as needed""]","model = NN(input_dim).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)
for epoch in range(1000):
    optimizer.zero_grad()
    outputs = model(train_set).squeeze()
    loss = criterion(outputs, train_target)
    loss.backward()
    optimizer.step()",DL,Pytorch,yaaangzhou/pg-s3-e24-eda-modeling-ensemle-nn,https://www.kaggle.com/yaaangzhou/pg-s3-e24-eda-modeling-ensemle-nn
playground-series-s3e24,classification,binary-classification,The task is to predict a patient's smoking status based on various health indicators.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. It includes 24 variables, with the target variable 'smoking' indicating whether an individual is a smoker (1) or not (0). There are no missing values in the dataset, and the features include various health indicators such as age, height, weight, and blood pressure measurements.",['smoking'],"[""remove id and target variable"", ""log transform numerical features"", ""normalize the data""]","model = TabNetClassifier(optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2), scheduler_params={""step_size"":10, ""gamma"":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR, device_name=device)
model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=['auc'], max_epochs=100, patience=50, batch_size=256, virtual_batch_size=128)",DL,Pytorch,kimtaehun/deep-learning-binary-classification-template,https://www.kaggle.com/kimtaehun/deep-learning-binary-classification-template
playground-series-s3e24,classification,binary-classification,The task is to predict a patient's smoking status based on various health indicators using binary classification.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a binary target variable 'smoking', while the test dataset is used to predict the probability of this target. A sample submission file is also provided in the correct format.",['smoking'],"[""remove the first column"", ""normalize features using MinMaxScaler"", ""split data into training and testing sets"", ""convert data to PyTorch tensors""]","model = SimpleClassifier(input_size=X_train_tensor.shape[1], hidden_size=128, output_size=1)
criterion = nn.BCELoss(reduction='mean')
optimizer = optim.Adam(model.parameters(), lr=0.001)
for epoch in range(num_epochs):
    for batch in train_loader:
        x, y = batch
        y_hat = model(x).squeeze(dim=1)
        loss = criterion(y_hat, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()",DL,Pytorch,abdullaharean/simple-neural-network-test,https://www.kaggle.com/abdullaharean/simple-neural-network-test
playground-series-s3e24,classification,binary-classification,The task is to predict a patient's smoking status based on various health indicators using binary classification.,Tabular,AUCROC,The dataset consists of training and test data generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. The training dataset includes a binary target variable 'smoking' indicating whether an individual is a smoker or not. The test dataset is used to predict the probability of smoking status. The dataset is in CSV format and has no missing values.,['smoking'],"[""drop columns with mode greater than 90%"", ""drop LDL column due to unreliable measurements"", ""add new features: bmi and hdl_total_ratio"", ""apply log transformation to positively skewed features"", ""scale features using QuantileTransformer""]","model = NeuralNetClassifier(X.shape[1], params)
trainer.fit(model, train_loader, val_loader)",DL,Pytorch,syerramilli/ps3e24-pytorch-fnn,https://www.kaggle.com/syerramilli/ps3e24-pytorch-fnn
playground-series-s3e24,classification,binary-classification,The task is to predict a patient's smoking status based on various health indicators using binary classification.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a binary target variable 'smoking', while the test dataset is used to predict the probability of this target. The dataset files include train.csv, test.csv, and a sample submission file.",['smoking'],"[""drop unnecessary columns"", ""scale features using StandardScaler"", ""apply polynomial feature transformation"", ""apply PCA for dimensionality reduction""]","model = defectsNet(learning_rate=0.01, input_dim=21, epochs=300)
trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)",DL,Pytorch,edoardobianchi/ensembling-optuna-score-0-875,https://www.kaggle.com/edoardobianchi/ensembling-optuna-score-0-875
playground-series-s3e22,classification,multiclass-classification,"Given various medical indicators, predict the health outcomes of horses.",Tabular,Micro-averaged F1-Score,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a portion of the Horse Survival Dataset. The feature distributions are similar to the original dataset but not identical. Participants can use the original dataset to explore differences and assess if incorporating it improves model performance. The training dataset contains an outcome column as the target variable, while the test dataset is used for predictions. The dataset is in CSV format and is approximately 386.6 kB in size.",['outcome'],"[""map categorical variables to numerical values"", ""impute missing values using KNN"", ""remove features with high variance inflation factor""]","model = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(20, activation='relu'),
    Dropout(0.2),
    Dense(20, activation='relu'),
    Dropout(0.2),
    Dense(10, activation='relu'),
    Dropout(0.1),
    Dense(20, activation='relu'),
    Dropout(0.2),
    Dense(20, activation='relu'),
    Dropout(0.2),
    Dense(3, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, batch_size=64, epochs=20, validation_data=(X_val, y_val))",DL,Tensorflow,jasonheesanglee/email-notification-colab-session-autoshutdown,https://www.kaggle.com/jasonheesanglee/email-notification-colab-session-autoshutdown
playground-series-s3e22,classification,multiclass-classification,"Given various medical indicators, predict the health outcomes of horses.",Tabular,Micro-averaged F1-Score,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a portion of the Horse Survival Dataset. The feature distributions are close to, but not exactly the same as the original dataset. The training dataset contains a categorical target column named outcome, while the test dataset is used to predict the same outcome. A sample submission file is also provided in the correct format.",['outcome'],"[""load training data"", ""load test data"", ""convert training data to TensorFlow dataset format"", ""convert test data to TensorFlow dataset format""]","model_TFDF = tfdf.keras.RandomForestModel()
model_TFDF.fit(train_ds,callbacks=[chkpt,tensorboard,earlystopping],epochs=1, task=tfdf.keras.Task.CLASSIFICATION)",DL,Tensorflow,adityakishor1/eda-and-randomforest-of-health-horses,https://www.kaggle.com/adityakishor1/eda-and-randomforest-of-health-horses
playground-series-s3e22,classification,multiclass-classification,"Given various medical indicators, predict the health outcomes of horses.",Tabular,Micro-averaged F1-Score,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a portion of the Horse Survival Dataset. The feature distributions are close to, but not exactly the same as the original dataset. The training dataset contains 1235 rows and 29 columns, while the test dataset has 824 rows and 28 columns. The target variable is categorical, indicating the health outcome of the horse, which can be 'lived', 'died', or 'euthanized'.",['outcome'],"[""drop id and hospital_number columns"", ""fill missing values with median for numerical columns"", ""fill missing values with mode or 'Unknown' for categorical columns"", ""label encode categorical variables"", ""scale numerical features using StandardScaler""]","model = Sequential()
model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(3, activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])
history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)",DL,Tensorflow,taniamartynenko/hourse-health-outcomes-eda-mll-s3e22tania,https://www.kaggle.com/taniamartynenko/hourse-health-outcomes-eda-mll-s3e22tania
playground-series-s3e22,classification,multiclass-classification,"Given various medical indicators, predict the health outcomes of horses.",Tabular,Micro-averaged F1-Score,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a portion of the Horse Survival Dataset. The feature distributions are close to, but not exactly the same as the original dataset. The training dataset contains a categorical target variable named outcome, while the test dataset is used to predict the same outcome. A sample submission file is also provided in the correct format.",['outcome'],"[""fill missing values with the median for numerical columns"", ""fill missing values with the mode for categorical columns"", ""drop the id column"", ""apply label encoding to categorical features"", ""log transform specific numerical features"", ""apply oversampling to balance the classes"", ""split the dataset into training and testing sets""]","model_nn = k.models.Sequential([
    k.layers.Dense(512, activation=""relu""),
    k.layers.Dense(128, activation=""tanh""),
    k.layers.Dense(256, activation=""relu""),
    k.layers.Dense(128, activation=""tanh""),
    k.layers.Dense(64, activation=""relu""),
    k.layers.Dense(32, activation=""relu""),
    k.layers.Dense(16, activation=""tanh""),
    k.layers.Dense(3, activation=""softmax"")
])

model_nn.compile(optimizer=""nadam"", loss=k.losses.CategoricalFocalCrossentropy(), metrics=[""accuracy""])
history = model_nn.fit(x_train1, y_train1, epochs=100, validation_data=(x_test1, y_test1), validation_split=0.2)",DL,Tensorflow,osamaabobakr/predict-health-outcomes-83,https://www.kaggle.com/osamaabobakr/predict-health-outcomes-83
playground-series-s3e22,classification,multiclass-classification,"Given various medical indicators, predict the health outcomes of horses.",Tabular,Micro-averaged F1-Score,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a portion of the Horse Survival Dataset. The feature distributions are similar to the original dataset. The training dataset contains a categorical target column named 'outcome', while the test dataset is used to predict the same outcome. A sample submission file is also provided to illustrate the required format for predictions.",['outcome'],"[""drop missing values"", ""one-hot encode categorical features""]","model = Sequential([
    layers.Conv1D(256,5,activation='relu'),
    layers.MaxPool1D(),
    layers.Dropout(0.2),
    layers.Conv1D(128,5,activation='relu'),
    layers.Conv1D(64,5,activation='relu'),
    layers.MaxPool1D(),
    layers.Dropout(0.2),
    layers.Conv1D(32,5,activation='relu'),
    layers.Flatten(),
    layers.Dense(64,activation='relu'),
    layers.Dense(32,activation='relu'),
    layers.Dense(3,activation='softmax')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss='categorical_crossentropy', metrics=['acc'])
history = model.fit(tf.expand_dims(x,axis=-1), y, epochs=10)",DL,Tensorflow,philopateergeorgei/eda-random-forest,https://www.kaggle.com/philopateergeorgei/eda-random-forest
playground-series-s3e22,classification,multiclass-classification,"Given various medical indicators, predict the health outcomes of horses.",Tabular,Micro-averaged F1-Score,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a portion of the Horse Survival Dataset. The feature distributions are close to, but not exactly the same as the original dataset. The training dataset contains a categorical target variable named outcome, while the test dataset is used to predict the same outcome. A sample submission file is also provided in the correct format.",['outcome'],"[""drop the 'id' column from both train and test datasets"", ""merge train and original datasets to enhance feature diversity"", ""fill missing values with mode for categorical features"", ""apply label encoding for binary categorical variables"", ""apply one-hot encoding for nominal categorical variables"", ""create new features based on domain knowledge""]","model = NN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(),lr=1e-3,weight_decay=1e-3)
for epoch in range(600):
    pred = model(train_set)
    pred = pred.squeeze()
    loss = criterion(pred, train_target)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()",DL,Pytorch,yaaangzhou/pg-s3-e22-eda-modeling,https://www.kaggle.com/yaaangzhou/pg-s3-e22-eda-modeling
playground-series-s3e22,classification,multiclass-classification,"Given various medical indicators, predict the health outcomes of horses.",Tabular,Micro-averaged F1-Score,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a portion of the Horse Survival Dataset. The feature distributions are close to, but not exactly the same as the original dataset. The training dataset contains a categorical target variable named outcome, while the test dataset is used to predict the same outcome. The dataset files include train.csv, test.csv, and a sample_submission.csv file in the correct format.",['outcome'],"[""drop the 'id' column"", ""merge training and original datasets"", ""fill missing values with mode"", ""map target variable to numerical values"", ""apply ordinal encoding to categorical variables"", ""one-hot encode specific categorical features""]","self.layer_1=nn.Linear(train_set.shape[1],128)
self.layer_2=nn.Linear(128,3)
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(),lr=1e-3,weight_decay=1e-3)
model.train()",DL,Pytorch,mathurmudit2022/top-1-forecasting-the-health-outcomes-of-horses,https://www.kaggle.com/mathurmudit2022/top-1-forecasting-the-health-outcomes-of-horses
playground-series-s3e22,classification,multiclass-classification,"Given various medical indicators, predict the health outcomes of horses.",Tabular,Micro-averaged F1-Score,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a portion of the Horse Survival Dataset. The feature distributions are close to, but not exactly the same as the original dataset. The training dataset contains a categorical target variable named outcome, while the test dataset is used to predict the same outcome. A sample submission file is also provided in the correct format.",['outcome'],"[""drop id column from train and test datasets"", ""encode categorical labels using label encoding"", ""scale numerical features using standard scaling"", ""one-hot encode the target variable"", ""split the training data into training and validation sets""]","model = MulticlassClassifier()
model.to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
train_loader = DataLoader(dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(dataset, batch_size=32, shuffle=True)

for i in tqdm(range(epochs), desc = 'Epoch'):
    model.train()
    for idx, (x, t) in enumerate(train_loader):
        optimizer.zero_grad()
        y = model(x.to(device))
        loss = loss_fn(y.to(device), t.to(device))
        loss.backward()
        optimizer.step()

model.eval()
with torch.no_grad():
    y = model(test_X.to(device))
    y = y.argmax(dim=1)",DL,Pytorch,lonewalker29/pytorch-neural-network-baseline,https://www.kaggle.com/lonewalker29/pytorch-neural-network-baseline
playground-series-s3e22,classification,multiclass-classification,"Given various medical indicators, predict the health outcomes of horses.",Tabular,Micro-averaged F1-Score,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a portion of the Horse Survival Dataset. The feature distributions are similar to the original dataset. The training dataset contains a categorical target variable named outcome, while the test dataset is used to predict the same outcome. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['outcome'],"[""drop id column from train and test datasets"", ""encode categorical labels using label encoding"", ""scale numerical features using standard scaling"", ""one-hot encode the target variable"", ""split the training data into training and validation sets""]","model = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, num_classes)
)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(train_X, train_Y, validation_data=(val_X, val_Y), epochs=100, batch_size=32)",DL,Pytorch,lonewalker29/lightgbm-baseline,https://www.kaggle.com/lonewalker29/lightgbm-baseline
playground-series-s3e22,classification,multiclass-classification,"Given various medical indicators, predict the health outcomes of horses.",Tabular,Micro-averaged F1-Score,"The dataset for this competition includes both training and test datasets generated from a deep learning model trained on a portion of the Horse Survival Dataset. The feature distributions are close to, but not exactly the same as the original dataset. The training dataset contains a categorical target column named 'outcome', while the test dataset is used to predict the same outcome. The dataset files include train.csv, test.csv, and a sample_submission.csv file in the correct format.",['outcome'],"[""one-hot encode categorical variables"", ""drop unnecessary columns"", ""fill missing values with zero"", ""standardize numerical features"", ""apply PCA for dimensionality reduction""]","self.hidden_layer1 = nn.Linear(75,128)
self.output_layer = nn.Linear(128,3)
model.compile(optimizer=optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay), loss=nn.BCEWithLogitsLoss())
model.fit(trainset, traintarget, epochs=num_epochs)",DL,Pytorch,sunnywhile/the-simplest-nn-by-pytorch,https://www.kaggle.com/sunnywhile/the-simplest-nn-by-pytorch
playground-series-s3e21,regression,continuous-regression,"The task is to submit a dataset that will be used to train a model for predicting dissolved oxygen levels in river water, with the evaluation based on the Root Mean Square Error of the model's predictions against a hidden test dataset.",Tabular,RMSE – Root Mean Squared Error,"The dataset is a synthetic representation of dissolved oxygen prediction in river water, containing various indicators measured at multiple monitoring stations. It includes features such as dissolved oxygen levels, ammonium ions, nitrite ions, nitrate ions, and biochemical oxygen demand. The dataset must be cleaned and formatted correctly for submission, ensuring no missing values and adherence to the structure of the sample submission file.",['target'],"[""remove negative values"", ""drop rows with missing values"", ""concatenate original training data with submission data"", ""clip target values to specified bounds"", ""set unimportant features to zero""]","autoencoder = models.Sequential([
        layers.Input(shape=(input_dim,)),
        layers.Dense(encoding_dim, activation='relu'),
        layers.Dense(input_dim, activation='linear')
    ])
autoencoder.compile(optimizer='adam', loss='mean_squared_error')
autoencoder.fit(scaled_features, scaled_features, epochs=96, batch_size=67, shuffle=True, validation_split=0.1)",DL,Tensorflow,arunklenin/in-depth-analysis-five-anomaly-detection-methods,https://www.kaggle.com/arunklenin/in-depth-analysis-five-anomaly-detection-methods
playground-series-s3e21,regression,continuous-regression,The task is to improve a dataset that will be used to train a model for predicting dissolved oxygen levels in river water.,Tabular,RMSE – Root Mean Squared Error,"The dataset is a synthetic collection based on dissolved oxygen prediction in river water. Participants must submit a dataset that will be used to train a model, ensuring it contains all required columns and has no missing values. The submission may have fewer rows than the provided sample but not more.",['target'],"[""fill missing values using forward fill"", ""remove outliers using isolation forest"", ""scale numerical features using robust scaler"", ""one-hot encode categorical features"", ""remove features with over 96% of the same value"", ""clip target variable to specified range""]","model.compile(optimizer='adam', loss='mean_squared_error'); model.fit(X, y, epochs=10, batch_size=32)",DL,Tensorflow,prabhdeep123/data-centric-way,https://www.kaggle.com/prabhdeep123/data-centric-way
playground-series-s3e21,regression,continuous-regression,The task is to improve a dataset that will be used to train a model for predicting dissolved oxygen levels in river water.,Tabular,RMSE – Root Mean Squared Error,"The dataset is a synthetic collection based on dissolved oxygen prediction in river water. Participants must submit a dataset that will be used to train a model, ensuring it contains all required columns and has no missing values. The submission may have fewer rows than the provided sample but not more.",['target'],"[""drop unnecessary columns"", ""split dataset into training and validation sets"", ""convert pandas dataframe to TensorFlow dataset""]","rmodel = tfdf.keras.RandomForestModel(task=tfdf.keras.Task.REGRESSION,
                                  num_trees=1000,
                                  max_depth=7,
                                  random_seed=42,
                                  verbose=0)
rmodel.fit(x=train_ds)
rmodel.compile(metrics=[""mae"",""mse""])
evaluation = rmodel.evaluate(val_ds, return_dict=True, verbose=0)",DL,Tensorflow,viji1609/pgs3-e21-eda-with-trees-visualization,https://www.kaggle.com/viji1609/pgs3-e21-eda-with-trees-visualization
playground-series-s3e21,regression,continuous-regression,The task is to improve a dataset that will be used to train a model for predicting dissolved oxygen levels in river water.,Tabular,RMSE – Root Mean Squared Error,"The dataset is a synthetic collection based on dissolved oxygen prediction in river water. Participants must submit a dataset that will be used to train a model, ensuring it contains all required columns and has no missing values. The submission may have fewer rows than the provided sample but not more.",['target'],"[""drop target column"", ""apply PCA for dimensionality reduction"", ""detect outliers"", ""replace outliers with mean value""]","model = keras.Sequential()
model.add(keras.layers.Input(shape=(6,)))
model.add(keras.layers.Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Dense(200, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Dense(1, activation='linear'))
optimizer = hp.Choice('optimizer', values=['adam', 'sgd','rmsprop', 'adadelta'])
loss = hp.Choice(""loss"" , values=['MeanSquaredError','MeanAbsoluteError','MeanAbsolutePercentageError','MeanSquaredLogarithmicError','CosineSimilarity'])
model.compile(optimizer=optimizer, loss=loss, metrics=[root_mean_squared_error])
history = model.fit(new, df_target , epochs=200  , validation_split=0.2)",DL,Tensorflow,sugataraychaudhary/playground-features-reduction-ann-mlp-keras,https://www.kaggle.com/sugataraychaudhary/playground-features-reduction-ann-mlp-keras
playground-series-s3e21,regression,continuous-regression,"The task is to improve a dataset that will be used to train a model for predicting dissolved oxygen levels in river water, with the goal of minimizing the Root Mean Square Error between predictions and actual values.",Tabular,RMSE – Root Mean Squared Error,"The dataset is a synthetic collection based on dissolved oxygen prediction in river water. Participants must submit a dataset that includes all columns from the sample submission file, ensuring no NaN values are present. The dataset is structured in a CSV format and contains various features related to oxygen levels, with a target variable to predict.",['target'],"[""remove features with more than 96% of the same value"", ""remove features with high multicollinearity"", ""remove features with excessive missing values"", ""remove duplicate rows"", ""clip target variable to specified range"", ""impute missing values with mean for numerical features"", ""impute missing values with mode for categorical features"", ""scale numerical features using RobustScaler""]","model.compile(optimizer='adam', loss='mean_squared_error'); model.fit(X, y, epochs=100, batch_size=32)",DL,Tensorflow,jailsonevora/ml-regression-problem-data-centric-way,https://www.kaggle.com/jailsonevora/ml-regression-problem-data-centric-way
playground-series-s3e21,regression,continuous-regression,The task is to improve a dataset that will be used to train a model for predicting dissolved oxygen levels in river water.,Tabular,RMSE – Root Mean Squared Error,"The dataset is a synthetic dataset based on dissolved oxygen prediction in river water. Participants must submit a dataset that will be used to train a model, ensuring it contains all required columns and has no missing values. The submission may have fewer rows than the provided sample submission but not more.",['target'],"[""label encode categorical columns"", ""scale features using MinMaxScaler"", ""convert pandas DataFrame to PyTorch tensors""]","generator = self.create_generator(self.latent_dim, self.input_dim)
discriminator = self.create_discriminator(input_dim).to(self.device)
loss_fn = nn.BCELoss()
generator_optimizer = optim.Adam(generator.parameters(), lr=lr)
discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr)
generator_loss.backward()
generator_optimizer.step()",DL,Pytorch,thomasmeiner/ps-s3-21-gans-even-16-few-rows-is-enough,https://www.kaggle.com/thomasmeiner/ps-s3-21-gans-even-16-few-rows-is-enough
playground-series-s3e19,regression,continuous-regression,The task is to predict the number of items sold for various fictitious learning modules across different stores and countries for the year 2022.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,The dataset consists of synthetic sales data for various fictitious learning modules from different Kaggle-branded stores in various countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures real-world effects such as seasonality and holiday impacts.,['num_sold'],"[""drop the 'id' column"", ""convert 'date' to datetime format"", ""extract year, month, and day from 'date'"", ""drop the original 'date' column"", ""create dummy variables for categorical columns""]","model = keras.Sequential()
model.add(Dense(864, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01), input_dim=16))
model.add(Dense(864, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(392, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(520, activation='relu'))
model.add(Dense(368, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)))
model.add(Dense(928, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)))
model.add(keras.layers.Dense(1, activation='linear'))
hp_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), loss=tf.keras.losses.MeanAbsolutePercentageError(), metrics=[tf.keras.metrics.MeanAbsoluteError()])
history = model.fit(X_train_preprocessed, y_train, epochs=200, batch_size=4096, validation_data=(X_test_preprocessed, y_test))",DL,Tensorflow,abhashrai/sales-forecasting-playground-s3e19,https://www.kaggle.com/abhashrai/sales-forecasting-playground-s3e19
playground-series-s3e19,regression,continuous-regression,The task is to predict the number of items sold for various products across different stores and countries for the year 2022 based on historical sales data.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset consists of synthetic sales data for various fictitious learning modules from different Kaggle-branded stores in various countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting sales. The dataset captures real-world effects such as seasonality and holiday impacts, and it is structured in CSV format with a total size of 12.79 MB.",['num_sold'],"[""standardize numerical features"", ""normalize numerical features"", ""apply power transformation to features""]","model_lstm = ResidualWrapper(
    Sequential([
        Conv1D(64, kernel_size=(3,), activation='relu', padding='same'),
        MaxPooling1D(2, strides=1, padding='same'),
        Bidirectional(LSTM(32, return_sequences=True)),
        Bidirectional(LSTM(32, return_sequences=True)),
        Dense(2048, activation='relu'),
        Dropout(0.2),
        Dense(512, activation='relu'),
        Dense(128, activation='relu'),
        Dense(n_features),
    ])
)

model_lstm.compile(loss=smape,
                 optimizer=Adam(lr=lr),
                 metrics=[smape])

history = model_lstm.fit(window.train,
                       epochs=max_epochs,
                       validation_data=window.val,
                       callbacks=[callback])",DL,Tensorflow,aletbm/forecasting-sales-lstm-multi-output-model,https://www.kaggle.com/aletbm/forecasting-sales-lstm-multi-output-model
playground-series-s3e19,regression,continuous-regression,The task is to predict the number of items sold for various fictitious learning modules across different countries and stores for the year 2022.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset consists of synthetic sales data for various fictitious learning modules from different Kaggle-branded stores in various countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting sales. The dataset captures real-world effects such as seasonality, weekends, and holidays.",['num_sold'],"[""convert date column to datetime format"", ""extract year, month, day, and day of week from date"", ""create seasonal features using sine and cosine transformations"", ""label encode categorical variables"", ""merge additional economic data"", ""account for holidays in the dataset""]",model.fit(),DL,Tensorflow,lastdance9/using-75-prophets-rank-119,https://www.kaggle.com/lastdance9/using-75-prophets-rank-119
playground-series-s3e19,regression,continuous-regression,The task is to predict the number of items sold for various fictitious learning modules across different countries and stores for the year 2022.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset consists of synthetic sales data for various fictitious learning modules from different Kaggle-branded stores in real countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting sales. The dataset captures effects like seasonality and holidays, simulating real-world sales patterns.",['num_sold'],"[""convert 'date' to datetime"", ""create features from 'date' including year, month, day, and day of week"", ""one-hot encode categorical variables for country, store, and product"", ""split data into training and testing sets"", ""reshape data for model input""]","model = tf.keras.Sequential()
model.add(layers.LSTM(128))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dropout(0.2))
model.add(layers.Dense(1))
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss=[""mse""], metrics = [smape])
model.fit(X_train_NN, y_train_NN, epochs=30, batch_size=128, validation_data=(X_test_NN, y_test_NN))",DL,Tensorflow,defdet/sales-prediction-with-nn,https://www.kaggle.com/defdet/sales-prediction-with-nn
playground-series-s3e19,regression,continuous-regression,The task is to predict the number of items sold for various fictitious learning modules across different countries and stores for the year 2022.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset consists of synthetic sales data for various fictitious learning modules from different Kaggle-branded stores in real countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures effects seen in real-world data, such as seasonality and holiday effects, and is structured in CSV format with a size of 12.79 MB.",['num_sold'],"[""convert date to datetime format"", ""extract month, day, year, and day of the week from date"", ""add holiday information"", ""encode categorical features using mapping and one-hot encoding"", ""normalize numerical features using min-max scaling""]","model=tf.keras.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape=(34,)))
model.add(tf.keras.layers.Dense(64,activation='relu',activity_regularizer=tf.keras.regularizers.l1(0.25)))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(64,activation='relu',activity_regularizer=tf.keras.regularizers.l1(0.25)))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(10,activation='relu',activity_regularizer=tf.keras.regularizers.l1(0.15)))
model.add(tf.keras.layers.Dense(1,activation='linear',activity_regularizer=tf.keras.regularizers.l1(0.09)))
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=""mean_absolute_percentage_error"",metrics=[smape_loss])
history=model.fit(train_tf.batch(96),epochs=300,callbacks=[early_stopping,checkpoint,lr],validation_data= val_tf.batch(96),use_multiprocessing=True)",DL,Tensorflow,realshaktigupta/forecasting-competition,https://www.kaggle.com/realshaktigupta/forecasting-competition
playground-series-s3e19,regression,continuous-regression,The task is to predict the number of items sold for various fictitious learning modules across different stores and countries for the year 2022.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset consists of synthetic sales data for various fictitious learning modules from different Kaggle-branded stores in real countries. It includes training and test sets with sales data for each date-country-store-item combination, capturing effects like seasonality and holidays. The training set is used to build the model, while the test set is for making predictions.",['num_sold'],"[""convert date column to datetime"", ""drop unnecessary columns"", ""apply one-hot encoding to categorical variables"", ""scale numerical features using standard scaler""]","model = NeuralNet(
        module=LSTM,
        batch_size=32,
        max_epochs=max_epochs,
        module__input_size=len(feat),
        module__hidden_size=30,
        module__num_layers=1,
        module__output_size=1,
        module__dropout_prob=0.50,
        module__device=device,
        criterion=nn.L1Loss,
        optimizer=optim.AdamW,
        lr=1e-3,
        callbacks=[
            ('lr_scheduler',
             LRScheduler(policy=ReduceLROnPlateau,
                         mode='min',
                         factor=0.1,
                         patience=10,
                         verbose=True)),
            early_stopping
        ],
        device=device,
        train_split=predefined_split(X_val),
        verbose=0,
    )
    model.fit(train_dataset)",DL,Pytorch,tetsutani/ps3e19-eda-ensemble-ml-pipeline-rnn-by-skorch,https://www.kaggle.com/tetsutani/ps3e19-eda-ensemble-ml-pipeline-rnn-by-skorch
playground-series-s3e19,regression,continuous-regression,The task is to predict the number of units sold for various fictitious learning modules across different stores and countries for the year 2022.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,The dataset consists of synthetic sales data for various fictitious learning modules from different Kaggle-branded stores in various countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting sales. The dataset captures real-world effects such as seasonality and holiday impacts.,['num_sold'],"[""create new features for month, day, and year"", ""label encode categorical variables for store, product, and country"", ""split the dataset into training and validation sets""]","self.network = torch.nn.Sequential(
        torch.nn.Linear(self.input_features,32),
        torch.nn.ReLU(),
        torch.nn.Linear(32,64),  
        torch.nn.ReLU(),
        torch.nn.Linear(64,self.output_features)
        
        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate,momentum=0.9)
        return optimizer

trainer.fit(model,data_module.train_dataloader(),data_module.val_dataloader())",DL,Pytorch,averma111/pytorch-pss3e19,https://www.kaggle.com/averma111/pytorch-pss3e19
playground-series-s3e19,regression,continuous-regression,The task is to predict the number of sales for various fictitious learning modules from different stores in different countries for the year 2022.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,The dataset consists of synthetic sales data for various items sold in fictitious Kaggle-branded stores across different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting sales. The dataset captures real-world effects such as seasonality and holiday impacts.,['num_sold'],"[""convert date to datetime format"", ""extract month from date"", ""normalize sales data""]","model = nn.Linear(input_dim, output_dim)
model.compile(optimizer='sgd', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=100, batch_size=32)",DL,Pytorch,ayushs9020/re-creating-sgd-regressor-from-scratch,https://www.kaggle.com/ayushs9020/re-creating-sgd-regressor-from-scratch
playground-series-s3e19,regression,continuous-regression,The task is to predict the number of items sold for various fictitious learning modules across different stores and countries for the year 2022.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,The dataset consists of synthetic sales data for various fictitious learning modules from different Kaggle-branded stores in various countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting sales. The dataset captures real-world effects such as seasonality and holiday impacts.,['num_sold'],"[""convert date column to datetime"", ""extract day of week from date"", ""extract month from date"", ""extract year from date"", ""create lagged features for num_sold"", ""apply Min-Max scaling to features""]","model = LSTMModel(input_size=NUM_FEATURES, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS).to(device)
criterion = nn.MSELoss()
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
for epoch in range(EPOCHS):
    train_loss = train_model(model, train_loader, optimizer, criterion, device)
    test_loss = evaluate_model(model, test_loader, criterion, device)",DL,Pytorch,muhammadrizqiassabil/kaggle-pleygron-lstm,https://www.kaggle.com/muhammadrizqiassabil/kaggle-pleygron-lstm
playground-series-s3e16,regression,continuous-regression,The task is to predict the age of crabs based on their physical attributes.,Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Crab Age Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes various physical attributes of crabs, and the target variable is the age of the crabs. The test dataset is used to predict the age, which can be output as either integer or float. Additional synthetic data can be generated for exploration and model performance improvement.",['Age'],"[""calculate volume from Height, Diameter, and Length"", ""calculate dimensions based on Height, Diameter, and Length"", ""calculate total weight from Shell, Viscera, and Shucked weights"", ""calculate weight to volume ratio"", ""calculate area of the crab"", ""calculate weight to area ratio"", ""calculate water weight"", ""drop the id column"", ""one-hot encode the Sex feature"", ""standardize the feature values""]","model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='linear')\n])\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\nmodel.fit(X_train, y_train, epochs=100)",DL,Tensorflow,prthmgoyl/deeplearningmodel,https://www.kaggle.com/prthmgoyl/deeplearningmodel
playground-series-s3e16,regression,continuous-regression,The task is to predict the age of crabs based on their physical attributes.,Tabular,MAE – Mean Absolute Error,"The dataset consists of synthetic data generated from a deep learning model trained on the Crab Age Prediction dataset. It includes both training and test datasets, with the target variable being the age of the crabs. The training dataset contains various physical attributes of the crabs, while the test dataset is used to evaluate the model's predictions. The dataset is in CSV format and has a size of 9.05 MB.",['Age'],"[""drop unnecessary columns"", ""scale features using MinMaxScaler"", ""handle outliers using z-scores""]","model = keras.models.Sequential([
    keras.layers.Dense(2000 , activation = ""relu"" , input_shape = X_train.shape[1:]),
    keras.layers.Dense(1790 , activation = ""relu""),
    keras.layers.Dense(1200 , activation = ""relu""),
    keras.layers.Dense(600 , activation = ""relu""),
    keras.layers.Dense(100 , activation = ""relu""),
    keras.layers.Dense(1) 
])
model.compile(optimizer='adam', loss='mean_absolute_error', metrics=[metrics.MeanAbsoluteError()])
history = model.fit(X_train , Y_train , epochs =100 , validation_data=(X_vaild , Y_vaild))",DL,Tensorflow,aakashjoshi123/crab-weight,https://www.kaggle.com/aakashjoshi123/crab-weight
playground-series-s3e16,regression,continuous-regression,The task is to predict the age of crabs based on their physical attributes.,Tabular,MAE – Mean Absolute Error,"The dataset consists of synthetic data generated from the Crab Age Prediction dataset. It includes features such as Sex, Length, Diameter, Height, Weight, Shucked Weight, Viscera Weight, Shell Weight, and the target variable Age. The training dataset is provided in train.csv, while the test dataset is in test.csv. The objective is to predict the Age of crabs, which is measured in months.",['Age'],"[""drop the 'id' column from train and test datasets"", ""concatenate synthetic and original training data"", ""apply MinMax scaling to numerical features"", ""apply one-hot encoding to categorical features"", ""split the data into training and testing sets""]","model = keras.Sequential([layers.Dense(units=10, activation='relu', input_dim=X_train.shape[1]),layers.Dense(200,activation='relu'),layers.Dense(200,activation='relu'),layers.Dense(200,activation='relu'),layers.Dense(1)])
model.compile(optimizer='adam',loss='mse',metrics=[tf.keras.metrics.RootMeanSquaredError()])
house_price = model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=516,epochs=200,callbacks=[early_stoping],verbose=0)",DL,Tensorflow,sachinpatil1280/crab-age-prediction-deep-learning-1-35,https://www.kaggle.com/sachinpatil1280/crab-age-prediction-deep-learning-1-35
playground-series-s3e16,regression,continuous-regression,"The task is to predict the age of crabs based on their physical attributes such as length, diameter, height, and weight.",Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Crab Age Prediction dataset. It includes features like length, diameter, height, weight, and age of crabs. The training dataset has the target variable 'Age', while the test dataset is used to predict the age of crabs. The dataset is in CSV format and has no missing values.",['Age'],"[""convert categorical column 'Sex' to numerical"", ""split the data into training and validation sets"", ""standardize the numeric columns""]","nn_model = keras.Sequential([layers.Input(shape=(None, TRAIN_COLS - 1)),layers.Dense(6,activation='relu'),layers.Dense(6,activation='relu'),layers.Dense(1),]);nn_model.compile(optimizer='adam', loss=""mae"");nn_model.fit(train_ds, validation_data=val_ds, epochs=100, callbacks=nn_model_callbacks);",DL,Tensorflow,varunguttikonda/regression-with-crab-age-dataset,https://www.kaggle.com/varunguttikonda/regression-with-crab-age-dataset
playground-series-s3e16,regression,continuous-regression,The task is to predict the age of crabs based on their physical attributes.,Tabular,MAE – Mean Absolute Error,"The dataset consists of synthetic data generated from a deep learning model trained on the Crab Age Prediction dataset. It includes training and test datasets, where the target variable is the age of the crabs. The training dataset contains features related to the crabs' physical attributes, and the test dataset is used to evaluate the model's predictions. The dataset is in CSV format and has a size of 9.05 MB.",['Age'],"[""drop unnecessary columns"", ""encode categorical variables using label encoding"", ""split the dataset into training and testing sets""]","model = tf.keras.Sequential([\n    tf.keras.layers.Dense(256, activation=tf.keras.activations.gelu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.keras.activations.gelu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64, activation=tf.keras.activations.gelu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(32, activation=tf.keras.activations.gelu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(16, activation=tf.keras.activations.gelu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(8, activation=tf.keras.activations.gelu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='linear')\n]);\n\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.006), metrics=[tf.keras.metrics.MeanAbsoluteError()]);\n\nhistory = model.fit(X_train, y_train, epochs=500, callbacks=[early_stopping, reduce_lr], validation_data=(X_test, y_test));",DL,Tensorflow,satyaprakashshukl/neural-network-ps-crab-series-3-e-16,https://www.kaggle.com/satyaprakashshukl/neural-network-ps-crab-series-3-e-16
playground-series-s3e16,regression,continuous-regression,The task is to predict the age of crabs based on their physical attributes using regression techniques.,Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Crab Age Prediction dataset. It includes features such as Length, Diameter, Height, Weight, Shell Weight, Shucked Weight, Viscera Weight, and a categorical feature Sex. The target variable is Age, which needs to be predicted. The training dataset is in train.csv, while the test dataset is in test.csv, and a sample submission format is provided in sample_submission.csv.",['Age'],"[""encode categorical variables using LabelEncoder"", ""standardize numerical features using StandardScaler""]","model = CrabAgeNN(X.shape[1]).to(device)
criterion = nn.L1Loss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)
model.train()  # Set model to training mode
train_loss = train_model(model, train_loader, criterion, optimizer, scheduler, device)",DL,Pytorch,ksmooi/predicting-crab-age-seblock-glu-resnet,https://www.kaggle.com/ksmooi/predicting-crab-age-seblock-glu-resnet
playground-series-s3e16,regression,continuous-regression,The task is to predict the age of crabs based on their physical attributes using regression techniques.,Tabular,MAE – Mean Absolute Error,"The dataset consists of synthetic data generated from a deep learning model trained on the Crab Age Prediction dataset. It includes both training and test datasets, with the target variable being the age of the crabs. The training dataset is provided in train.csv, while the test dataset is in test.csv. A sample submission file is also included to guide the format of the predictions. The dataset is in CSV format and has a size of 9.05 MB.",['Age'],"[""drop the 'id' column from train and test datasets"", ""add a column to indicate if the data is generated"", ""apply ordinal encoding to categorical features"", ""standardize numerical features using StandardScaler"", ""drop unnecessary columns after encoding""]","model = TabNetRegressor(**tabnet_params)
model.fit(X_train=X_train.values,
          y_train=y_train.values.reshape(-1,1),
          eval_set=[(X_val.values, y_val.values.reshape(-1,1))],
          eval_name = [""val""],
          eval_metric = ['mae'],
          max_epochs=MAX_EPOCH,
          patience=20, batch_size=BATCH_SIZE,
          num_workers=1, drop_last=False)",DL,Pytorch,drmwnnrafi/ps3e16-crab-age-tabnet,https://www.kaggle.com/drmwnnrafi/ps3e16-crab-age-tabnet
playground-series-s3e16,regression,continuous-regression,"The task is to predict the age of crabs based on their physical attributes such as length, diameter, height, and weight.",Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Crab Age Prediction dataset. It includes features like sex, length, diameter, height, weight, shucked weight, viscera weight, and shell weight. The target variable is the age of the crab. The training dataset is provided in train.csv, while the test dataset is in test.csv. A sample submission file is also included to guide the format of predictions.",['Age'],"[""one-hot encode categorical variables"", ""standardize numerical features"", ""split data into training and validation sets""]","model = CrabNNet(X_train.shape[1],1)
model.to(device)

optimizer = config.optim_func(model.parameters(), config.learning_rate, momentum=config.momentum)

history = train.fit(config.epochs, config.learning_rate, model, train_dl, val_dl, config.optim_func, config.momentum)",DL,Pytorch,averma111/pytorch-crabage-s3e16,https://www.kaggle.com/averma111/pytorch-crabage-s3e16
playground-series-s3e16,regression,continuous-regression,The task is to predict the age of crabs based on their physical attributes.,Tabular,MAE – Mean Absolute Error,"The dataset consists of synthetic data generated from a deep learning model trained on the Crab Age Prediction dataset. It includes training and test datasets where the target variable is the age of crabs. The training dataset contains features related to the physical attributes of crabs, and the test dataset is used to evaluate the model's predictions. The dataset is in CSV format and is approximately 9.05 MB in size.",['Age'],"[""generate new features such as Viscera Ratio, Shell Ratio, Surface Area, and Volume"", ""encode categorical variable 'Sex' as binary""]","model = TabNetRegressor(verbose=1, seed=42, optimizer_fn=torch.optim.Adam)
model.fit(
    X_train = X_train,
    y_train = y_train,
    eval_set=[(X_val, y_val)],
    patience=5,
    max_epochs=20,
    eval_metric=[""mae""])
pred = model.predict(test.to_numpy())",DL,Pytorch,riverallzero/basic-tabnetregressor,https://www.kaggle.com/riverallzero/basic-tabnetregressor
playground-series-s3e15,regression,missing-value-imputation,The objective is to predict the missing values of the feature x_e_out [-] (equilibrium quality).,Tabular,RMSE – Root Mean Squared Error,"The dataset for this competition consists of both training and testing data generated from a deep learning model trained on the Predicting Critical Heat Flux dataset. The feature distributions are similar but not identical to the original dataset. The main file is data.csv, which contains the features and the target variable with missing values. A sample submission file is also provided in sample_submission.csv, which outlines the required format for submissions.",['x_e_out [-]'],"[""median-impute missing values"", ""standardize numerical features"", ""one-hot encode categorical features""]","model = keras.Sequential([layers.Dense(128, activation='relu', input_shape=(input_shape,)),layers.Dense(64, activation='relu'),layers.Dense(1)])
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, validation_split=0.2)",DL,Tensorflow,mateuszk013/playground-series-s3e15-heat-flux-visual-eda,https://www.kaggle.com/mateuszk013/playground-series-s3e15-heat-flux-visual-eda
playground-series-s3e15,regression,missing-value-imputation,The task is to predict the missing values of the feature x_e_out [-] based on other available features in the dataset.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of processed records of experimental critical heat flux and boundary conditions. It includes features such as pressure, mass flux, and dimensions related to heat flux experiments. The objective is to impute missing values for the feature x_e_out [-].",['x_e_out [-]'],"[""impute missing values using an iterative imputer"", ""scale features using robust scaling"", ""one-hot encode categorical variables""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_dim,)))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(Dense(64, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=100, batch_size=32)",DL,Tensorflow,ashishkumarak/feature-imputation-with-heat-flux-dataset,https://www.kaggle.com/ashishkumarak/feature-imputation-with-heat-flux-dataset
playground-series-s3e15,regression,missing-value-imputation,The task is to predict the missing values of the feature x_e_out based on the available data.,Tabular,RMSE – Root Mean Squared Error,"The dataset for this competition consists of generated data from a deep learning model trained on the Predicting Critical Heat Flux dataset. It includes features such as pressure, mass flux, and equilibrium quality, with the objective being to impute missing values for the target feature x_e_out. The dataset is provided in CSV format and is approximately 1.73 MB in size.",['x_e_out [-]'],"[""rename columns for clarity"", ""drop unnecessary columns"", ""fill missing values with mean"", ""standardize features using StandardScaler""]","model = keras.models.Sequential([keras.layers.Dense(2000 , activation = ""tanh"" , input_shape = X_train.shape[1:]),keras.layers.Dense(1790 , activation = ""tanh""),keras.layers.Dense(1200 , activation = ""tanh""),keras.layers.Dense(600 , activation = ""tanh""),keras.layers.Dense(100 , activation = ""tanh""),keras.layers.Dense(1)]);model.compile(loss = 'mean_squared_error' , optimizer = 'adam' , metrics = ['accuracy']);history = model.fit(X_train , Y_train , epochs = 50 , validation_data=(X_vaild , Y_vaild))",DL,Tensorflow,aakashjoshi123/heatflux,https://www.kaggle.com/aakashjoshi123/heatflux
playground-series-s3e15,regression,missing-value-imputation,The task is to predict the missing values of the feature x_e_out based on the available data.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of generated data from a deep learning model trained on the Predicting Critical Heat Flux dataset. It includes features such as pressure, mass flux, and others, with the objective of imputing missing values for the feature x_e_out. The dataset is provided in CSV format and is approximately 1.73 MB in size.",['x_e_out'],"[""rename columns for clarity"", ""interpolate missing values"", ""drop unnecessary columns"", ""split data into features and target"", ""convert DataFrame to tensor""]","model = tf.keras.Sequential([tf.keras.layers.Dense(256, activation=tf.keras.activations.gelu),tf.keras.layers.Dropout(0.2),tf.keras.layers.Dense(128, activation=tf.keras.activations.gelu),tf.keras.layers.Dropout(0.2),tf.keras.layers.Dense(64, activation=tf.keras.activations.gelu),tf.keras.layers.Dropout(0.2),tf.keras.layers.Dense(32, activation=tf.keras.activations.gelu),tf.keras.layers.Dropout(0.2),tf.keras.layers.Dense(16, activation=tf.keras.activations.gelu),tf.keras.layers.Dropout(0.2),tf.keras.layers.Dense(8, activation=tf.keras.activations.gelu),tf.keras.layers.Dropout(0.2),tf.keras.layers.Dense(1, activation='linear')]);model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate=0.006),metrics=[tf.keras.metrics.MeanAbsoluteError()]);kuchbhiha = model.fit(X_train,y_train,epochs=50,callbacks=[early_stopping,reduce_lr],validation_data=(X_test, y_test))",DL,Tensorflow,satyaprakashshukl/neural-network-feature-imputation-with-a-heat,https://www.kaggle.com/satyaprakashshukl/neural-network-feature-imputation-with-a-heat
playground-series-s3e15,regression,missing-value-imputation,The task is to predict the missing values of the feature x_e_out based on other available features in the dataset.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of generated data from a deep learning model trained on the Predicting Critical Heat Flux dataset. It includes features such as pressure, mass_flux, D_e, D_h, length, and others, with some values missing for the target feature x_e_out. The dataset is provided in CSV format and is approximately 1.73 MB in size.",['x_e_out'],"[""drop rows with null values"", ""scale features using standard scaler"", ""categorize features using label encoding""]","model_nn = keras.models.Sequential([keras.layers.Dense(50,activation='relu',kernel_regularizer='l1'),keras.layers.Dense(30,activation='relu'),keras.layers.Dense(10,activation='relu'),keras.layers.Dense(1,activation='relu')])
model_nn.compile(optimizer='adam',loss=root_mean_squared_error,metrics=['accuracy'])
model_nn.fit(nn_X_train,nn_y_train,verbose=1,epochs=100,validation_data=(nn_X_val,nn_y_val))",DL,Tensorflow,shashankjat10/heat-flux-imputation-nn-xgboost,https://www.kaggle.com/shashankjat10/heat-flux-imputation-nn-xgboost
playground-series-s3e15,regression,missing-value-imputation,The task is to predict the missing values of the feature x_e_out [-] based on other available features in the dataset.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of generated data from a deep learning model trained on the Predicting Critical Heat Flux dataset. It includes both training and testing data, with the objective of imputing missing values for the feature x_e_out [-]. The dataset is in CSV format and has a size of 1.73 MB.",['x_e_out [-]'],"[""fill missing values with mean for pressure"", ""fill missing values with mean for mass_flux"", ""fill missing values with mean for length"", ""standardize the dataset""]","model = RegressionFluxNet(trial=best_trial,in_features=X_train.shape[1],n_layers=best_trial.params['n_layers'],dropout=best_trial.params['dropout'],n_output=1).to(device)
optimizer = getattr(torch.optim, params['optimizer'])(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])
model.train()
for epoch in range(0, 5):
    # Training loop
    for i, data in enumerate(trainloader, 0):
        inputs, targets = data
        inputs, targets = inputs.float(), targets.float()
        targets = targets.reshape((targets.shape[0], 1))
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_function(outputs, targets)
        loss.backward()
        optimizer.step()",DL,Pytorch,averma111/pytorch-ps3e15-optuna,https://www.kaggle.com/averma111/pytorch-ps3e15-optuna
playground-series-s3e15,regression,missing-value-imputation,The task is to predict the missing values of the feature x_e_out [-] based on the available data.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of generated data from a deep learning model trained on the Predicting Critical Heat Flux dataset. It includes both training and testing data, with the objective of imputing missing values for the feature x_e_out [-]. The dataset is in CSV format and has a size of 1.73 MB.",['x_e_out [-]'],"[""drop the id column"", ""log-transform skewed features"", ""categorify categorical features"", ""fill missing values"", ""normalize continuous features""]","learn = tabular_learner(dls, y_range=[-0.87, 0.23], layers=[200,100], n_out=1, loss_func=F.mse_loss,metrics=r_mse)
learn.fit_one_cycle(50,2e-3)",DL,Pytorch,edwinhung/heatflux-fastai-tabularpandas,https://www.kaggle.com/edwinhung/heatflux-fastai-tabularpandas
playground-series-s3e14,regression,continuous-regression,The task is to predict the yield of wild blueberries based on various environmental and biological features.,Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Wild Blueberry Yield Prediction Dataset. It includes features such as average blueberry clone size, honeybee density, temperature ranges, and precipitation days. The target variable is the yield of blueberries. The training dataset is provided in train.csv, while the test dataset is in test.csv. A sample submission file is also included to guide the format of predictions.",['yield'],"[""drop unnecessary columns"", ""check for missing values"", ""apply MinMax scaling"", ""perform log transformation"", ""apply square root transformation"", ""apply Box-Cox transformation"", ""apply Yeo-Johnson transformation"", ""apply power transformation"", ""apply PCA on transformed features"", ""one-hot encode categorical features"", ""standardize numerical features""]","ann = Sequential()
ann.add(Dense(32, input_dim=X_train.shape[1], kernel_initializer='he_uniform', activation=""relu""))
ann.add(Dense(128,  kernel_initializer='he_uniform', activation='relu'))
ann.add(Dropout(0.6))
ann.add(Dense(64,  kernel_initializer='he_uniform', activation=""relu""))
ann.add(Dropout(0.5))
ann.add(Dense(2,  kernel_initializer='he_uniform', activation=""relu""))
ann.add(Dropout(0.1))
ann.add(Dense(1, kernel_initializer = 'he_uniform'))
ann.compile(loss=mean_abs_error, optimizer=nadam)",DL,Tensorflow,arunklenin/ps3e14-blueberry-yield-prediction-challenge,https://www.kaggle.com/arunklenin/ps3e14-blueberry-yield-prediction-challenge
playground-series-s3e14,regression,continuous-regression,The task is to predict the yield of wild blueberries based on various environmental and biological features.,Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. It includes features such as bee densities, temperature ranges, and rainfall days, with the target variable being the yield of blueberries. The training dataset contains 15289 records and 17 features, while the test dataset has 10194 records and the same number of features.",['yield'],"[""drop the 'id' column"", ""create new features based on existing ones"", ""scale numerical features using StandardScaler and RobustScaler""]","model_nn = tf.keras.models.Sequential()
model_nn.add(tf.keras.layers.Dense(n_neurons[0], activation='relu'))
for i in range(1, n_hidden_layers):
    model_nn.add(tf.keras.layers.Dense(n_neurons[i], activation='relu'))
model_nn.add(tf.keras.layers.Dense(1))
optimizer = tf.keras.optimizers.Adam(lr=best_params['learning_rate'])
model_nn.compile(loss='mse', optimizer=optimizer)
model_nn.fit(X_scaled, y, epochs=50, batch_size=32, verbose=0)",DL,Tensorflow,kishore24/eda-statistics-model-building-comparison,https://www.kaggle.com/kishore24/eda-statistics-model-building-comparison
playground-series-s3e14,regression,continuous-regression,The task is to predict the yield of wild blueberries based on various features.,Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named 'yield', while the test dataset is used to predict this target based on the provided features. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['yield'],"[""drop unnecessary columns"", ""apply feature engineering to create new features"", ""one-hot encode categorical variables"", ""split the dataset into training and testing sets""]","model = Sequential()
model.add(Dense(8, activation='relu', input_dim=4))
model.add(Dense(6, activation='softmax'))

model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer='adam', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=1000, batch_size=128)",DL,Tensorflow,yuvannabawa/wild-blueberry,https://www.kaggle.com/yuvannabawa/wild-blueberry
playground-series-s3e14,regression,continuous-regression,The task is to predict the yield of wild blueberries based on various features.,Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named 'yield', while the test dataset is used to predict the yield based on other features. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['yield'],"[""drop unnecessary columns"", ""split data into training and testing sets""]","model = ctb.CatBoostRegressor(**cb3_params)
model.fit(x_train, y_train, eval_set=[(x_test, y_test)], early_stopping_rounds=900, verbose=False)",DL,Tensorflow,andreystartup/ps3e14-catboost-optuna,https://www.kaggle.com/andreystartup/ps3e14-catboost-optuna
playground-series-s3e14,regression,continuous-regression,The task is to predict the yield of wild blueberries based on various features.,Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named 'yield', while the test dataset is used to predict the yield based on the other features. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['yield'],"[""standardize features using StandardScaler"", ""apply PCA to reduce dimensionality of correlated features""]","model = tf.keras.models.Sequential([\n    tf.keras.layers.Input(shape=(D,)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(\n    optimizer = tf.keras.optimizers.Adam(0.0001),\n    loss='mse',\n    metrics=['mae']\n)\nhistory = model.fit(X_tr, y_tr, validation_data=(X_v, y_v), epochs=100, callbacks=[scheduler])",DL,Tensorflow,ashutoshojha/playground-s3-e14-second-submission,https://www.kaggle.com/ashutoshojha/playground-s3-e14-second-submission
playground-series-s3e14,regression,continuous-regression,The task is to predict the yield of wild blueberries based on various features.,Tabular,MAE – Mean Absolute Error,The dataset consists of training and test data generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. The training dataset contains features and the target variable 'yield'. The test dataset is used to predict the yield based on the provided features. The dataset is relatively small with 15289 data points and includes a sample submission file for formatting predictions.,['yield'],"[""drop unwanted columns"", ""drop duplicates"", ""generate new features"", ""scale numerical features""]","self.network = torch.nn.Sequential(
        torch.nn.Linear(input_features,1),
        torch.nn.LeakyReLU()
        )
model.compile()",DL,Pytorch,averma111/pytorch-ps-s3e14,https://www.kaggle.com/averma111/pytorch-ps-s3e14
playground-series-s3e14,regression,continuous-regression,The task is to predict the yield of wild blueberries based on various features.,Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Wild Blueberry Yield Prediction Dataset. The training dataset includes features and the target variable 'yield', while the test dataset is used to predict the yield based on the features provided. The dataset is in CSV format and has a size of 2.99 MB.",['yield'],"[""drop the 'id' column"", ""drop specified columns that are not needed"", ""apply standard scaling to the features""]","model = NN(n_hidden_layers=len(hidden_dims), input_dim=n_features, hidden_dims=hidden_dims, output_dim=1, activation=config[""activation""], dropout_prob=config[""dropout_prob""])
optimizer = torch.optim.Adam(model.parameters(), lr=config[""lr""])
model.train()
for epoch in range(n_epochs):
    # training loop code here
    pass
",DL,Pytorch,eleonoraricci/ps3e14-berrylicious-predictions-with-pytorch,https://www.kaggle.com/eleonoraricci/ps3e14-berrylicious-predictions-with-pytorch
playground-series-s3e14,regression,continuous-regression,The task is to predict the yield of wild blueberries based on various features.,Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named 'yield', while the test dataset is used to predict the yield based on the other features. The dataset files include train.csv, test.csv, and a sample_submission.csv file.",['yield'],"[""check for null values"", ""check for duplicates"", ""split the dataset into training and validation sets""]","model = SimpleModel().to(device)
criterion = nn.L1Loss()
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-2)
scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
for epoch in range(EPOCHS):
    model.train()
    for i, (X_tr, y_tr) in enumerate(train_dataloader):
        X_tr = X_tr.to(device)
        y_tr = y_tr.to(device).reshape(-1, 1)
        y_pred = model(X_tr)
        loss = criterion(y_pred, y_tr)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    scheduler.step()",DL,Pytorch,rakgyunim/simple-pytorch-trial,https://www.kaggle.com/rakgyunim/simple-pytorch-trial
playground-series-s3e14,regression,continuous-regression,The task is to predict the yield of wild blueberries based on various features.,Tabular,MAE – Mean Absolute Error,"The dataset consists of training and test data generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. The training dataset includes features and the target variable 'yield', while the test dataset contains features for which the yield needs to be predicted. The dataset allows for exploration of differences with the original dataset and potential improvements in model performance by incorporating it.",['yield'],"[""drop the 'id' column from train and test datasets"", ""add a new feature 'fruit_seed' as the product of 'fruitset' and 'seeds'"", ""remove highly correlated features from the dataset"", ""standardize the datasets using StandardScaler""]","self.model=Regressor(self)
self.model.to(self.device)
optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.beta1,self.beta2))
criterion=torch.nn.L1Loss()
self.model.train()
for epoch in range(1,self.epochs+1):
    loss_train_epoch=self.step_epoch(optimizer,train_loader,criterion)
    loss_eval_epoch=self.step_epoch(optimizer,eval_loader,criterion,mode='val')
    print(f'Epoch:{epoch}/{self.epochs}\tloss_train_epoch:{loss_train_epoch}\tloss_eval_epoch:{loss_eval_epoch}')",DL,Pytorch,luojike/mlp-regressor,https://www.kaggle.com/luojike/mlp-regressor
playground-series-s3e14,regression,continuous-regression,The task is to predict the yield of wild blueberries based on various features.,Tabular,MAE – Mean Absolute Error,The dataset consists of training and test data generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset. The training dataset contains features and the target variable 'yield'. The test dataset is used to predict the yield based on the provided features. The dataset is relatively small with 15289 data points and includes a sample submission file for formatting predictions.,['yield'],"[""drop unwanted columns"", ""drop duplicates"", ""generate new features"", ""scale numerical features""]","self.network = torch.nn.Sequential(
        torch.nn.Linear(input_features,1),
        torch.nn.LeakyReLU()
        )
model.compile()",DL,Pytorch,dhananjaykumarravi/pytorch-ps-s3e14,https://www.kaggle.com/dhananjaykumarravi/pytorch-ps-s3e14
playground-series-s3e13,classification,multi-label-classification,The task is to predict multiple disease prognoses based on input features from the dataset.,Tabular,MAP@N – Mean Average Precision,"The dataset consists of training and test data generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named prognosis, while the test dataset is used for making predictions. The sample submission file demonstrates the required format for predictions, where spaces in prognoses are replaced with underscores.",['prognosis'],"[""drop the 'id' column"", ""encode the 'prognosis' column using label encoding"", ""drop highly correlated columns""]","rf = tfdf.keras.RandomForestModel()
rf.compile(metrics=[""accuracy""])
rf.fit(x=train_ds)",DL,Tensorflow,pyrotech/tensorflow-decision-tree-tfdf-forest-33-55,https://www.kaggle.com/pyrotech/tensorflow-decision-tree-tfdf-forest-33-55
playground-series-s3e13,classification,multi-label-classification,"The task is to predict multiple disease prognoses based on symptoms from the provided dataset, with the output formatted to include the top three predictions for each instance.",Tabular,MAP@N – Mean Average Precision,"The dataset consists of training and test data generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named prognosis, which has been modified to replace spaces with underscores. The test dataset is used to predict the prognosis for each id. The dataset is in CSV format and is approximately 284.43 kB in size.",['prognosis'],"[""replace spaces in prognosis with underscores"", ""one-hot encode prognosis as target variable"", ""concatenate original dataset with training data if included"", ""create noisy data sequence for training""]","inputs = layers.Input(shape=input_shape)
x = layers.Dropout(.2)(inputs)
x = layers.Dense(64, activation=mish)(x)
for _ in range(1):
    x = layers.BatchNormalization()(x)
    x = layers.Dense(32, activation=mish)(x)
    x = layers.Dropout(.3)(x)
outputs = layers.Dense(N_CLASSES, activation='softmax')(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=[tfr.keras.metrics.MeanAveragePrecisionMetric(topn=3)])
history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=epochs, callbacks=callbacks, verbose=0)",DL,Tensorflow,paddykb/ps-s3e13-keras-haz-prognosis,https://www.kaggle.com/paddykb/ps-s3e13-keras-haz-prognosis
playground-series-s3e13,classification,multi-label-classification,The task is to predict multiple disease prognoses for patients based on input features.,Tabular,MAP@N – Mean Average Precision,"The dataset consists of training and test data generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. The feature distributions are similar to the original dataset, but some prognoses contain underscores instead of spaces to accommodate the evaluation metric. The training dataset includes a target column named prognosis, while the test dataset is used for making predictions. The dataset is in CSV format and is approximately 284.43 kB in size.",['prognosis'],"[""drop unnecessary columns"", ""encode target variable using ordinal encoding"", ""convert target variable to categorical format"", ""split data into training and testing sets""]","model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(classes), activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stop])",DL,Tensorflow,satyaprakashshukl/neural-network-ps-series-3-e-13,https://www.kaggle.com/satyaprakashshukl/neural-network-ps-series-3-e-13
playground-series-s3e13,classification,multi-label-classification,The task is to predict multiple disease prognoses based on input features from the dataset.,Tabular,MAP@N – Mean Average Precision,"The dataset consists of training and test data generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named prognosis, while the test dataset is used for making predictions. The sample submission file provides the correct format for submissions, where spaces in prognoses are replaced with underscores.",['prognosis'],"[""encode target variable using ordinal encoding"", ""split dataset into training and validation sets""]","input_shape = [X_train.shape[1]]
model = keras.Sequential([
    layers.BatchNormalization(input_shape=input_shape),
    layers.Dense(512, activation='swish'),
    layers.BatchNormalization(),
    layers.Dropout(0.4),
    layers.Dense(512, activation='swish'),
    layers.BatchNormalization(),
    layers.Dropout(0.4),
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.4),
    layers.Dense(11, activation='softmax')
])
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
model.fit(
    X_train, y_train,
    epochs=90,
    validation_data=[X_valid, y_valid],
    callbacks = stop_training
)",DL,Tensorflow,nitleenk/tabular-vector-borne-disease,https://www.kaggle.com/nitleenk/tabular-vector-borne-disease
playground-series-s3e13,classification,multi-label-classification,The task is to predict multiple disease prognoses based on input features from the dataset.,Tabular,MAP@N – Mean Average Precision,"The dataset consists of training and test data generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. The feature distributions are similar to the original dataset, but not identical. The training dataset includes a target column named prognosis, while the test dataset is used for making predictions. The sample submission file provides the correct format for submissions, which includes an id and the predicted prognosis.",['prognosis'],"[""drop the id column from training and test datasets"", ""encode the target prognosis using ordinal encoding"", ""convert the target prognosis to categorical format"", ""split the training data into training and validation sets""]","model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(classes), activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stop])",DL,Tensorflow,samfouad/classification-with-a-tvcbd-mlm,https://www.kaggle.com/samfouad/classification-with-a-tvcbd-mlm
playground-series-s3e13,classification,multi-label-classification,The task is to predict multiple disease prognoses based on input features from the dataset.,Tabular,MAP@N – Mean Average Precision,"The dataset consists of training and test data generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named prognosis, which contains disease names. The test dataset is used to make predictions for the prognosis. The original dataset can be utilized for comparison and potential performance improvement. The prognosis names in the competition dataset have underscores instead of spaces to comply with the evaluation metric.",['prognosis'],"[""replace spaces in prognosis with underscores"", ""map disease names to integer values"", ""concatenate training data with original dataset"", ""drop unnecessary columns"", ""reset index for training and test sets""]","model = TabNetClassifier(**tabnet_params)
model.fit(X_train=X_train_.values, y_train=y_train_.values,
          eval_name=[""train"", ""valid""], eval_metric=[""logloss""],
          eval_set=[(X_train_.values, y_train_.values), (X_val.values, y_val.values)],
          batch_size=batch_size,
          max_epochs=max_epochs,
          from_unsupervised=pretrainer)",DL,Pytorch,tetsutani/ps3e13-tabnet-baseline-compare-xgb-lgbm-cat,https://www.kaggle.com/tetsutani/ps3e13-tabnet-baseline-compare-xgb-lgbm-cat
playground-series-s3e13,classification,multi-label-classification,The task is to predict multiple disease prognoses based on given symptoms from the dataset.,Tabular,MAP@N – Mean Average Precision,"The dataset consists of training and test data generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named prognosis, while the test dataset requires predictions for the same. The prognosis values in the dataset have underscores instead of spaces to comply with the evaluation metric.",['prognosis'],"[""label encode the target variable"", ""convert data to tensors"", ""split data into training and validation sets"", ""create a DataLoader for the training dataset""]","model = NeuralNet(input_features, num_classes, hidden_size = 64, num_hidden_layers = 1)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    for batch_X, batch_Y in train_dataloader:
        optimizer.zero_grad()
        output = model(batch_X)
        loss = criterion(output, batch_Y)
        loss.backward()
        optimizer.step()",DL,Pytorch,d3stron/disease-prediction-using-pytorch-and-neural-nets,https://www.kaggle.com/d3stron/disease-prediction-using-pytorch-and-neural-nets
playground-series-s3e13,classification,multi-label-classification,The task is to predict multiple disease prognoses based on input features from the dataset.,Tabular,MAP@N – Mean Average Precision,"The dataset consists of training and test data generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. The feature distributions are similar to the original dataset, but not identical. The training dataset includes a target column named prognosis, while the test dataset is used to make predictions. The prognosis values in the dataset have underscores instead of spaces to comply with the evaluation metric requirements. The dataset is provided in CSV format and is approximately 284.43 kB in size.",['prognosis'],"[""map prognosis labels to integers"", ""drop unnecessary columns from the dataset""]","self.model = nn.Sequential(
            nn.Linear(64,128), 
            nn.Linear(128,64),
            nn.Linear(64,128),
            nn.Linear(128,64), 
            nn.Linear(64,len(class_names))
        )

optimizer = optim.Adam(self.parameters(), lr=1e-3)
trainer.fit(model,trainmodule)",DL,Pytorch,stpeteishii/pss3-ep13-pytorch-lightning-linear,https://www.kaggle.com/stpeteishii/pss3-ep13-pytorch-lightning-linear
playground-series-s3e13,classification,multi-label-classification,The task is to predict multiple disease prognoses based on input features from the dataset.,Tabular,MAP@N – Mean Average Precision,"The dataset consists of training and test data generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named prognosis, which contains multiple disease labels separated by underscores. The test dataset is used to predict these prognoses. The dataset files include train.csv for training, test.csv for testing, and sample_submission.csv for submission format guidance.",['prognosis'],"[""drop the id column"", ""convert features to float32"", ""encode labels using LabelEncoder""]","self.hidden1 = torch.nn.Linear(n_inputs, 128)
self.hidden2 = torch.nn.Linear(128, 32)
self.hidden3 = torch.nn.Linear(32, 1)
model.compile(optimizer='adam', loss='mse')
history = model.fit(train_dl, epochs=1000)",DL,Pytorch,averma111/ps3e13-map-k-pytorch,https://www.kaggle.com/averma111/ps3e13-map-k-pytorch
playground-series-s3e11,regression,continuous-regression,The task is to predict the cost associated with media campaigns based on various features.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and test sets generated from a deep learning model trained on a media campaign cost prediction dataset. The training set includes a target variable 'cost' that needs to be predicted, while the test set is used for evaluation. The dataset is in CSV format and has a size of 49.83 MB, with a license under Attribution 4.0 International (CC BY 4.0).",['cost'],"[""log1p transformation of the target variable"", ""one-hot encoding of categorical features"", ""standard scaling of features""]","inputs = Input(shape=(n_inputs, ))
x0 = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(8e-6), activation='relu')(inputs)
x1 = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(8e-6), activation='relu')(x0)
x2 = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(8e-6), activation='relu')(x1)
x3 = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(2e-6))(x2)
x = Dense(1, kernel_regularizer=tf.keras.regularizers.l2(2e-6))(x3)
regressor = Model(inputs, x)
regressor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1/128), loss=tf.keras.losses.MeanSquaredError())
history = model.fit(X_tr, y_tr, validation_data=(X_va, y_va), epochs=1000, batch_size=1024, shuffle=True, callbacks=callbacks)",DL,Tensorflow,ambrosm/pss3e11-zoo-of-models,https://www.kaggle.com/ambrosm/pss3e11-zoo-of-models
playground-series-s3e11,regression,continuous-regression,The task is to predict the cost of media campaigns based on various features related to store sales and customer demographics.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and test sets generated from a deep learning model trained on a media campaign cost prediction dataset. It includes features such as store sales, unit sales, and various demographic factors. The training dataset contains the target variable 'cost', which represents the cost of acquiring customers. The test dataset is used to evaluate the model's predictions. The dataset is in CSV format and has a size of 49.83 MB.",['cost'],"[""combine original and playground datasets"", ""drop unnecessary columns"", ""replace specific values in unit_sales"", ""perform feature engineering to create new features"", ""scale the store_sqft column using MinMaxScaler"", ""split the dataset into training and testing sets""]","nn_model = tf.keras.models.Sequential([
    tf.keras.Input(shape=X_train.shape[1:]),
    tf.keras.layers.Dense(units=64, activation='relu', name=""L1""),
    tf.keras.layers.Dense(units=64, activation='relu', name=""L2""),
    tf.keras.layers.Dense(units=32, activation='relu', name=""L3""),
    tf.keras.layers.Dense(units=1, activation='linear', name='Output')
],name=""simple"")
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
loss = tf.keras.losses.mean_squared_error
nn_model.compile(optimizer=optimizer, loss=loss, metrics=[Rmsle])
history = nn_model.fit(X, y, validation_split=0.2, batch_size=32, epochs=100, verbose=0)",DL,Tensorflow,mohammadrazeghi/playgrounds3e11-simple-eda-regressionmodeling,https://www.kaggle.com/mohammadrazeghi/playgrounds3e11-simple-eda-regressionmodeling
playground-series-s3e11,regression,continuous-regression,The task is to predict the cost based on the provided features in the dataset.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and test data generated from a deep learning model trained on a media campaign cost prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named 'cost', while the test dataset is used for making predictions. A sample submission file is also provided to illustrate the required format for submissions.",['cost'],"[""remove outliers from the target variable""]","model.compile(optimizer='adam', loss='mean_squared_log_error')
model.fit(x_train, y_train, epochs=100, batch_size=32)",DL,Tensorflow,mpwolke/rmsle-constant-and-random-predictions,https://www.kaggle.com/mpwolke/rmsle-constant-and-random-predictions
playground-series-s3e11,regression,continuous-regression,The task is to predict the cost based on various features in the dataset.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and test sets generated from a deep learning model trained on a media campaign cost prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named cost, while the test dataset is used for making predictions. A sample submission file is also provided to illustrate the required format for submissions.",['cost'],"[""clip unit_sales to a maximum of 5"", ""calculate children_ratio as total_children divided by num_children_at_home"", ""replace infinite values in children_ratio with 10"", ""fill missing values in children_ratio with 0"", ""drop unnecessary columns from the dataset"", ""scale features using StandardScaler""]","model1 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(32,activation='relu',input_shape = [X_train.shape[1]]),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(64,activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(32,activation = 'relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(8,activation = 'relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(8,activation = 'relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(8,activation = 'relu'),
    tf.keras.layers.Dense(1)
])

model1.compile(optimizer=opti,
              
             loss = 'msle')

history =model1.fit(X_train,y_train,
                  validation_data=(X_valid,y_valid),
                  batch_size=512,
                  epochs=100)",DL,Tensorflow,sahilkumar101/ps3-e11-nn,https://www.kaggle.com/sahilkumar101/ps3-e11-nn
playground-series-s3e11,regression,continuous-regression,The task is to predict the cost associated with customer acquisition based on various features.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and test sets generated from a deep learning model trained on a media campaign cost prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target variable 'cost' which is the value to be predicted, while the test dataset is used for making predictions. The dataset is in CSV format and has a size of 49.83 MB, licensed under Attribution 4.0 International (CC BY 4.0).",['cost'],"[""drop the 'salad_bar' variable"", ""scale the data using MinMaxScaler""]","XGB_md = XGBRegressor(tree_method = 'gpu_hist', max_depth = 7, n_estimators = 800, learning_rate = 0.01).fit(X_train, Y_train)
XGB_pred = XGB_md.predict(X_test)",DL,Tensorflow,timothylincoln2/s3-e11-ps-cost-prediction-with-eda,https://www.kaggle.com/timothylincoln2/s3-e11-ps-cost-prediction-with-eda
playground-series-s3e11,regression,continuous-regression,The task is to predict the cost associated with media campaigns based on various features.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and test sets generated from a deep learning model trained on the Media Campaign Cost Prediction dataset. The training set includes features and the target variable 'cost', while the test set is used for making predictions. The dataset is in CSV format and has a size of 49.83 MB, with a license under Attribution 4.0 International (CC BY 4.0).",['cost'],"[""drop the 'id' column"", ""calculate 'child_report_ratio' as the ratio of 'num_children_at_home' to 'total_children'"", ""fill missing values in 'child_report_ratio' with 0"", ""calculate 'revenue_per_store' as the ratio of 'store_sales(in millions)' to 'unit_sales(in millions)'"", ""drop unnecessary columns from the training and test sets"", ""scale features using StandardScaler"", ""split the dataset into training and testing sets""]","model = NN(n_hidden_layers = 3, input_dim = n_features, hidden_dims = [100, 100, 100], output_dim = 1, activation = nn.ReLU())
loss_function = RMSLE()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
for epoch in range(n_epochs):
    outputs = model(inputs)
    loss = loss_function(outputs.squeeze(), labels)
    optimizer.step()",DL,Pytorch,eleonoraricci/pgs-s3e11-pytorch-implementation,https://www.kaggle.com/eleonoraricci/pgs-s3e11-pytorch-implementation
playground-series-s3e11,regression,continuous-regression,The task is to predict the cost value for each id in the test set based on the provided training data.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of a training set and a test set generated from a deep learning model trained on a media campaign cost prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named cost, while the test dataset is used to predict the cost values. The dataset is in CSV format and has a size of 49.83 MB, licensed under Attribution 4.0 International (CC BY 4.0).",['cost'],"[""fill missing categorical values with 'VV_likely'"", ""median-impute missing numerical values"", ""encode categorical features using label encoding"", ""split dataset into training and validation sets""]","regressor = TabNetRegressor(**final_params)
regressor.fit(X_train=X_train, y_train=y_train,
              eval_set=[(X_train, y_train), (X_valid, y_valid)],
              eval_name=['train', 'valid'],
              eval_metric=['rmsle', 'mae', 'rmse', 'mse'],
              patience=TabNet_params['patience'], max_epochs=epochs,
              batch_size=1024, virtual_batch_size=128,
              drop_last=False,
              augmentations=aug)",DL,Pytorch,dongkyumoon/tabnet-with-optuna,https://www.kaggle.com/dongkyumoon/tabnet-with-optuna
playground-series-s3e9,regression,continuous-regression,The task is to predict the concrete strength based on various input features.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set and a test set generated from a deep learning model trained on the Concrete Strength Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named Strength, while the test dataset is used to predict the Strength values. Additional data can be incorporated to explore differences and improve model performance.",['Strength'],"[""impute missing values"", ""drop unnecessary columns"", ""scale features using robust scaler"", ""create new feature ratios""]","model = tf.keras.Sequential([
  tf.keras.layers.Dense(64, activation=""selu""),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(32, activation=""selu""),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(16, activation=""selu""),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(8),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1, activation='linear')
])

model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[keras.metrics.RootMeanSquaredError()])

history = model.fit(X_train, y_train, epochs=100, callbacks=[early_stopping, reduce_lr], validation_data=(X_valid, y_valid), verbose=1)",DL,Tensorflow,alexandershumilin/ps-s3-e9-ensemble-model,https://www.kaggle.com/alexandershumilin/ps-s3-e9-ensemble-model
playground-series-s3e9,regression,continuous-regression,The task is to predict the concrete strength based on various input features.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set and a test set generated from a deep learning model trained on the Concrete Strength Prediction dataset. The feature distributions are similar to the original dataset. The training dataset includes a target column named Strength, while the test dataset is used to predict the Strength values. A sample submission file is provided to illustrate the required format for predictions.",['Strength'],"[""remove duplicate entries"", ""drop unnecessary columns"", ""apply feature engineering to create new features""]","model = tf.keras.Sequential([Input(shape=(input_shape,)), Dense(128, activation='relu'), Dropout(0.2), Dense(64, activation='relu'), Dense(1)])
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=100, batch_size=32)",DL,Tensorflow,samuraikaggle/s3e9ens-optuna-cb-xb-lb,https://www.kaggle.com/samuraikaggle/s3e9ens-optuna-cb-xb-lb
playground-series-s3e9,regression,continuous-regression,The task is to predict the concrete strength based on various input features.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set and a test set generated from a deep learning model trained on the Concrete Strength Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named Strength, while the test dataset is used to predict the Strength values. A sample submission file is provided to illustrate the required format for predictions.",['Strength'],"[""drop the 'id' column from training and test datasets"", ""scale features using StandardScaler"", ""split the training data into training and validation sets""]","model = tf.keras.Sequential([\n  tf.keras.layers.Dense(256, activation=""selu""),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(128, activation=""selu""),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(64, activation=""elu""),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(32, activation=""selu""),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(16, activation=""elu""),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(8),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(1,activation='linear')    \n]);\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.006));\nkuchbhiha = model.fit(X_train,y_train,epochs=100,callbacks=[early_stopping,reduce_lr],validation_data=(X_test, y_test));",DL,Tensorflow,satyaprakashshukl/improved-neural-network-ps-series-3-e-9,https://www.kaggle.com/satyaprakashshukl/improved-neural-network-ps-series-3-e-9
playground-series-s3e9,regression,continuous-regression,The task is to predict the concrete strength based on various input features.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test data generated from a deep learning model trained on the Concrete Strength Prediction dataset. The feature distributions are similar but not identical to the original dataset. The training dataset includes a target column named Strength, while the test dataset is used to predict the Strength values. A sample submission file is provided to illustrate the required format for predictions.",['Strength'],"[""remove the id column from the dataset"", ""standardize the feature values"", ""split the training data into training and validation sets""]","keras_model = tf.keras.Sequential([tf.keras.layers.Dense(64, activation='relu'),tf.keras.layers.Dense(32, activation='relu'),tf.keras.layers.Dense(1)])
criterion = tf.keras.losses.MeanSquaredError()
optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)
keras_model.compile(optimizer=optimizer,loss=criterion, metrics=metrics)
history = keras_model.fit(train_X, train_y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(val_X, val_y))",DL,Tensorflow,masatakasuzuki/nn-model-starter-keras-pytorch,https://www.kaggle.com/masatakasuzuki/nn-model-starter-keras-pytorch
playground-series-s3e9,regression,continuous-regression,The task is to predict the concrete strength based on various input features.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test sets generated from a deep learning model trained on the Concrete Strength Prediction dataset. The training dataset includes features such as CementComponent, BlastFurnaceSlag, FlyAshComponent, WaterComponent, SuperplasticizerComponent, CoarseAggregateComponent, FineAggregateComponent, AgeInDays, and the target variable Strength. The test dataset is used to predict the Strength values. The dataset is in CSV format and has no missing values.",['Strength'],"[""scale features using StandardScaler"", ""drop unnecessary columns"", ""split dataset into training and validation sets""]","model = Sequential()
model.add(Dense(128, input_shape=X_train.shape[1:], activation='relu', kernel_initializer='normal'))
model.add(Dense(64, activation='relu', kernel_initializer='normal'))
model.add(Dense(1, activation=None, kernel_initializer='normal'))
model.compile(optimizer=Adam(lr=0.01), loss='mean_squared_error', metrics=['mean_absolute_error'])
model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=[EarlyStopping(monitor='val_loss', patience=3)])",DL,Tensorflow,nikoolaylovyagin/playground-series-regressor,https://www.kaggle.com/nikoolaylovyagin/playground-series-regressor
playground-series-s3e9,regression,continuous-regression,The task is to predict the concrete strength based on various input features.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set and a test set generated from a deep learning model trained on the Concrete Strength Prediction dataset. The training dataset includes a target column named Strength, while the test dataset is used to predict the Strength values. The dataset files are in CSV format and include train.csv, test.csv, and a sample submission file.",['Strength'],"[""add generated indicator column for train and test datasets"", ""concatenate original dataset with training data"", ""create new feature Water_Cement by dividing WaterComponent by CementComponent"", ""create new feature Coarse_Fine by dividing CoarseAggregateComponent by FineAggregateComponent"", ""create new feature Aggregate by summing CoarseAggregateComponent and FineAggregateComponent"", ""create new feature Aggregate_Cement by dividing Aggregate by CementComponent"", ""create new feature Slag_Cement by dividing BlastFurnaceSlag by CementComponent"", ""create new feature Ash_Cement by dividing FlyAshComponent by CementComponent"", ""create new feature Plastic_Cement by dividing SuperplasticizerComponent by CementComponent"", ""create new feature Age_Water by dividing AgeInDays by WaterComponent""]","model = Reg()
loss_fn = nn.MSELoss()
optim =  torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))
model.to(device)
for i in range(50):
    for x_batch, y_batch in train_loader:
        x_batch = x_batch.view(x_batch.shape[0], -1).to(device)
        y_batch = y_batch.type(torch.FloatTensor).to(device)
        y_pred = model(x_batch.float()).to(device)
        loss = loss_fn(y_pred, y_batch)
        optim.zero_grad()
        loss.backward()
        optim.step()",DL,Pytorch,rustam1488/s3e9-ens-model,https://www.kaggle.com/rustam1488/s3e9-ens-model
playground-series-s3e9,regression,continuous-regression,The task is to predict the concrete strength based on various input features.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test sets generated from a deep learning model trained on the Concrete Strength Prediction dataset. The training dataset includes a target column named Strength, while the test dataset requires predictions for this target. The dataset files include train.csv for training, test.csv for testing, and sample_submission.csv for submission format guidance.",['Strength'],"[""drop duplicates"", ""standardize features""]","model = net().to(device)
loss = nn.MSELoss()
optim = torch.optim.Adam(model.parameters(), lr=0.001)",DL,Pytorch,deshram/ps-s3-e9-xgb-cgb-lgb-rf-ann,https://www.kaggle.com/deshram/ps-s3-e9-xgb-cgb-lgb-rf-ann
playground-series-s3e7,classification,binary-classification,The task is to predict whether a hotel reservation will be canceled based on various features related to the booking.,Tabular,AUCROC,"The dataset consists of training and test sets generated from a deep learning model trained on a reservation cancellation prediction dataset. It includes features such as the number of adults and children, type of meal plan, room type reserved, lead time, and previous cancellations. The target variable is booking_status, indicating whether the reservation was canceled or not.",['booking_status'],"[""combine original dataset with training data"", ""replace outliers with NaN"", ""fill NaN values with mean"", ""normalize features using MinMaxScaler"", ""split dataset into training and validation sets"", ""create TensorFlow datasets for training, validation, and testing""]","inputs = keras.Input(shape=(None, 17))
bn = layers.BatchNormalization()(inputs)
x1 = layers.Dense(512, activation='relu')(bn)
x1 = layers.Dropout(.5)(x1)
x2 = layers.Dense(512, activation='relu')(bn)
x2 = layers.Dropout(.5)(x2)
gn1 = layers.GaussianNoise(.1)(x1)
gn2 = layers.GaussianNoise(.1)(x2)
x1 = layers.Dense(256, activation='relu')(gn1)
x1 = layers.Dropout(.5)(x1)
x2 = layers.Dense(300, activation='relu')(gn2)
x2 = layers.Dropout(.6)(x2)
gn1 = layers.GaussianNoise(.1)(x1)
gn2 = layers.GaussianNoise(.1)(x2)
x1 = layers.Dense(128, activation='relu')(gn1)
x1 = layers.Dropout(.2)(x1)
x2 = layers.Dense(112, activation='relu')(gn2)
x2 = layers.Dropout(.2)(x2)
gn1 = layers.GaussianNoise(.08)(x1)
gn2 = layers.GaussianNoise(.07)(x2)
x1 = layers.Dense(64, activation='relu')(gn1)
x2 = layers.Dense(96, activation='relu')(gn2)
x1 = layers.Dense(32, activation='relu')(x1)
x2 = layers.Dense(32, activation='relu')(x2)
x1 = layers.Dense(16, activation='relu')(x1)
x2 = layers.Dense(16, activation='relu')(x2)
output1 = layers.Dense(1, activation='sigmoid', name='Y-1')(x1)
output2 = layers.Dense(1, activation='sigmoid', name='Y-2')(x2)
model = keras.Model(inputs, outputs=[output1, output2])
model.compile(optimizer='Adam', loss=binary_focal_loss(), metrics=[tf.metrics.AUC()])
history = model.fit(training_batches, validation_data=val_batches, epochs=EPOCHS, callbacks=[early_stopping], verbose=0)",DL,Tensorflow,hikmatullahmohammadi/reservation-cancellation-simple-dnn-tf-keras,https://www.kaggle.com/hikmatullahmohammadi/reservation-cancellation-simple-dnn-tf-keras
playground-series-s3e7,classification,binary-classification,The task is to predict whether a hotel reservation will be canceled based on various customer and booking attributes.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on a reservation cancellation prediction dataset. It includes features such as booking ID, number of adults and children, number of weekend and week nights, meal plan type, car parking requirements, room type, lead time, arrival date details, market segment type, previous cancellations, and special requests. The target variable is booking_status, indicating if the reservation was canceled.",['booking_status'],"[""one-hot encode categorical columns"", ""scale numerical columns using robust scaler""]","model = Sequential()
model.add(Dense(8116, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4096,activation= 'relu'))
model.add(Dropout(0.5))
model.add(Dense(2048,activation= 'relu'))
model.add(Dropout(0.2))
model.add(Dense(256,activation = 'relu'))
model.add(Dropout(0.2))
model.add(Dense(128,activation = 'selu'))
model.add(Dropout(0.1))
model.add(Dense(8,activation= 'relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss=[BinaryFocalLoss(gamma =1)], optimizer= tf.keras.optimizers.Nadam(4e-5),metrics = [tf.keras.metrics.AUC(num_thresholds=700000,curve='ROC')])
model.fit(X_train,y_train,validation_split = 0.26,batch_size = 256,epochs = 15)",DL,Tensorflow,ashishkumarak/reservation-cancellation-prediction-lgbm-tf-rf,https://www.kaggle.com/ashishkumarak/reservation-cancellation-prediction-lgbm-tf-rf
playground-series-s3e7,classification,binary-classification,The task is to predict whether a hotel reservation will be canceled based on various features related to the booking.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model trained on a reservation cancellation prediction dataset. The training dataset includes features that describe the booking and the target variable is booking_status, indicating whether the reservation was canceled. The test dataset is used to predict booking_status. The dataset is in CSV format and has a size of 3.86 MB.",['booking_status'],"[""median-impute missing values"", ""one-hot encode categorical variables"", ""standardize numerical features""]","model = tf.keras.Sequential([tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),tf.keras.layers.Dense(64, activation='relu'),tf.keras.layers.Dense(1, activation='sigmoid')])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))",DL,Tensorflow,manthanx/ps3e7-indepth-eda-xgboost-optuna,https://www.kaggle.com/manthanx/ps3e7-indepth-eda-xgboost-optuna
playground-series-s3e7,classification,binary-classification,The task is to predict whether a reservation will be cancelled based on various features.,Tabular,AUCROC,The dataset consists of training and test sets generated from a deep learning model trained on a reservation cancellation prediction dataset. The training dataset includes a target column 'booking_status' indicating whether a reservation was cancelled. The test dataset is used to predict the same target. The dataset files are in CSV format and the total size is 3.86 MB.,['booking_status'],"[""check for null values"", ""convert categorical columns to category type"", ""drop the 'id' column from train and test datasets"", ""encode categorical features using CatBoostEncoder""]","inputs = Input(shape=(17))
X = inputs
for i in [512,256,128,64]:
    X = Dense(units=i)(X)
    X = BatchNormalization()(X)
    X = Activation('relu')(X)
X = Dense(units=1)(X)
X = BatchNormalization()(X)
outputs = Activation('sigmoid')(X)
model = Model(inputs =inputs, outputs=outputs)
model.compile(loss='BinaryCrossentropy',optimizer = tf.keras.optimizers.Adam(tf.keras.optimizers.schedules.ExponentialDecay(0.001,3200,0.9)), metrics=['AUC'])
history = model.fit(X_train,y_train, epochs=300, batch_size=64, validation_data=(X_cv,y_cv))",DL,Tensorflow,jayantdon/playground-series-s3e7,https://www.kaggle.com/jayantdon/playground-series-s3e7
playground-series-s3e7,classification,binary-classification,The task is to predict whether a reservation will be cancelled based on various features.,Tabular,AUCROC,The dataset consists of training and test sets generated from a deep learning model trained on a reservation cancellation prediction dataset. The training dataset includes a target column 'booking_status' indicating whether a reservation was cancelled. The test dataset is used to predict the same target. The dataset files are in CSV format and the total size is 3.86 MB.,['booking_status'],"[""drop the 'id' column"", ""fix date anomalies"", ""create date features"", ""create interaction features"", ""one-hot encode categorical features""]","x_input = Input(shape=(X.shape[-1]), name='input')
x1 = Dense(128, activation='sigmoid')(x_input)
d1 = Dropout(0.1)(x1)
x2 = Dense(64, activation='sigmoid')(d1)
d2 = Dropout(0.1)(x2)
x3 = Dense(32, activation='sigmoid')(d2)
d3 = Dropout(0.1)(x3)
output = Dense(1, activation='sigmoid', name='output')(d3)
model = Model(x_input, output, name='nn_model')
model.compile(optimizer='adam', loss=tfa.losses.SigmoidFocalCrossEntropy(alpha=0.1, gamma=3), metrics='AUC')
model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=300, verbose=VERBOSE, batch_size=BATCH_SIZE, class_weight={0: 1, 1: 10}, callbacks=[lr, es])",DL,Tensorflow,samuraikaggle/samurai-s3e7ens-cb-xb-lb,https://www.kaggle.com/samuraikaggle/samurai-s3e7ens-cb-xb-lb
playground-series-s3e5,regression,ordinal-regression,The task is to predict the quality of wine based on various properties using regression techniques.,Tabular,Quadratic Weighted Kappa,"The dataset for this competition includes both training and test sets generated from a deep learning model trained on the Wine Quality dataset. The feature distributions are similar to the original dataset. The training dataset contains a target column named quality, which is an ordinal integer. The test dataset is used to predict the quality values. A sample submission file is also provided in the correct format.",['quality'],"[""standardize features using StandardScaler""]","model = keras.models.Sequential([
    keras.layers.Dense(128, activation=""relu"", input_shape=[12]),
    keras.layers.Dense(64, activation=""relu""),
    keras.layers.Dense(1, activation='linear')
])

model.compile(loss=""mean_squared_error"", optimizer=('adam'), metrics=['mae'])

show_model = model.fit(x_train_sc, y_train, epochs=300, validation_split=0.2)",DL,Tensorflow,meeratif/ps-eda-gradientboostingclassifier,https://www.kaggle.com/meeratif/ps-eda-gradientboostingclassifier
playground-series-s3e5,regression,ordinal-regression,The task is to predict the quality of wine based on various properties using regression techniques.,Tabular,Quadratic Weighted Kappa,"The dataset for this competition includes a training set and a test set generated from a deep learning model trained on the Wine Quality dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains various properties of wine, with the target variable being the quality, which is an ordinal integer. The test dataset is used to predict the quality of wine, and a sample submission file is provided to illustrate the required format.",['quality'],"[""remove duplicates"", ""impute missing values"", ""normalize features"", ""encode categorical variables""]","model = tf.keras.Sequential([tf.keras.layers.Dense(128, activation='relu', input_shape=(input_shape,)),tf.keras.layers.Dense(64, activation='relu'),tf.keras.layers.Dense(1)])
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(train_X, train_y, epochs=100, batch_size=32, validation_split=0.2)",DL,Tensorflow,kotrying/ps-s3e5-using-polars,https://www.kaggle.com/kotrying/ps-s3e5-using-polars
playground-series-s3e5,regression,ordinal-regression,The task is to predict the quality of wine based on various chemical properties using regression techniques.,Tabular,Quadratic Weighted Kappa,"The dataset consists of training and test sets generated from a deep learning model trained on the Wine Quality dataset. It includes various chemical properties of red wine and the target variable is the quality, which is an ordinal integer. The training dataset is provided in train.csv and the test dataset in test.csv, with a sample submission file available for formatting guidance.",['quality'],"[""drop unnecessary columns"", ""concatenate additional datasets"", ""sample and reset index"", ""create new features based on existing ones"", ""apply KernelPCA for dimensionality reduction"", ""scale features using RobustScaler"", ""one-hot encode target variable""]","model = Sequential()
model.add(Dense(4096, input_shape= (X_train.shape[1],), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1024,activation= 'relu'))
model.add(Dense(512,activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(256,activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(128,activation = 'relu'))
model.add(Dense(6, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Nadam(4e-4),metrics = [tfa.metrics.CohenKappa(num_classes = 6,weightage = 'quadratic')])
model.fit(X_train,y_train,validation_split = 0.26,batch_size =128,epochs = 60)",DL,Tensorflow,ashishkumarak/wine-quality-prediction-tensorflow,https://www.kaggle.com/ashishkumarak/wine-quality-prediction-tensorflow
playground-series-s3e5,regression,ordinal-regression,The task is to predict the quality of wine based on various properties using regression techniques.,Tabular,Quadratic Weighted Kappa,"The dataset for this competition includes both training and test sets generated from a deep learning model trained on the Wine Quality dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a target column named 'quality', which is an ordinal integer, while the test dataset is used to predict the same target. A sample submission file is also provided in the correct format.",['quality'],"[""drop the 'Id' column from training and test sets"", ""standardize the features using StandardScaler""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2)",DL,Tensorflow,kagleo123/playground-s3e5-visualization-modeling,https://www.kaggle.com/kagleo123/playground-s3e5-visualization-modeling
playground-series-s3e5,regression,ordinal-regression,The task is to predict the quality of wine based on various properties using regression techniques.,Tabular,Quadratic Weighted Kappa,"The dataset for this competition includes both training and test sets generated from a deep learning model trained on the Wine Quality dataset. The feature distributions are similar but not identical to the original dataset. The training dataset contains a target column named 'quality', which is an ordinal integer, while the test dataset is used to predict the same target. A sample submission file is also provided in the correct format.",['quality'],"[""remove duplicates"", ""drop outliers"", ""standardize features""]","model = tf.keras.Sequential([
        normalize,
        layers.Dense(64, activation='relu'),
        layers.Dense(1)
    ])
    model.compile(
        loss = tf.keras.losses.MeanSquaredError(),
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.005),
        metrics=metrics,
    )
    model.fit(
        train_features, train_labels,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        callbacks=[early_stopping],
        validation_data=(val_features, val_labels),
    )",DL,Tensorflow,martynovandrey/ps-3-5-keras-nn-model,https://www.kaggle.com/martynovandrey/ps-3-5-keras-nn-model
playground-series-s3e5,regression,ordinal-regression,The task is to predict the quality of wine based on various properties using regression techniques.,Tabular,Quadratic Weighted Kappa,"The dataset for this competition includes a training set and a test set generated from a deep learning model trained on the Wine Quality dataset. The feature distributions are similar to the original dataset. The training dataset contains a target column named 'quality', which is an ordinal integer, while the test dataset is used to predict the same target. A sample submission file is also provided in the correct format.",['quality'],"[""mean-std normalize the data"", ""convert the tabular data to a PyTorch dataset""]","model = NNRegressorModel()
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_func = WeightedKappaLoss(num_classes=6, device=device)
fit(model, train_dataset, valid_dataset, loss_func=loss_func, optimizer=optimizer, epochs=250, device=device)",DL,Pytorch,ryanirl/ps-s03e05-pytorch-nn-modeling,https://www.kaggle.com/ryanirl/ps-s03e05-pytorch-nn-modeling
playground-series-s3e5,regression,ordinal-regression,The task is to predict the quality of wine based on various properties using regression techniques.,Tabular,Quadratic Weighted Kappa,"The dataset consists of training and test data generated from a deep learning model trained on the Wine Quality dataset. The feature distributions are similar to the original dataset. The training dataset contains a target column named 'quality', which is an ordinal integer. The test dataset is used to predict the 'quality' values. The dataset files include train.csv, test.csv, and a sample submission file.",['quality'],"[""subtract 3 from target variable"", ""fill missing values in 'id' with -1"", ""convert 'id' to integer type"", ""reset index and drop the old index"", ""apply logarithmic transformation to numerical features"", ""create synthetic features based on existing features""]","self.mod1 = nn.Sequential(
            nn.Linear(in_features=in_features, out_features=inner_features),
            nn.ReLU(inplace=False),
            nn.Dropout(p=0.5, inplace=False),
            nn.Linear(in_features=inner_features, out_features=inner_features),
            nn.ReLU(inplace=False),
            nn.Dropout(p=0.5, inplace=False),
            nn.Linear(in_features=inner_features, out_features=inner_features),
            nn.ReLU(inplace=False),
            nn.Dropout(p=0.5, inplace=False),
            nn.Linear(in_features=inner_features, out_features=inner_features),
            nn.ReLU(inplace=False),
            nn.Dropout(p=0.5, inplace=False),
            nn.Linear(in_features=inner_features, out_features=inner_features),
            nn.ReLU(inplace=False),
            nn.Dropout(p=0.5, inplace=False),
        )
        self.out = nn.Sequential(
            nn.Linear(in_features=inner_features+1, out_features=out_features),
            nn.Softmax(dim=0)
        )
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5, betas=(0.9, 0.999), weight_decay=1e-3)",DL,Pytorch,kdmitrie/pgs35-ensemble-of-pytorch-models,https://www.kaggle.com/kdmitrie/pgs35-ensemble-of-pytorch-models
playground-series-s3e5,regression,ordinal-regression,The task is to predict the quality of wine based on various properties using regression techniques.,Tabular,Quadratic Weighted Kappa,"The dataset for this competition includes both training and test data generated from a deep learning model trained on the Wine Quality dataset. The feature distributions are similar to the original dataset. The training dataset contains a target column named quality, which is an ordinal integer. The test dataset is used to predict the quality values. A sample submission file is also provided in the correct format.",['quality'],"[""normalize features using MinMaxScaler""]","model = DNN()
model = model.to(device)
criterion = MAELoss()
optimizer = optim.Adam(model.parameters(),lr=0.0001)
train_loss_list, val_loss_list = run(300, optimizer, criterion, device, train_loader, val_loader,model)",DL,Pytorch,shiraeharuto/dnn-regression,https://www.kaggle.com/shiraeharuto/dnn-regression
playground-series-s3e5,regression,ordinal-regression,"The task is to predict the quality of wine based on various physicochemical properties, with quality represented as an ordinal integer value.",Tabular,Quadratic Weighted Kappa,"The dataset consists of training and test sets generated from a deep learning model trained on the Wine Quality dataset. It includes 11 physicochemical features such as acidity, residual sugar, and alcohol content, with the target variable being the quality of the wine, which is an ordinal integer. The training dataset is provided in train.csv, while the test dataset is in test.csv, and a sample submission format is included in sample_submission.csv.",['quality'],"[""drop unnecessary columns"", ""standardize features"", ""apply PCA for dimensionality reduction""]","model = nn.Sequential(nn.Linear(6, 128), nn.ReLU(), nn.Linear(128, 6), nn.Softmax(dim=1))
model.compile(optimizer='adam', loss='cross_entropy', metrics=['accuracy'])
model.fit(X_train, Y_train, epochs=100, batch_size=32)",DL,Pytorch,excelsior7/9th-place-solution-playground-series-sea-3-epi5,https://www.kaggle.com/excelsior7/9th-place-solution-playground-series-sea-3-epi5
playground-series-s3e5,regression,ordinal-regression,The task is to predict the quality of wine based on various properties using regression techniques.,Tabular,Quadratic Weighted Kappa,"The dataset for this competition includes a training dataset and a test dataset generated from a deep learning model trained on the Wine Quality dataset. The feature distributions are similar to the original dataset. The training dataset contains features and the target variable 'quality', which is an ordinal integer. The test dataset is used to predict the 'quality'.",['quality'],"[""convert from Pandas dataframe to numpy arrays"", ""create PyTorch dataset from numpy arrays"", ""split dataset into training and validation sets"", ""create data loaders for training and validation sets""]","model = HousingModel()
optimizer = opt_func(model.parameters(), lr)
history = fit(epochs, lr, model, train_loader, val_loader, grad_clip)",DL,Pytorch,itzdevil/linear-regression,https://www.kaggle.com/itzdevil/linear-regression
playground-series-s3e3,classification,binary-classification,The task is to predict the probability of employee attrition based on various features.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model focused on employee attrition. The training dataset includes a binary target variable, Attrition, while the test dataset is used to predict the probability of this target. The dataset contains various features related to employee demographics and job characteristics, and it is available in CSV format.",['Attrition'],"[""drop the Over18 column"", ""replace weird Education values"", ""one-hot encode categorical features"", ""scale numerical features using StandardScaler""]","model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Input(shape=(45,)))
model.add(Dense(64, activation=relu))
model.add(Dropout(0.2))
model.add(Dense(32, activation=relu))
model.add(Dropout(0.1))
model.add(tf.keras.layers.Dense(units=1,  activation=sigmoid))

optimizer = Adam(learning_rate=config.learning_rate, beta_1=0.9, beta_2=0.999, clipnorm=1.0)
loss = tf.keras.losses.BinaryCrossentropy()
metrics = [tf.keras.metrics.AUC()]
model.compile(optimizer=optimizer,
              loss=loss,
              metrics=metrics)

history = model.fit(X_train1, y_train, batch_size=config.batch_size,epochs=config.epochs, verbose=0 ,validation_split = 0.2,callbacks = [WandbCallback()])",DL,Tensorflow,usharengaraju/integratedgradients-tensorflow-w-b,https://www.kaggle.com/usharengaraju/integratedgradients-tensorflow-w-b
playground-series-s3e3,classification,binary-classification,The task is to predict the probability of employee attrition based on various features related to employee demographics and job characteristics.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model focused on employee attrition. It includes features that are similar to but not identical to the original dataset. The training dataset contains a binary target variable, Attrition, indicating whether an employee has left the company. The test dataset is used to predict the probability of attrition for each employee. The dataset is provided in CSV format and is approximately 455.63 kB in size.",['Attrition'],"[""drop constant columns"", ""impute missing values"", ""scale numerical features"", ""encode categorical features""]","model = tf.keras.Sequential()\nmodel.add(layers.Dense(128, activation='relu', input_shape=(input_dim,)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\nmodel.fit(x_train, y_train, epochs=50, batch_size=32, validation_split=0.2)",DL,Tensorflow,kirillka95/ps-s03e03-eda-16-models-test-0-94,https://www.kaggle.com/kirillka95/ps-s03e03-eda-16-models-test-0-94
playground-series-s3e3,classification,binary-classification,The task is to predict the probability of employee attrition based on various features.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model focused on employee attrition. The training dataset includes a binary target variable, Attrition, while the test dataset is used to predict the probability of this target. The dataset is in CSV format and has a size of 455.63 kB, with an Attribution 4.0 International license.",['Attrition'],"[""convert target variable to binary"", ""drop unnecessary columns"", ""scale numerical features using robust scaler"", ""one-hot encode categorical features""]","model = Sequential()
model.add(Dense(140000, input_dim=54, activation='relu'))
model.add(Dense(4000,activation= 'relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss=['binary_crossentropy','mse'], optimizer= tf.keras.optimizers.Adam(6e-5),metrics = [tf.keras.metrics.AUC(num_thresholds=700000,curve='ROC')])
model.fit(X_train,y_train,validation_split = 0.25,batch_size = 128,epochs = 15)",DL,Tensorflow,ashishkumarak/employee-retention-workforce-reduction-prediction,https://www.kaggle.com/ashishkumarak/employee-retention-workforce-reduction-prediction
playground-series-s3e3,classification,binary-classification,The task is to predict the probability of employee attrition based on various features.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model focused on employee attrition. It includes features that are similar to but not identical to the original dataset. The training dataset contains a binary target variable, Attrition, while the test dataset is used to predict the probability of attrition. The dataset is provided in CSV format and is approximately 455.63 kB in size.",['Attrition'],"[""drop unnecessary columns"", ""encode categorical variables using WOE encoding"", ""scale numerical features using standard scaling"", ""combine training and additional data for feature enrichment""]","model = keras.Sequential([
    layers.Dense(512), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=dr),
    layers.Dense(256), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=dr),
    layers.Dense(128), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=dr),
    layers.Dense(64), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=dr),
    layers.BatchNormalization(),
    layers.Dense(32), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=dr),
    layers.Dense(16), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=dr),
    layers.Dense(8),
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=dr),
    layers.Dense(4), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dense(2), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dense(1, activation='sigmoid')
])
opt = keras.optimizers.Adam(learning_rate=0.0001)
model.compile(
    optimizer=opt,
    loss=tfa.losses.SigmoidFocalCrossEntropy(
        alpha=0.8,
        gamma=2.0
    ),
    metrics='AUC',
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=64,
    epochs=500,
    class_weight={0: 1.0, 1: class_weight},
    callbacks=[early_stopping, plat],
    verbose=0
)",DL,Tensorflow,alexandershumilin/ps-s3-e3-ensemble-model-woe-encoding-optuna,https://www.kaggle.com/alexandershumilin/ps-s3-e3-ensemble-model-woe-encoding-optuna
playground-series-s3e3,classification,binary-classification,The task is to predict the probability of employee attrition based on various features related to employee demographics and job characteristics.,Tabular,AUCROC,"The dataset consists of synthetic and original employee attrition data, with features that include demographics, job satisfaction, and performance metrics. The training dataset contains a binary target variable indicating attrition, while the test dataset is used to predict attrition probabilities. The dataset is in CSV format and is approximately 455.63 kB in size.",['Attrition'],"[""merge synthetic and original datasets"", ""drop irrelevant columns"", ""encode categorical features"", ""scale numeric features"", ""apply SMOTE for handling class imbalance""]","model = tf.keras.Sequential([tf.keras.layers.Dense(128, activation='relu', input_shape=(input_shape,)),tf.keras.layers.Dense(64, activation='relu'),tf.keras.layers.Dense(1, activation='sigmoid')])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))",DL,Tensorflow,validmodel/playground-s-3-e-3-eda-fe-fs-modeling,https://www.kaggle.com/validmodel/playground-s-3-e-3-eda-fe-fs-modeling
playground-series-s3e3,classification,binary-classification,The task is to predict the probability of employee attrition based on various features.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model focused on employee attrition. It includes features that are similar but not identical to the original dataset. The training dataset contains a binary target variable, Attrition, while the test dataset is used to predict the probability of this target. The dataset is provided in CSV format and is approximately 455.63 kB in size.",['Attrition'],"[""assert no missing values"", ""stratified split into train, validation, and test sets""]","data_config = DataConfig(
    target=[""Attrition""],
    continuous_cols=num_col_names[:-1],
    categorical_cols=cat_col_names,
    num_workers = 4
)
trainer_config = TrainerConfig(
    auto_lr_find = True,
    batch_size   = 128,
    max_epochs   =  50,
)
optimizer_config = OptimizerConfig()
model_config = GatedAdditiveTreeEnsembleConfig(
    task=""classification"",
    tree_depth  =  4,
    num_trees   = 10,
    chain_trees = False,
    gflu_stages =  2,
    metrics=['accuracy', ""auroc""],
    metrics_params=[dict(task=""multiclass"", num_classes=2), dict(task=""multiclass"", num_classes=2)]
)
tabular_model = TabularModel(
    data_config=data_config,
    model_config=model_config,
    optimizer_config=optimizer_config,
    trainer_config=trainer_config,
)
tabular_model.fit(train=train, validation=val, loss=weighted_loss)",DL,Pytorch,carlmcbrideellis/pytorch-tabular-gated-additive-tree-ensemble,https://www.kaggle.com/carlmcbrideellis/pytorch-tabular-gated-additive-tree-ensemble
playground-series-s3e3,classification,binary-classification,The task is to predict the probability of employee attrition based on various features related to employee demographics and job characteristics.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model focused on employee attrition. It includes features that are similar but not identical to the original dataset. The training dataset contains a binary target variable indicating attrition, while the test dataset is used to predict the probability of attrition. A sample submission file is also provided to guide the format of the predictions.",['Attrition'],"[""drop unnecessary columns"", ""create binary features based on age and salary"", ""combine features to create new metrics""]","tabular_model = TabularModel(
    data_config=data_config,
    model_config=model_config,
    optimizer_config=optimizer_config,
    trainer_config=trainer_config,
)

# Train the model
    tabular_model.train(model, datamodule)",DL,Pytorch,manujosephv/ps3e3-pytorch-tabular-gate,https://www.kaggle.com/manujosephv/ps3e3-pytorch-tabular-gate
playground-series-s3e3,classification,binary-classification,The task is to predict the probability of employee attrition based on various features.,Tabular,AUCROC,"The dataset consists of training and test data generated from a deep learning model focused on employee attrition. The training dataset includes a binary target variable, Attrition, while the test dataset is used to predict the probability of attrition. The dataset is in CSV format and contains various features related to employee attributes.",['Attrition'],"[""drop constant columns"", ""encode categorical columns"", ""scale data""]","model_ft = models.vgg16(pretrained=True)
model_ft.classifier[6] = nn.Linear(4096, num_classes)
model_ft = model_ft.to(device)
optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)
criterion = nn.CrossEntropyLoss()
model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)",DL,Pytorch,ctoraq/my-pipeline,https://www.kaggle.com/ctoraq/my-pipeline
playground-series-s3e1,regression,continuous-regression,The task is to predict the median house value based on various features related to housing and demographics in California.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of synthetic data generated from a deep learning model trained on the California Housing Dataset. It includes a training set and a test set, with the target variable being MedHouseVal. The training dataset contains various features such as median income, house age, average rooms, and population, among others. The test dataset is used to evaluate the model's predictions.",['MedHouseVal'],"[""standardize features using StandardScaler"", ""normalize features using MinMaxScaler"", ""remove outliers from target variable"", ""apply log transformation to target variable"", ""drop outliers based on median income"", ""scale median income feature"", ""scale house age feature"", ""scale average rooms feature"", ""scale population feature"", ""scale average occupancy feature"", ""scale average bedrooms feature"", ""convert binary indicators to integer type"", ""scale latitude feature"", ""scale longitude feature"", ""scale distance feature"", ""scale cluster feature""]","model = Sequential()
model.add(Dense(units=512, input_dim=10, kernel_initializer='normal', activation='relu'))
model.add(Dense(units=100, kernel_initializer='normal', activation='tanh'))
model.add(Dense(1, kernel_initializer='normal'))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X_train, y_train ,batch_size = 20, epochs = 50, verbose=1)",DL,Tensorflow,jonbown/california-house-price-prediction,https://www.kaggle.com/jonbown/california-house-price-prediction
playground-series-s3e1,regression,continuous-regression,The task is to predict the MedHouseVal for given features in the dataset.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test data generated from a deep learning model based on the California Housing Dataset. It includes features that are close to the original dataset but not identical. The training dataset contains the target variable MedHouseVal, while the test dataset is used for making predictions. The dataset files are in CSV format and the total size is 6.48 MB.",['MedHouseVal'],"[""drop irrelevant features"", ""scale features using MinMaxScaler""]","model = Sequential()
model.add(Dense(256, input_shape=(features,), activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='Adam', metrics=['accuracy', tf.keras.metrics.RootMeanSquaredError(name=""rmse"")])
history = model.fit(X_train, y_train,
                    callbacks=[early_stop, model_checkpoint],
                    epochs=200,
                    batch_size=48,
                    validation_split=0.2,
                    shuffle=True,
                    verbose=0)",DL,Tensorflow,behroozsohrabi/playground-nn-estimator-jan-2023,https://www.kaggle.com/behroozsohrabi/playground-nn-estimator-jan-2023
playground-series-s3e1,regression,continuous-regression,The task is to predict the median house value based on various features derived from the California Housing Dataset.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set and a test set generated from a deep learning model trained on the California Housing Dataset. The features are similar to the original dataset, and the target variable is the median house value. The training dataset includes a column for the target variable, while the test dataset is used for making predictions. The dataset is in CSV format and has a size of 6.48 MB.",['MedHouseVal'],"[""drop the 'id' column"", ""generate additional features based on coordinates"", ""apply reverse geocoding to obtain place names"", ""replace place names with a limited set of categories"", ""one-hot encode categorical variables"", ""scale features using MinMaxScaler""]","model = keras.Sequential([
    layers.Dense(128), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=0.3),
    layers.Dense(64), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=0.3),
    layers.Dense(32), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=0.3),
    layers.Dense(16), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dropout(rate=0.3),
    layers.Dense(8), 
    layers.BatchNormalization(),
    layers.Dropout(rate=0.3),
    layers.Dense(4), 
    layers.BatchNormalization(),
    layers.Dropout(rate=0.3),
    layers.Dense(2), 
    layers.LeakyReLU(alpha=0.3),
    layers.Dense(1)
]);

opt = keras.optimizers.Adam(learning_rate=lr_schedule);
model.compile(
    optimizer=opt,
    loss=""MSE"",
    metrics=[keras.metrics.RootMeanSquaredError()]
);
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=500,
    callbacks=[early_stopping],
    verbose=1
);",DL,Tensorflow,alexandershumilin/playground-series-s3-e1-keras-nn,https://www.kaggle.com/alexandershumilin/playground-series-s3-e1-keras-nn
playground-series-s3e1,regression,continuous-regression,The task is to predict the median house value based on various features of the housing dataset.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set and a test set generated from a deep learning model trained on the California Housing Dataset. The features in the dataset include various attributes related to housing, and the target variable is the median house value. The training dataset is provided in train.csv, while the test dataset is in test.csv. A sample submission file is also included to guide the format of the predictions.",['MedHouseVal'],"[""scale features using standard scaler"", ""add rotated coordinate features""]","model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=100, batch_size=32)",DL,Tensorflow,sharvalishinde/pss3e1-histgradientboost-lbgm-xgboost-ensembling,https://www.kaggle.com/sharvalishinde/pss3e1-histgradientboost-lbgm-xgboost-ensembling
playground-series-s3e1,regression,continuous-regression,The task is to predict the MedHouseVal target variable based on features derived from a synthetic dataset generated from a deep learning model.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set and a test set generated from a deep learning model trained on the California Housing Dataset. The feature distributions are similar to the original dataset. The training dataset includes the target variable MedHouseVal, while the test dataset is used for making predictions. The dataset is in CSV format and has a size of 6.48 MB.",['MedHouseVal'],"[""remove outliers based on specified criteria"", ""scale features using MinMaxScaler or StandardScaler""]","model = tf.keras.models.Sequential([
        layers.Dense(256, activation='relu', kernel_regularizer=reg.l2(l)),
        layers.Dense(128, activation='relu', kernel_regularizer=reg.l2(l)),
        layers.Dense(64, activation='relu', kernel_regularizer=reg.l2(l)),
        layers.Dense(1, activation='sigmoid'),
        layers.Rescaling(scale=5.0),
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(conf.init_lr),
        loss=""mse"",
        metrics=tf.keras.metrics.RootMeanSquaredError(name=""rmse"")
    )
    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=conf.batch_size, callbacks=[plat, es], verbose=conf.verbose)",DL,Tensorflow,kirillka95/ps-s3e01-tensorflow-approach-kfold,https://www.kaggle.com/kirillka95/ps-s3e01-tensorflow-approach-kfold
playground-series-s3e1,regression,continuous-regression,The task is to predict the median house value based on various features from the dataset.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set and a test set generated from a deep learning model trained on the California Housing Dataset. The feature distributions are similar to the original dataset. The training dataset includes a target column named MedHouseVal, while the test dataset is used for making predictions. A sample submission file is also provided to illustrate the required format for submissions.",['MedHouseVal'],"[""load training and test data"", ""create custom dataset class"", ""convert features to tensor format"", ""create data loaders for training, validation, and testing""]","model = NeuralNetwork().to(device)
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

for t in range(epochs):
    epoch_loss_history, epoch_score_history = train_loop(train_dataloader, model, loss_fn, optimizer)
    validate_loop(validate_dataloader, model, loss_fn)",DL,Pytorch,agorinenko/neural-network-for-regression-with-pytorch,https://www.kaggle.com/agorinenko/neural-network-for-regression-with-pytorch
playground-series-s3e1,regression,continuous-regression,The task is to predict the median house value based on various features of the housing dataset.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of synthetic data generated from a deep learning model trained on the California Housing Dataset. It includes a training set and a test set, with the target variable being MedHouseVal. The training dataset contains various features related to housing, while the test dataset is used for making predictions. The dataset is in CSV format and has a size of 6.48 MB.",['MedHouseVal'],"[""round values of specific columns to integers"", ""create a new feature for distance from city center"", ""cap outliers in specified columns"", ""scale features using standard scaling""]","model = TabNetRegressor(verbose=1,seed=42)
model.fit(X_train=X_train_1, y_train=y_train_1,
          eval_set=[(X_valid, y_valid)],
          patience=200, max_epochs=2000,
          eval_metric=['rmse'])
y_train_pred = model.predict(X_train_scaled)
y_val_pred = model.predict(X_val_scaled)",DL,Pytorch,jaygohel/playground-series-seasn-3-ep-1-lr-pr-xgboost,https://www.kaggle.com/jaygohel/playground-series-seasn-3-ep-1-lr-pr-xgboost
tabular-playground-series-nov-2022,classification,binary-classification,The task is to blend multiple binary classification model predictions to improve accuracy and minimize log loss on unseen data.,Tabular,Log Loss,"The dataset consists of a folder containing submission files with binary predictions and a CSV file with ground truth labels for the first half of the rows. The goal is to blend these submissions to enhance model predictions, using the ground truth to evaluate improvements in log loss.",['pred'],"[""clip predictions to avoid extreme values"", ""drop columns with inverted predictions"", ""apply PCA for dimensionality reduction"", ""add statistical features like mean and standard deviation""]","input_ = Input(shape = X_train.shape[1])
d0 = Dense(1024 * sc // 5 + 1)(input_)
a0 = LeakyReLU()(d0)
dr0 = Dropout(rate=dr)(a0)
d01 = Dense(512 * sc // 5 + 1)(dr0)
a01 = LeakyReLU()(d01)
dr01 = Dropout(rate=dr)(a01)
d1 = Dense(256 * sc // 5 + 1)(dr01)
a1 = LeakyReLU()(d1)
dr1 = Dropout(rate=dr)(a1)
d2 = Dense(128 * sc // 5 + 1)(dr1)
a2 = LeakyReLU()(d2)
dr2 = Dropout(rate=dr)(a2)
d3 = Dense(64 * sc // 5 + 1)(dr2)
a3 = LeakyReLU()(d3)
dr3 = Dropout(rate=dr)(a3)
d4 = Dense(32 * sc // 5 + 1)(dr3)
a4 = LeakyReLU()(d4)
dr4 = Dropout(rate=dr)(a4)
d5 = Dense(16 * sc // 5 + 1)(dr4)
a5 = LeakyReLU()(d5)
dr5 = Dropout(rate=dr)(a5)
d6 = Dense(8 * sc // 5 + 1)(dr5)
a6 = LeakyReLU()(d6)
dr6 = Dropout(rate=dr)(a6)
d7 = Dense(4)(dr6)
a7 = LeakyReLU()(d7)
dr7 = Dropout(rate=dr)(a7)
concat3 = Concatenate()([dr7, input_])
d8 = Dense(2)(concat3)
a8 = LeakyReLU()(d8)
dr8 = Dropout(rate=dr)(a8)
out = Dense(1, activation='sigmoid')(dr8)

model1 = Model(inputs=input_, outputs=out)

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
                initial_learning_rate=lr,
                decay_steps=1000,
                decay_rate=dec_r)
opt = keras.optimizers.Ftrl(learning_rate=lr_schedule)

model1.compile(
optimizer=opt,
loss='binary_crossentropy',
metrics=['binary_accuracy'],
)
history = model1.fit(
      X_train, y_train,
      validation_data=(X_valid, y_valid),
      batch_size=bs,
      epochs=300,
      callbacks=[early_stopping],
      verbose=1,
     )",DL,Tensorflow,alexandershumilin/tps-nov-2022-simple-keras-model-pca,https://www.kaggle.com/alexandershumilin/tps-nov-2022-simple-keras-model-pca
tabular-playground-series-nov-2022,classification,binary-classification,"The task is to blend various submission files to improve model predictions for a binary classification problem, using ground truth labels for validation.",Tabular,Log Loss,"The dataset consists of a folder containing submission files with predictions for a binary classification task. The ground truth labels are provided in train_labels.csv for the first half of the rows. The goal is to blend these submissions to enhance prediction accuracy, with the ability to validate improvements using the ground truth labels.",['label'],"[""scale data using standard scaler""]","model = keras.Sequential()
model.add(layers.Input(shape=(X_train.shape[1],)))
for i in range(CFG['depth']-1):
    model.add(layers.Dense(units=CFG['units']))
    model.add(layers.BatchNormalization())
    model.add(layers.Activation(CFG['activation']))
    model.add(layers.Dropout(rate=CFG['dropout_rate']))
model.add(layers.Dense(units=1, activation='sigmoid'))
model.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy',metrics=['binary_accuracy'])
history = model.fit(train_generator,validation_data=(X_valid,y_valid),callbacks=[early_stopping,scheduler],batch_size=CFG['batch_size'],epochs=CFG['epochs'],verbose=CFG['verbose'])",DL,Tensorflow,samuelcortinhas/tps-nov-22-blending-predictions-with-nn,https://www.kaggle.com/samuelcortinhas/tps-nov-22-blending-predictions-with-nn
tabular-playground-series-nov-2022,classification,binary-classification,"The task is to predict the probability of a binary outcome based on a set of features, using blended predictions from various models to improve accuracy.",Tabular,Log Loss,"The dataset consists of a folder containing submission files with binary classification predictions and a CSV file with ground truth labels for the first half of the rows in these submissions. The goal is to blend these submissions to enhance model predictions, with the ability to evaluate improvements using the provided ground truth labels.",['label'],"[""standardize features using StandardScaler""]","inputs = Input(shape=(X.shape[1],))
x0 = Dense(128, kernel_regularizer=REG1, activation='Mish')(inputs)
x0 = Dropout(0.3)(x0)
x0 = BatchNormalization()(x0)
x1 = Dense(128, kernel_regularizer=REG1, activation='Mish')(x0)
x1 = Dropout(0.3)(x1)
x1 = BatchNormalization()(x1)
x = Concatenate()([x0, x1])
x = Dense(1, kernel_regularizer=REG2, activation='sigmoid')(x)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=tf.keras.losses.BinaryCrossentropy(), metrics=tf.keras.metrics.BinaryCrossentropy())
history = model.fit(X_train_scaled, y_train, validation_data=(X_valid_scaled, y_valid), batch_size=128, epochs=130, callbacks=[early_stopping, plateau], verbose=0)",DL,Tensorflow,alexryzhkov/multistart-nns-with-lightautoml,https://www.kaggle.com/alexryzhkov/multistart-nns-with-lightautoml
tabular-playground-series-nov-2022,classification,binary-classification,"The task is to predict the probability of a binary outcome based on a set of features, using blended predictions from various models to improve accuracy.",Tabular,Log Loss,"The dataset consists of a folder containing submission files with binary classification predictions and a CSV file with ground truth labels for the first half of the rows in these submissions. The goal is to blend these submissions to enhance model predictions, with the ability to evaluate improvements using the provided ground truth labels.",['label'],"[""standardize features using StandardScaler""]","inputs = Input(shape=(X.shape[1],))
x0 = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(1e-06), activation=tfa.activations.mish)(inputs)
x0 = Dropout(0.3)(x0)
x0 = BatchNormalization()(x0)
x1 = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(1e-06), activation=tfa.activations.mish)(x0)
x1 = Dropout(0.3)(x1)
x1 = BatchNormalization()(x1)
x = Concatenate()([x0, x1])
x = Dense(1, kernel_regularizer=tf.keras.regularizers.l2(1e-07), activation='sigmoid')(x)
model = Model(inputs, x)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=tf.keras.losses.BinaryCrossentropy(), metrics=tf.keras.metrics.BinaryCrossentropy())
history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=128, epochs=200, callbacks=[early_stopping, plateau], verbose=0)",DL,Tensorflow,pourchot/tps-11-2022-neural-network,https://www.kaggle.com/pourchot/tps-11-2022-neural-network
tabular-playground-series-nov-2022,classification,binary-classification,"The task is to blend various submission files to improve model predictions for a binary classification problem, using ground truth labels for validation.",Tabular,Log Loss,"The dataset consists of a folder containing submission files with predictions for a binary classification task. The ground truth labels are provided in train_labels.csv. Each submission file corresponds to a logloss score based on the first half of the prediction rows against the ground truth labels. The goal is to blend these submission files to enhance prediction accuracy, with the ability to validate improvements using the ground truth labels for the first half of the rows.",['pred'],"[""import submission files"", ""reshape data for model"", ""split data into training and validation sets""]","model = tf.keras.Sequential([\n    InputLayer(input_shape=im_shape),\n    Conv1D(filters=32, kernel_size=4, activation=ACTIVATION_FUNCTION),\n    AvgPool1D(3),\n    Dropout(0.3),\n    Flatten(),\n    Dense(150, activation=ACTIVATION_FUNCTION),\n    Dense(100, activation=ACTIVATION_FUNCTION),\n    Dense(50, activation=ACTIVATION_FUNCTION),\n    Dense(1, activation=OUTPUT_FUNCTION),\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n    loss=""binary_crossentropy"",\n    metrics=[""accuracy""],\n)\n\nhistory = model.fit(\n        X_train, y_train,\n        epochs=15,\n        validation_data=(X_val, y_val)\n)",DL,Tensorflow,francescoliveras/tps-nov-2022-en-es,https://www.kaggle.com/francescoliveras/tps-nov-2022-en-es
tabular-playground-series-nov-2022,classification,binary-classification,"The task is to blend various submission files to improve model predictions for a binary classification problem, using ground truth labels for evaluation.",Tabular,Log Loss,"The dataset consists of a folder containing submission files with predictions for a binary classification task. The ground truth labels are provided in train_labels.csv. Each submission file corresponds to a logloss score for the first half of the prediction rows against the ground truth labels. The goal is to blend these submissions to enhance prediction accuracy, with the ability to evaluate improvements using the ground truth for the first half of the rows.",['pred'],"[""drop unnecessary columns"", ""scale features using MinMaxScaler"", ""split data into training and validation sets""]","model = nn.Sequential(
    nn.Linear(299, out_features),
    nn.ReLU(),
    nn.Linear(out_features, 1),
    nn.Sigmoid()
)
optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])
model.to(device)
model.fit()",DL,Pytorch,hosseinbehjat/pytorch-optimization-using-optuna-tps-nov-2022,https://www.kaggle.com/hosseinbehjat/pytorch-optimization-using-optuna-tps-nov-2022
tabular-playground-series-nov-2022,classification,binary-classification,"The task is to blend various submission files to improve model predictions for a binary classification problem, using ground truth labels for evaluation.",Tabular,Log Loss,"The dataset consists of a folder containing submission files with binary predictions and a file named train_labels.csv that provides the ground truth labels for the first half of the rows in the submission files. The goal is to blend these submissions to enhance prediction accuracy, with the ability to evaluate improvements using the provided ground truth labels.",['pred'],"[""standardize features using StandardScaler"", ""drop irrelevant features based on importance"", ""apply label encoding to target variable"", ""clip values to avoid extreme probabilities""]","model = TabNetClassifier(optimizer_fn=torch.optim.Adam,optimizer_params=dict(lr=2e-2),scheduler_params={""step_size"":10,""gamma"":0.9},scheduler_fn=torch.optim.lr_scheduler.StepLR,mask_type='sparsemax')
model.fit(X_train,y_train,eval_set=[(X_train, y_train), (X_test, y_test)],eval_name=['train', 'valid'],eval_metric=['logloss'],max_epochs=EPOCHS , patience=50,batch_size=BATCH_SIZE, virtual_batch_size=128,num_workers=0,weights=1,drop_last=False)",DL,Pytorch,slythe/tabnet-template-meta-modelling,https://www.kaggle.com/slythe/tabnet-template-meta-modelling
tabular-playground-series-sep-2022,regression,continuous-regression,"The task is to predict the number of items sold for four products across two stores in six countries over the year 2021, based on historical sales data.",Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for four items from two competing stores located in six different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures various effects seen in real-world data, such as seasonality and holiday effects, and is fictional but designed to reflect realistic sales patterns.",['num_sold'],"[""parse dates in the dataset"", ""replace spaces and colons in product names"", ""group sales data by date, country, store, and product"", ""create new features for month, day, and day of the week"", ""encode holiday names and create binary indicators for holidays"", ""drop unnecessary columns after feature engineering""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)",DL,Tensorflow,juhjoo/4-41-tpg-sep-ridge-lasso-linear-elastic-ensemble,https://www.kaggle.com/juhjoo/4-41-tpg-sep-ridge-lasso-linear-elastic-ensemble
tabular-playground-series-sep-2022,regression,continuous-regression,The task is to predict the number of items sold for four products across two stores in six countries over the year 2021.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for four items from two competing stores located in six different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures various effects seen in real-world data, such as seasonality and holiday effects.",['num_sold'],"[""extract year, month, day, quarter, week, and day of the week from the date"", ""create binary features for month start, month end, quarter start, quarter end, year start, and year end"", ""apply sine and cosine transformations to the month feature"", ""identify holidays and create a binary holiday feature"", ""create a binary feature for weekends"", ""drop unnecessary columns""]","task = Task('reg')
roles = {'target': 'num_sold', 'drop': ['row_id', 'country']}
automl = TabularUtilizedAutoML(task = task, general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned', 'cb_tuned'], ['linear_l2', 'lgb', 'lgb_tuned', 'cb_tuned']]}, timeout = TIMEOUT, cpu_limit = 4, reader_params = {'n_jobs': 4, 'cv': N_FOLDS, 'random_state': RANDOM_STATE}, lgb_params = {'default_params': {'num_trees': 100000, 'learning_rate': 0.005, 'early_stopping_rounds': 1000}, 'freeze_defaults': True})
oof_pred = automl.fit_predict(train, roles = roles, verbose=1)
test_pred = automl.predict(test)
test['num_sold']=test_pred",DL,Tensorflow,mikita1580/tabular-playground-sep-lama,https://www.kaggle.com/mikita1580/tabular-playground-sep-lama
tabular-playground-series-sep-2022,regression,continuous-regression,The task is to predict the number of items sold for four products across two stores in six countries over the year 2021.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,The dataset contains sales data for four items from two competing stores located in six different countries. It includes training data with sales figures for each date-country-store-item combination and a test set for predicting future sales. The dataset captures various real-world effects such as seasonality and holiday impacts.,['num_sold'],"[""drop the row_id column from the training and test datasets"", ""convert the pandas DataFrame to a TensorFlow dataset for model training""]","rf_model = tfdf.keras.RandomForestModel(hyperparameter_template=""better_default"",num_trees=75, task=tfdf.keras.Task.REGRESSION)
rf_model.compile(metrics=[""SMAPE""])
rf_model.fit(x=train_tfds)",DL,Tensorflow,mpwolke/smape-tf-df-plot-model-colab,https://www.kaggle.com/mpwolke/smape-tf-df-plot-model-colab
tabular-playground-series-sep-2022,regression,continuous-regression,The task is to predict the number of items sold for four products across two stores in six countries over the year 2021.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,The dataset contains sales data for four items from two competing stores located in six different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures various real-world effects such as seasonality and holiday impacts.,['num_sold'],"[""normalize the target variable using standard scaling"", ""pivot the training data to create a time series format"", ""reshape the data for LSTM input""]","lstm_mult_model2 = keras.models.Sequential([
    keras.layers.Bidirectional(keras.layers.LSTM(100, activation='tanh', return_sequences=True), input_shape=(2*365, 1)),
    keras.layers.Dropout(rate=0.5),
    keras.layers.Bidirectional(keras.layers.LSTM(50, activation='tanh', return_sequences=True)),
    keras.layers.Dropout(rate=0.5),
    keras.layers.Bidirectional(keras.layers.LSTM(20, activation='tanh')),
    keras.layers.Dropout(rate=0.5),
    keras.layers.Dense(365)
])

optimizer = keras.optimizers.Adam(lr=0.0005)

lstm_mult_model2.compile(loss=""mse"", optimizer= optimizer, metrics=[smape_loss, last_time_step_mse, ])

history_mult_lstm2 = lstm_mult_model2.fit(X_train, Y_train, epochs=200, batch_size=64,
                    validation_data=(X_valid, Y_valid), callbacks=[early_stopping])",DL,Tensorflow,mikita1580/tabular-playground-sep-nn,https://www.kaggle.com/mikita1580/tabular-playground-sep-nn
tabular-playground-series-sep-2022,regression,continuous-regression,The task is to predict the number of items sold for four products across two stores in six countries over the year 2021.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,The dataset contains sales data for four items from two competing stores located in six different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures various real-world effects such as seasonality and holiday impacts.,['num_sold'],"[""convert date column to datetime format""]","rf_model = tfdf.keras.RandomForestModel(hyperparameter_template=""better_default"",num_trees=75, task=tfdf.keras.Task.REGRESSION)
rf_model.compile(metrics=[""SMAPE""])
rf_model.fit(x=train_tfds)",DL,Tensorflow,mpwolke/one-not-smart-kaggler-goose,https://www.kaggle.com/mpwolke/one-not-smart-kaggler-goose
tabular-playground-series-sep-2022,regression,continuous-regression,The task is to predict the number of items sold for four products across two stores in six countries for the year 2021 based on historical sales data.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset consists of sales data for four items from two competing stores located in six different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures various effects seen in real-world data, such as seasonality and holiday effects, and is fictional but designed to reflect realistic sales patterns.",['num_sold'],"[""convert date column to datetime format"", ""create time index for time series"", ""add day of week, week of year, and month as categorical features"", ""calculate log of num_sold for normalization"", ""compute average sales by country, store, and product"", ""identify holidays and create holiday-related features""]","tft = TemporalFusionTransformer.from_dataset(training,learning_rate=LEARNING_RATE,lstm_layers=2,hidden_size=16,attention_head_size=2,dropout=0.2,hidden_continuous_size=8,output_size=1,loss=SMAPE(),log_interval=10,reduce_on_plateau_patience=4)
trainer.fit(tft,train_dataloaders=train_dataloader,val_dataloaders=val_dataloader)",DL,Pytorch,tomwarrens/temporal-fusion-transformer-in-pytorch,https://www.kaggle.com/tomwarrens/temporal-fusion-transformer-in-pytorch
tabular-playground-series-sep-2022,regression,continuous-regression,The task is to predict the number of items sold for four products across two stores in six countries over the year 2021 based on historical sales data.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset consists of sales data for four items from two competing stores located in six different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures various real-world effects such as seasonality and holiday impacts, and is fictional in nature. The training set is used to train the model, while the test set is used for evaluation.",['num_sold'],"[""convert date column to datetime"", ""extract year, month, day, and day of the week from date"", ""standardize numerical features"", ""encode categorical features using ordinal encoding""]","model = NBeatsModel(n_blocks, n_stacks, input_dim, output_dim, width)
model.compile(optimizer=optimizer, loss=criterion)
model.fit(train_dl, epochs=num_epochs)",DL,Pytorch,masatomurakawamm/n-beats-dnn-for-univariate-time-series-forecast,https://www.kaggle.com/masatomurakawamm/n-beats-dnn-for-univariate-time-series-forecast
tabular-playground-series-sep-2022,regression,continuous-regression,The task is to predict the number of items sold for four products across two stores in six countries over the year 2021.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for each date-country-store-item combination, including a training set with historical sales and a test set for predictions. It captures various effects such as seasonality and holidays, and is structured in CSV format with a size of 5.73 MB.",['num_sold'],"[""extract date features from the date column"", ""one-hot encode categorical variables"", ""fill missing values with column mean"", ""apply log transformation to target variable""]","regressor = TabNetRegressor(verbose=0,seed=42)
regressor.fit(X_train=X_train, y_train=y_train,
              eval_set=[(X_valid, y_valid)],
              patience=50, max_epochs=100,
              eval_metric=['mse'])",DL,Pytorch,sidharkal/playground-notebook,https://www.kaggle.com/sidharkal/playground-notebook
tabular-playground-series-sep-2022,regression,continuous-regression,The task is to predict the number of items sold for four products across two stores in six countries over the year 2021.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,The dataset contains sales data for four items from two competing stores located in six different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures various real-world effects such as seasonality and holiday impacts.,['num_sold'],"[""convert date column to datetime"", ""extract year, month, day from date"", ""apply log transformation to target variable"", ""merge additional datasets for economic indicators"", ""target encode categorical features""]","model = TabNetRegressor(**final_params)
model.fit(X_train=X, y_train=target,
          patience=TabNet_params['patience'], max_epochs=epochs,
          eval_metric=['rmse'])",DL,Pytorch,naiborhujosua/tps-10-tabnet-xgboost,https://www.kaggle.com/naiborhujosua/tps-10-tabnet-xgboost
tabular-playground-series-sep-2022,regression,continuous-regression,"The task is to predict the number of items sold for four products across two stores in six countries over a year, based on historical sales data.",Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for four items sold in two stores across six countries for the year 2021. It includes features such as date, country, store, product, and the number of items sold. The training set provides historical sales data, while the test set requires predictions for the same features. The dataset captures various effects seen in real-world sales data, including seasonality and holiday effects.",['num_sold'],"[""check for null values"", ""apply one-hot encoding to categorical features"", ""create date-based features such as day of the week and month"", ""calculate ratio features based on historical sales data"", ""shuffle and split the dataset into training and testing sets""]","class Net(T.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.hid1 = T.nn.Linear(20, 60)
        self.hid2 = T.nn.Linear(60, 20)
        self.oupt = T.nn.Linear(20, 1)

        T.nn.init.xavier_uniform_(self.hid1.weight)
        T.nn.init.zeros_(self.hid1.bias)
        T.nn.init.xavier_uniform_(self.hid2.weight)
        T.nn.init.zeros_(self.hid2.bias)
        T.nn.init.xavier_uniform_(self.oupt.weight)
        T.nn.init.zeros_(self.oupt.bias)

    def forward(self, x):
        z = T.relu(self.hid1(x))
        z = T.relu(self.hid2(z))
        z = self.oupt(z)  # no activation
        return z

rate_learning = 1e-2
weight_decay  = 1e-4
model = Net()
criterion = T.nn.MSELoss(reduction='mean')
optimizer = T.optim.Adam(model.parameters(), lr=rate_learning, weight_decay=weight_decay)
epochs = 3
def train_model(df_metrics, loader, epochs):
    for epoch in range(epochs):
        epoch_loss = 0
        for x, y in loader:
            optimizer.zero_grad()
            yhat = model.forward(x.float())
            loss = criterion(yhat, y.reshape(y.shape[0],1).float())
            epoch_loss += loss.data 
            loss.backward()
            optimizer.step()
        loss_train = ((epoch_loss / len(loader)) ** (1/2)).item()
        df_metrics = df_metrics.append({'loss_train': loss_train}, ignore_index=True)
    return df_metrics
df_metrics = train_model(df_metrics, loader_trn, epochs)
for x, y in loader_prd:
    yhat = model(x.float())
    submission = pd.DataFrame({'num_sold':yhat.detach().numpy().reshape(1,-1)[0]}, index=df_test.index)
    submission.to_csv('submission.csv')",DL,Pytorch,alexishenko/pytorch-deep-learning-net,https://www.kaggle.com/alexishenko/pytorch-deep-learning-net
tabular-playground-series-aug-2022,classification,binary-classification,The task is to predict the probability of product failure based on various product attributes and measurement values from lab testing.,Tabular,AUCROC,"This dataset contains results from a product testing study, including product attributes and measurement values for each product. Each product is tested in a simulated environment to determine if it fails after absorbing a certain amount of fluid. The training data includes the target variable indicating failure, while the test set requires predicting the likelihood of failure for new product codes based on their lab test results.",['failure'],"[""impute missing values for loading based on probability density function overlap"", ""impute measurements 3 through 9 and 17 using correlations"", ""scale features using robust scaling and standard scaling""]","inputs = Input(shape=(5))
outputs = Dense(1, activation='sigmoid')(inputs)
model = Model(inputs, outputs)
model.compile(optimizer=OPTIMIZER, metrics=METRIC, loss=LOSS)
history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCHS, verbose=VERBOSE, batch_size=BATCH_SIZE, shuffle=True, callbacks=callbacks, class_weight=class_weight)",DL,Tensorflow,michaelbryantds/auc-custom-loss-function-top-4,https://www.kaggle.com/michaelbryantds/auc-custom-loss-function-top-4
tabular-playground-series-aug-2022,classification,binary-classification,The task is to predict the probability of product failure based on various lab testing measurements and attributes.,Tabular,AUCROC,"This dataset contains results from a product testing study, where each product is identified by a product code and has associated attributes and measurement values from lab tests. The goal is to predict whether a product will fail based on these measurements in a simulated real-world environment.",['failure'],"[""fill missing values with column mean"", ""convert object columns to category"", ""apply one-hot encoding to categorical variables""]","model4 =   tf.keras.Sequential([
  tf.keras.layers.Dense(50, activation=""relu""),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(50, activation=""relu"", bias_regularizer='l2'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1, activation=""sigmoid"")
])

model4.compile(loss=""binary_crossentropy"",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=[""accuracy""])
model4.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))",DL,Tensorflow,kevinmorgado/top-20-product-failure-prediction,https://www.kaggle.com/kevinmorgado/top-20-product-failure-prediction
tabular-playground-series-aug-2022,classification,binary-classification,The task is to predict the probability of product failure based on various product attributes and lab testing results.,Tabular,AUCROC,"This dataset contains results from a product testing study, including product attributes and measurement values for each product. The goal is to predict individual product failures based on lab test results for new product codes. The training data includes a target variable indicating failure, while the test set is used to predict the likelihood of failure for each product ID.",['failure'],"[""combine train and test sets"", ""drop unnecessary columns"", ""fill missing values using KNN imputation"", ""transform skewed features using logarithm"", ""one-hot encode categorical variables"", ""scale features using standard scaling"", ""split data into training and test sets""]","model = tf.keras.Sequential([tf.keras.Input(45),tf.keras.layers.Dense(100, activation = 'relu'),tf.keras.layers.Dropout(0.3),tf.keras.layers.Dense(50, activation = 'relu'),tf.keras.layers.Dropout(0.2),tf.keras.layers.Dense(1, activation= 'sigmoid')])
model.compile(optimizer= tf.keras.optimizers.Adam(),loss='binary_crossentropy',metrics=[tf.keras.metrics.AUC(name='auc')])
history = model.fit(train_final,target,validation_split=0.20,batch_size=BATCH_SIZE,epochs=EPOCHS)",DL,Tensorflow,anubhavgoyal10/tps-august-ann-score-0-584,https://www.kaggle.com/anubhavgoyal10/tps-august-ann-score-0-584
tabular-playground-series-aug-2022,classification,binary-classification,The task is to predict the probability of product failure based on various product attributes and measurement values from lab tests.,Tabular,AUCROC,"This dataset contains results from a product testing study, including product attributes and measurement values for each product. The goal is to predict individual product failures based on these attributes and measurements. The training data includes a target variable indicating failure, while the test set is used to predict the likelihood of failure for new product codes.",['failure'],"[""create binary indicators for missing values in measurement columns"", ""calculate area from product attributes"", ""fill missing values using linear regression and KNN imputation"", ""apply Weight of Evidence encoding to categorical variables"", ""scale features using standard scaling""]","model3 = keras.Sequential([layers.BatchNormalization(input_shape=[x_train.shape[1]]),layers.Dense(512, activation='swish'),layers.BatchNormalization(),layers.Dropout(0.3),layers.Dense(512, activation='swish'),layers.BatchNormalization(),layers.Dropout(0.3),layers.Dense(512, activation='swish'),layers.BatchNormalization(),layers.Dropout(0.3),layers.Dense(512, activation='swish'),layers.BatchNormalization(),layers.Dropout(0.3),layers.Dense(512, activation='swish'),layers.BatchNormalization(),layers.Dropout(0.3),layers.Dense(256, activation='swish'),layers.BatchNormalization(),layers.Dense(1, activation='sigmoid'),]);model3.compile(optimizer='adam',loss='binary_crossentropy',metrics=[keras.metrics.AUC(name = 'auc'),]);history = model3.fit(train_X, train_y,validation_data=(valid_X, valid_y),batch_size=512,epochs=200,callbacks=[early_stopping],);",DL,Tensorflow,mehrankazeminia/tps22aug-logisticr-lgbm-keras,https://www.kaggle.com/mehrankazeminia/tps22aug-logisticr-lgbm-keras
tabular-playground-series-aug-2022,classification,binary-classification,The task is to predict the probability of product failure based on various product attributes and measurement values from lab testing.,Tabular,AUCROC,"This dataset contains results from a product testing study, including product attributes and measurement values for each product. Each product is tested in a simulated environment to determine if it fails after absorbing a certain amount of fluid. The training data includes the target variable indicating failure, while the test set requires predicting the likelihood of failure for new product codes based on their lab test results.",['failure'],"[""impute missing values with median"", ""apply one-hot encoding to categorical features"", ""scale numerical features using standardization""]","model = keras.Sequential([layers.BatchNormalization(),layers.Dense(31, activation='tanh', input_shape=[31]),layers.Dropout(rate=0.2),layers.Dense(80, activation='tanh'),layers.Dropout(rate=0.1),layers.Dense(60, activation='tanh'),layers.Dropout(rate=0.1),layers.Dense(30, activation='tanh'),layers.Dropout(rate=0.1),layers.Dense(20, activation='tanh'),layers.Dropout(rate=0.1),layers.Dense(8, activation='relu'),layers.Dropout(rate=0.1),layers.Dense(20, activation='tanh'),layers.Dropout(rate=0.1),layers.Dense(5, activation='tanh'),layers.Dropout(rate=0.1),layers.Dense(2, activation='relu'),layers.Dropout(rate=0.1),layers.Dense(1, activation='sigmoid')]);model.compile(optimizer='sgd',loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),metrics=tf.keras.metrics.AUC());history=my_model.fit(X_train, y_train,validation_data=(X_valid, y_valid),epochs=1000,batch_size=512,verbose=0,callbacks=[early_stopping])",DL,Tensorflow,imnaho/predict-with-neural-network,https://www.kaggle.com/imnaho/predict-with-neural-network
tabular-playground-series-aug-2022,classification,binary-classification,The task is to predict the probability of product failure based on various product attributes and measurement values from lab testing.,Tabular,AUCROC,"This dataset contains results from a product testing study, including product attributes and measurement values for each product. Each product is tested in a simulated environment to determine if it fails after absorbing a certain amount of fluid. The training data includes a target variable indicating failure, while the test set requires predicting the likelihood of failure for new product codes based on their lab test results.",['failure'],"[""create binary indicators for missing values"", ""calculate area as a product of attributes"", ""fill missing values using regression and imputation"", ""encode categorical variables using label encoding"", ""scale features using power transformation"", ""apply PCA for dimensionality reduction""]","model = NeuralNet(X_tr.shape[1], (128, 64), (0.2, 0.2))
loss_fn   = AUCMLoss()
optimizer = PESG(model, 
                 loss_fn=loss_fn,
                 lr=lr, 
                 momentum=0.9,
                 margin=margin, 
                 epoch_decay=epoch_decay, 
                 weight_decay=weight_decay)",DL,Pytorch,sejoongkim/pytorch-libauc-neural-network,https://www.kaggle.com/sejoongkim/pytorch-libauc-neural-network
tabular-playground-series-aug-2022,classification,binary-classification,The task is to predict the probability of product failure based on various measurement values and product attributes from a testing study.,Tabular,AUCROC,"This dataset contains results from a product testing study, including product attributes and measurement values for each product. The goal is to predict whether a product will fail based on its lab test results. The training data includes a target variable indicating failure, while the test set is used to predict failure probabilities for new product codes.",['failure'],"[""impute missing values using SimpleImputer or IterativeImputer"", ""normalize continuous variables"", ""categorify categorical variables""]","learn = tabular_learner(dls, config=my_config, metrics=[accuracy])
learn.fit_one_cycle(20, 5e-3, cbs=SaveModelCallback(fname='kaggle_tps_2022_aug', with_opt=True))",DL,Pytorch,casati8/kaggle-tps2022-aug-fastai-baseline,https://www.kaggle.com/casati8/kaggle-tps2022-aug-fastai-baseline
tabular-playground-series-aug-2022,classification,binary-classification,The task is to predict the probability of product failure based on various attributes and measurement values from a product testing study.,Tabular,AUCROC,"This dataset contains results from a product testing study, including product attributes and measurement values for each product. The goal is to predict individual product failures based on lab test results. The training data includes a target variable indicating failure, while the test set is used to predict the likelihood of failure for new product codes.",['failure'],"[""fill missing values with mean for numeric columns"", ""fill missing values with mode for categorical columns"", ""drop unnecessary columns"", ""scale features using standard scaling"", ""apply PCA for dimensionality reduction"", ""split data into training and validation sets""]","model = Net()
optimizer = Adam(params=model.parameters(), lr=0.01, weight_decay=0.0001)
loss_func = BCELoss()
train(model, train_dataloader, 1000)",DL,Pytorch,zhangcheche/tps-8-eda-pytorch-baseline,https://www.kaggle.com/zhangcheche/tps-8-eda-pytorch-baseline
tabular-playground-series-aug-2022,classification,binary-classification,The task is to predict the probability of product failure based on various product attributes and measurement values from lab tests.,Tabular,AUCROC,"This dataset contains results from a product testing study, including product attributes and measurement values for each product. The goal is to predict individual product failures based on lab test results for new product codes. The training data includes a target variable indicating failure, while the test set is used to predict the likelihood of failure for each product ID.",['failure'],"[""create missing value indicators"", ""calculate area from attributes"", ""fill missing values using linear regression"", ""fill remaining missing values using KNN"", ""calculate statistical features from measurements"", ""apply weight of evidence encoding""]","model = TabNetClassifier(n_d=15, n_a=15, n_steps=3, gamma=1.3, n_independent=2, n_shared=2, momentum=0.02, clip_value=None, lambda_sparse=1e-3, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=1e-3, weight_decay=1e-2), scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau, scheduler_params={'mode':'max', 'factor':0.9, 'patience':5, 'eps':1e-4, 'verbose':True}, mask_type='entmax', seed=CFG.seed)
model.fit(np.array(x_train), np.array(y_train.ravel()), eval_set=[(np.array(x_valid), np.array(y_valid.ravel()))], max_epochs=CFG.max_epochs, patience=50, batch_size=CFG.batch_size, virtual_batch_size=32, eval_metric=['auc'])",DL,Pytorch,medali1992/aug-tps-tabnetclassifier,https://www.kaggle.com/medali1992/aug-tps-tabnetclassifier
tabular-playground-series-aug-2022,classification,binary-classification,The task is to predict the probability of product failure based on various product attributes and measurement values from lab tests.,Tabular,AUCROC,"This dataset contains results from a product testing study, including product attributes and measurement values for each product. The goal is to predict whether a product will fail based on its lab test results. The training data includes a target variable indicating failure, while the test set is used to predict the likelihood of failure for new product codes.",['failure'],"[""one-hot encode categorical variables"", ""create interaction features"", ""scale numerical features using standardization""]","model = Net(input_size, num_classes)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
criterion = nn.BCELoss()
for epoch in range(num_epochs):
    for (x, y) in generate_batches(X_train, y_train, 350):
        inputs = Variable(torch.from_numpy(x)).float()
        labels = Variable(torch.from_numpy(y)).type(torch.FloatTensor)
        outputs = model(inputs)
        loss = criterion(torch.squeeze(outputs), labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()",DL,Pytorch,kobzetsu/pytorch-for-start,https://www.kaggle.com/kobzetsu/pytorch-for-start
tabular-playground-series-apr-2022,classification,binary-classification,The goal is to classify 60-second sequences of sensor data to determine if a subject is in Activity State 0 or 1.,Tabular,AUCROC,"The dataset consists of approximately 26,000 recordings from thirteen biological sensors, each lasting 60 seconds, collected from nearly 1,000 participants. It includes a training set with unique sequence IDs, subject IDs, time steps, and sensor values, along with a labels file indicating the activity state for each sequence. The test set contains around 12,000 sequences for which predictions need to be made.",['state'],"[""merge training data with labels"", ""scale features using standard scaler""]","mod = Sequential()
mod.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.L2(1e-4), input_dim=X_train.shape[1]))
mod.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.AUC(name = 'auc')])
mod.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), verbose=False)",DL,Tensorflow,kellibelcher/time-series-classification-with-lstms-sensor-eda,https://www.kaggle.com/kellibelcher/time-series-classification-with-lstms-sensor-eda
tabular-playground-series-apr-2022,classification,binary-classification,The task is to classify 60-second sequences of sensor data to determine whether a subject was in one of two activity states during that time.,Tabular,AUCROC,"The dataset consists of approximately 26,000 recordings of 60-second sequences from thirteen biological sensors, collected from nearly 1,000 experimental participants. It includes a training set with labels indicating the state for each sequence, a test set for predictions, and a sample submission file. Each sequence is uniquely identified, and the sensor values are recorded at one-second intervals.",['state'],"[""reduce memory usage"", ""standardize sensor values"", ""reshape data for LSTM input""]","x_input = Input(shape=(train_df.shape[-2:]))
x1 = Bidirectional(LSTM(units=512, return_sequences=True))(x_input)
x2 = Bidirectional(LSTM(units=256, return_sequences=True))(x1)
z1 = Bidirectional(GRU(units=256, return_sequences=True))(x1)
c = Concatenate(axis=2)([x2, z1])
x3 = Bidirectional(LSTM(units=128, return_sequences=True))(c)
x4 = GlobalMaxPooling1D()(x3)
x5 = Dense(units=128, activation='selu')(x4)
x_output = Dense(1, activation='sigmoid')(x5)
model = Model(inputs=x_input, outputs=x_output, name='lstm_model')
model.compile(optimizer=""adam"", loss=""binary_crossentropy"", metrics='AUC')
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, verbose=VERBOSE, batch_size=BATCH_SIZE, callbacks=[lr, chk_point, es])",DL,Tensorflow,javigallego/tps-apr22-eda-fe-lstm-tutorial,https://www.kaggle.com/javigallego/tps-apr22-eda-fe-lstm-tutorial
tabular-playground-series-apr-2022,classification,binary-classification,The task is to classify 60-second sequences of sensor data to determine whether a subject was in one of two activity states during that time.,Tabular,AUCROC,"The dataset consists of approximately 26,000 recordings of 60-second sequences from thirteen biological sensors, collected from nearly one thousand experimental participants. Each recording is identified by a unique sequence ID, and the dataset includes the subject ID, time steps, and sensor values. The training labels indicate the state associated with each sequence, while the test set requires predictions for the state variable for around 12,000 sequences.",['state'],"[""merge training data with labels"", ""check for missing values"", ""create lag features for sensors"", ""scale sensor values using standard scaling"", ""reshape data for model input""]","x_input = Input(shape=(train.shape[-2:]))
    x0 = Bidirectional(LSTM(units=256, return_sequences=True))(x_input)
    x1 = Bidirectional(LSTM(units=512, return_sequences=True))(x0)
    x2 = Bidirectional(LSTM(units=256, return_sequences=True))(x1)
    z1 = Bidirectional(GRU(units=256, return_sequences=True))(x1)
    c = Concatenate(axis=2)([x2, z1])
    x3 = Bidirectional(LSTM(units=256, return_sequences=True))(c)
    x4 = GlobalMaxPooling1D()(x3)
    x5 = Dense(units=128, activation='selu')(x4)
    x_output = Dense(1, activation='sigmoid')(x5)
    model = Model(inputs=x_input, outputs=x_output, name='lstm_model')
    model.compile(optimizer=""adam"", loss=""binary_crossentropy"", metrics='AUC')
    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=25, verbose=64, batch_size=BATCH_SIZE, callbacks=[lr, chk_point, es])",DL,Tensorflow,tyrionlannisterlzy/xgboost-dnn-ensemble-lb-0-980,https://www.kaggle.com/tyrionlannisterlzy/xgboost-dnn-ensemble-lb-0-980
tabular-playground-series-apr-2022,classification,binary-classification,The task is to predict the probability of a subject being in a specific activity state based on 60-second sequences of sensor data.,Tabular,AUCROC,"The dataset consists of approximately 26,000 recordings from thirteen biological sensors, each lasting 60 seconds, collected from nearly one thousand experimental participants. It includes a training set with unique identifiers for sequences and subjects, time steps, sensor values, and corresponding class labels indicating the activity state. The test set contains similar sequences for which predictions are required.",['state'],"[""merge training data with labels"", ""create lag features for sensors"", ""fill missing values with zero"", ""scale features using standard scaling"", ""reshape data for model input""]","input_layer = Input(shape=(train.shape[-2:]))
x1 = Bidirectional(LSTM(768, return_sequences=True))(input_layer)
x21 = Bidirectional(LSTM(512, return_sequences=True))(x1)
x22 = Bidirectional(LSTM(512, return_sequences=True))(input_layer)
l2 = Concatenate(axis=2)([x21, x22])
x31 = Bidirectional(LSTM(384, return_sequences=True))(l2)
x32 = Bidirectional(LSTM(384, return_sequences=True))(x21)
l3 = Concatenate(axis=2)([x31, x32])
x41 = Bidirectional(LSTM(256, return_sequences=True))(l3)
x42 = Bidirectional(LSTM(128, return_sequences=True))(x32)
l4 = Concatenate(axis=2)([x41, x42])
l5 = Concatenate(axis=2)([x1, l2, l3, l4])
x7 = Dense(128, activation='selu')(l5)
x8 = Dropout(0.1)(x7)
output_layer = Dense(units=1, activation=""sigmoid"")(x8)
model = Model(inputs=input_layer, outputs=output_layer, name='DNN_Model')
model.compile(optimizer=""adam"",loss=""binary_crossentropy"", metrics=[AUC(name = 'auc')])
history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=30, batch_size = 64, callbacks = [es,lr],verbose = False)",DL,Tensorflow,hamzaghanmi/tps-april-tensorflow-bi-lstm,https://www.kaggle.com/hamzaghanmi/tps-april-tensorflow-bi-lstm
tabular-playground-series-apr-2022,classification,binary-classification,The task is to classify 60-second sequences of sensor data to determine whether a subject was in one of two activity states during that time.,Tabular,AUCROC,"The dataset consists of approximately 26,000 recordings of 60-second sequences from thirteen biological sensors, collected from nearly one thousand experimental participants. It includes a training set with unique identifiers for each sequence and subject, time steps for the recordings, and sensor values. The training labels indicate the state associated with each sequence, while the test set contains sequences for which predictions need to be made.",['state'],"[""create new features based on sensor values"", ""standardize sensor values using StandardScaler"", ""reshape input data for model training""]","input_layer = Input(shape=(X_train.shape[-2:]))
x1 = Bidirectional(LSTM(768, return_sequences=True))(input_layer)
x21 = Bidirectional(LSTM(512, return_sequences=True))(x1)
x22 = Bidirectional(LSTM(512, return_sequences=True))(input_layer)
l2 = Concatenate(axis=2)([x21, x22])
x31 = Bidirectional(LSTM(384, return_sequences=True))(l2)
x32 = Bidirectional(LSTM(384, return_sequences=True))(x21)
l3 = Concatenate(axis=2)([x31, x32])
x41 = Bidirectional(LSTM(256, return_sequences=True))(l3)
x42 = Bidirectional(LSTM(128, return_sequences=True))(x32)
l4 = Concatenate(axis=2)([x41, x42])
l5 = Concatenate(axis=2)([x1, l2, l3, l4])
x7 = Dense(128, activation='selu')(l5)
x8 = Dropout(0.3)(x7)
output_layer = Dense(units=1, activation=""sigmoid"")(x8)
model = Model(inputs=input_layer, outputs=output_layer, name='DNN_Model')
model.compile(optimizer=""adam"",loss=""binary_crossentropy"", metrics=[AUC(name = 'auc')])
model.fit(X_train_part, y_train_part, validation_data=(X_valid, y_valid), epochs=30, batch_size=64, callbacks=[es,lr], verbose=False)",DL,Tensorflow,hasanbasriakcay/tpsapr22-fe-pseudo-labels-bi-lstm,https://www.kaggle.com/hasanbasriakcay/tpsapr22-fe-pseudo-labels-bi-lstm
tabular-playground-series-apr-2022,classification,binary-classification,The task is to classify 60-second sequences of sensor data to determine whether a subject was in one of two activity states during the sequence.,Tabular,AUCROC,"The dataset consists of approximately 26,000 recordings of 60-second sequences from thirteen biological sensors, collected from nearly one thousand experimental participants. It includes a training set with unique identifiers for each sequence and subject, as well as the sensor values at one-second intervals. The training labels indicate the state associated with each sequence, while the test set contains sequences for which predictions must be made.",['state'],"[""drop unnecessary columns"", ""standardize sensor values"", ""split data into training and testing sets""]","model = LSTM()
model = model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr = lr)
criterion = nn.BCEWithLogitsLoss()
train(epochs, model, optimizer, criterion, sheduler, train_dataloader, test_dataloader)",DL,Pytorch,lordozvlad/tps-apr-pytorch-bidirectional-lstm,https://www.kaggle.com/lordozvlad/tps-apr-pytorch-bidirectional-lstm
tabular-playground-series-apr-2022,classification,binary-classification,The task is to classify 60-second sequences of sensor data to determine whether a subject was in one of two activity states during that time.,Tabular,AUCROC,"The dataset consists of approximately 26,000 recordings of 60-second sequences from thirteen biological sensors, collected from nearly 1,000 experimental participants. Each recording is identified by a unique sequence ID and includes time steps for each second, along with sensor values. The training labels indicate the state associated with each sequence, while the test set requires predictions for the state of approximately 12,000 sequences.",['state'],"[""check for missing values"", ""add new features including lag, difference, rolling mean, and rolling std"", ""drop NaN values after feature creation"", ""scale data using StandardScaler"", ""reshape data into 3D tensors for RNN input"", ""split data into training and validation sets"", ""convert data into tensors"", ""create DataLoader for training, validation, and test sets""]","model_lstm = RNN(input_size, hidden_sizes, sequence_length)
criterion = nn.BCELoss()
optimizer = optim.Adam(model_lstm.parameters(), lr=max_learning_rate)
scheduler = optim.lr_scheduler.OneCycleLR(optimizer,
                                          max_lr = max_learning_rate,
                                          epochs = epochs,
                                          steps_per_epoch = len(dataloaders[""train""]),
                                          pct_start = 0.2,
                                          anneal_strategy = ""cos"")
train_model(model = model_lstm,
            trainloader = dataloaders[""train""],
            validloader = dataloaders[""val""],
            criterion = criterion,
            optimizer = optimizer,
            scheduler = scheduler,
            epochs = epochs,
            device = my_device,
            print_every = 2)",DL,Pytorch,ahmetcelik158/tps-apr-22-lstm-with-pytorch,https://www.kaggle.com/ahmetcelik158/tps-apr-22-lstm-with-pytorch
tabular-playground-series-apr-2022,classification,binary-classification,The task is to classify 60-second sequences of sensor data to determine whether a subject was in one of two activity states during that time.,Tabular,AUCROC,"The dataset consists of approximately 26,000 recordings of 60-second sequences from thirteen biological sensors, collected from nearly one thousand experimental participants. Each recording is identified by a unique sequence ID, and the dataset includes a corresponding labels file that indicates the state associated with each sequence. The test set contains around 12,000 sequences for which predictions need to be made.",['state'],"[""combine training and test data for batch processing"", ""check for missing values"", ""split data into training and validation sets""]","model = LSTM(input_size=7)
model.fit(X_train_np, y_train_np, num_epochs=epoch, sequence_num=sequence_num, batch_size=batch_size)",DL,Pytorch,aboriginal3153/tps-apr-22-lstm-using-pytorch,https://www.kaggle.com/aboriginal3153/tps-apr-22-lstm-using-pytorch
tabular-playground-series-apr-2022,classification,binary-classification,The task is to classify 60-second sequences of sensor data to determine whether a subject was in one of two activity states during that time.,Tabular,AUCROC,"The dataset consists of approximately 26,000 recordings of 60-second sequences from thirteen biological sensors, collected from nearly one thousand experimental participants. It includes a training set with unique identifiers for each sequence and subject, as well as the sensor values at one-second intervals. The training labels indicate the state associated with each sequence, while the test set requires predictions for the state variable for around 12,000 sequences.",['state'],"[""reduce memory usage by optimizing data types"", ""drop unnecessary columns"", ""scale features using RobustScaler""]","model = LSTM(input_size,num_classes,hidden_size,num_classes)
model.to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
torch.cuda.empty_cache()
for epoch in progress_bar(range(num_epochs)):
    model.train()
    for trainX, train_y in train_dataloader:
        outputs = model(trainX.to(device,dtype=torch.float32)).squeeze(-1)
        optimizer.zero_grad()
        loss = criterion(outputs, train_y.to(device,dtype=torch.float32))
        loss.backward()
        optimizer.step()",DL,Pytorch,himanshunayal/lstm-pytorch,https://www.kaggle.com/himanshunayal/lstm-pytorch
tabular-playground-series-apr-2022,classification,binary-classification,The task is to classify 60-second sequences of sensor data to determine whether a subject was in one of two activity states during that time.,Tabular,AUCROC,"The dataset consists of approximately 26,000 recordings of 60-second sequences from thirteen biological sensors, collected from nearly one thousand experimental participants. It includes a training set with unique identifiers for each sequence and subject, time steps for the recordings, and sensor values. The training labels file contains the class labels for each sequence, indicating the associated state. The test set comprises around 12,000 sequences for which predictions need to be made.",['state'],"[""pivot the dataframe for easier access to sequences"", ""apply Gramian Angular Field transformation to convert sensor time series to images""]","model = simplenet()
model.to(cfg.device)
optimizer = Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, amsgrad=False)
scheduler = get_scheduler(cfg, optimizer, train_cv_dl)
criterion = nn.BCEWithLogitsLoss()

for epoch in range(cfg.epochs):
    avg_loss, _ = train_fn(cfg, train_cv_dl, model, criterion, optimizer, epoch, scheduler)
    avg_val_loss, preds_val_cv = valid_fn(cfg, val_cv_dl, model, criterion)",DL,Pytorch,delai50/gramian-angular-field-imaging-cnn2d-0-957-plb,https://www.kaggle.com/delai50/gramian-angular-field-imaging-cnn2d-0-957-plb
tabular-playground-series-mar-2022,regression,continuous-regression,"The task is to predict traffic congestion levels for roadways based on time, space, and directional features.",Tabular,MAE – Mean Absolute Error,"The dataset consists of traffic congestion measurements across 65 roadways in a major U.S. metropolitan area, recorded from April to September 1991. It includes a training set with features such as time, coordinates, direction of travel, and congestion levels, as well as a test set for making predictions. The congestion target is normalized between 0 and 100, and the dataset is provided in CSV format.",['congestion'],"[""convert time to datetime format"", ""remove records from official holidays"", ""filter data for specific weekdays and months"", ""create new time-related features"", ""map average congestion values to the dataset"", ""clip predictions to specified bounds""]","model_nbeats = NBEATSModel(input_chunk_length=504, output_chunk_length=72, generic_architecture=True, num_stacks=10, num_blocks=1, num_layers=4, layer_widths=512, n_epochs=20, nr_epochs_val_period=1, batch_size=32, model_name=""nbeats_run"")
model_nbeats.fit(train, val_series=val, verbose=True)",DL,Tensorflow,abdulravoofshaik/qc-the-notebook-with-four-lines-of-code,https://www.kaggle.com/abdulravoofshaik/qc-the-notebook-with-four-lines-of-code
tabular-playground-series-mar-2022,regression,continuous-regression,"The task is to predict traffic congestion levels for roadways based on time, space, and directional features.",Tabular,MAE – Mean Absolute Error,"The dataset consists of traffic congestion measurements across 65 roadways in a major U.S. metropolitan area, recorded from April to September 1991. It includes a training set with features such as time, coordinates, direction of travel, and congestion levels, as well as a test set for making predictions on a specific day. The congestion target is normalized between 0 and 100.",['congestion'],"[""drop the 'direction' column after cyclical encoding"", ""drop the 'time' column after extracting time features""]","model = keras.Sequential()
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1))
model.compile(optimizer='adam', loss='mean_absolute_error')
model.fit(X_train, y_train, epochs=50, batch_size=32)",DL,Tensorflow,samuelcortinhas/tps-mar-22-traffic-flow-forecasting,https://www.kaggle.com/samuelcortinhas/tps-mar-22-traffic-flow-forecasting
tabular-playground-series-mar-2022,regression,continuous-regression,The task is to predict traffic congestion levels for roadways based on historical data and various features.,Tabular,MAE – Mean Absolute Error,"The dataset consists of traffic congestion measurements across 65 roadways in a major U.S. metropolitan area, recorded from April to September 1991. It includes a training set with features such as time, coordinates, and direction of travel, along with the target congestion levels normalized between 0 and 100. The test set requires predictions for congestion on a specific day, September 30, 1991.",['congestion'],"[""drop the target column from the training set"", ""encode categorical variables using one-hot encoding"", ""scale numerical features using MinMaxScaler"", ""reduce memory usage of the dataset"", ""extract date features from the time column"", ""drop unnecessary columns""]","model = Sequential()
model.add(InputLayer(input_shape=(X.shape[-1])))
model.add(Dense(1024, activation='selu', kernel_initializer=""lecun_normal""))
model.add(Dropout(0.3))
model.add(Dense(512, activation='selu', kernel_initializer=""lecun_normal""))
model.add(Dropout(0.2))
model.add(Dense(256, activation='selu', kernel_initializer=""lecun_normal""))
model.add(Dropout(0.1))
model.add(Dense(128, activation='selu'))
model.add(Dense(1, activation='linear'))
NN_Model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_Model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=callbacks, validation_data=(X_val, y_val), verbose=False)",DL,Tensorflow,abdelrahmanrabah/nn-kfold-model1,https://www.kaggle.com/abdelrahmanrabah/nn-kfold-model1
tabular-playground-series-mar-2022,regression,continuous-regression,"The task is to predict traffic congestion levels for roadways based on historical data, with values ranging from 0 to 100.",Tabular,MAE – Mean Absolute Error,"The dataset consists of traffic congestion measurements across 65 roadways in a major U.S. metropolitan area, recorded from April to September 1991. It includes features such as time, east-west and north-south coordinates, and direction of travel. The target variable is the congestion level, normalized between 0 and 100. The training set contains historical data, while the test set requires predictions for a specific date.",['congestion'],"[""convert time to datetime format"", ""create rolling mean features"", ""create Fourier features"", ""one-hot encode categorical variables"", ""normalize numerical features""]","normalizer = keras.layers.Normalization(input_shape=input_shape, axis=None)
model = keras.Sequential([
    normalizer,
    keras.layers.Dense(64,activation=""relu""),
    keras.layers.Dense(64,activation=""relu""),
    keras.layers.Dense(1)
])
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),loss=""mae"",metrics=[""mae"",""mse""])
history = model.fit(X.values, y.values, batch_size=batch_size, epochs=epochs, callbacks=[lr],validation_split=0.1,shuffle=True)",DL,Tensorflow,mustafakeser4/tps-mar22-deterministicprocess-cal-fourier,https://www.kaggle.com/mustafakeser4/tps-mar22-deterministicprocess-cal-fourier
tabular-playground-series-mar-2022,regression,continuous-regression,"The task is to predict traffic congestion levels for roadways based on historical data, using time, space, and directional features.",Tabular,MAE – Mean Absolute Error,"The dataset consists of traffic congestion measurements across 65 roadways in a major U.S. metropolitan area, recorded from April to September 1991. It includes a training set with features such as unique identifiers, time intervals, coordinates, direction of travel, and normalized congestion levels ranging from 0 to 100. The test set requires predictions for congestion on a specific day, September 30, 1991, based on the same features.",['congestion'],"[""map direction to integer values"", ""convert time to datetime format"", ""extract month, day of week, and hour from time"", ""drop original time column""]","sequence_inputs = []
sequence_vectors = []
dense_inputs = []
dense_vectors = []
for column in sequence_categorical_columns:
    sequence_input = keras.Input(shape=(sequence_length, 1), name=f""{column}_sequnce_input"")
    lookup = lookupLayersMap[column]
    vocab_size = len(lookup.get_vocabulary())
    embed_dimension = max(math.ceil(np.sqrt(vocab_size)), 2)
    sequence_vector = lookup(sequence_input)
    sequence_vector = keras.layers.Embedding(vocab_size, embed_dimension, input_length=sequence_length)(sequence_vector)
    sequence_vector = keras.layers.Reshape((-1, embed_dimension))(sequence_vector)
    sequence_vectors.append(sequence_vector)
    sequence_inputs.append(sequence_input)
target_sequence_input = keras.Input(shape=(sequence_length, 1))
sequence_inputs.append(target_sequence_input)
sequence_vectors.append(target_sequence_input)
sequence_vector = keras.layers.Concatenate(axis=-1)(sequence_vectors)
sequence_vector = keras.layers.LSTM(128, return_sequences=True)(sequence_vector)
sequence_vector = keras.layers.LSTM(64, return_sequences=False)(sequence_vector)
sequence_vector = keras.layers.Dense(128, activation=""relu"")(sequence_vector)
sequence_vector = keras.layers.Dense(128, activation=""relu"")(sequence_vector)
sequence_vector = keras.layers.Dense(128, activation=""relu"")(sequence_vector)
sequence_vector = keras.layers.Dense(128, activation=""relu"")(sequence_vector)
for column in categorical_columns:
    dense_input = keras.Input(shape=(1, ), name=f""{column}_dense_input"")
    lookup = lookupLayersMap[column]
    vocab_size = len(lookup.get_vocabulary())
    embed_dimension = max(math.ceil(np.sqrt(vocab_size)), 2)
    dense_vector = lookup(dense_input)
    dense_vector = keras.layers.Embedding(vocab_size, embed_dimension, input_length=1)(dense_vector)
    dense_vector = keras.layers.Reshape((-1,))(dense_vector)
    dense_vectors.append(dense_vector)
    dense_inputs.append(dense_input)
dense_vector = keras.layers.Concatenate(axis=-1)(dense_vectors)
dense_vector = keras.layers.Dense(128, activation=""relu"")(dense_vector)
dense_vector = keras.layers.Dense(128, activation=""relu"")(dense_vector)
dense_vector = keras.layers.Dense(128, activation=""relu"")(dense_vector)
dense_vector = keras.layers.Dense(128, activation=""relu"")(dense_vector)
vector = keras.layers.Concatenate(axis=-1)([sequence_vector, dense_vector])
vector = keras.layers.Dense(32, activation=""relu"")(vector)
output = keras.layers.Dense(1)(vector)
model = keras.Model(inputs=sequence_inputs + dense_inputs, outputs=output)
model.compile(loss=""huber_loss"", optimizer=""adam"", metrics=[""mae""])
model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[es, cp])",DL,Tensorflow,lonnieqin/tps-22-03-with-lstm,https://www.kaggle.com/lonnieqin/tps-22-03-with-lstm
tabular-playground-series-mar-2022,regression,continuous-regression,"The task is to predict traffic congestion levels for roadways based on time, space, and directional features.",Tabular,MAE – Mean Absolute Error,"The dataset consists of traffic congestion measurements across 65 roadways in a major U.S. metropolitan area, recorded from April to September 1991. It includes a training set with features such as time, coordinates, direction of travel, and congestion levels, as well as a test set for making predictions. The congestion target is normalized between 0 and 100.",['congestion'],"[""add datetime features"", ""label encode categorical variables"", ""merge median, min, and max congestion values"", ""drop unnecessary columns"", ""apply sine and cosine transformations for cyclical features""]","tft = TemporalFusionTransformer.from_dataset(training,learning_rate=LR,hidden_size=HIDDEN_SIZE,attention_head_size=ATTENTION_HEAD_SIZE,dropout=DROPOUT,hidden_continuous_size=HIDDEN_CONTINUOUS_SIZE,output_size=OUTPUT_SIZE,loss=QuantileLoss(),log_interval=10)
trainer.fit(tft,train_dataloader=train_dataloader,val_dataloaders=val_dataloader)",DL,Pytorch,abdulravoofshaik/private-lb-top-2-google-s-tft,https://www.kaggle.com/abdulravoofshaik/private-lb-top-2-google-s-tft
tabular-playground-series-mar-2022,regression,continuous-regression,"The task is to predict traffic congestion levels for roadways based on time, location, and direction of travel.",Tabular,MAE – Mean Absolute Error,"The dataset consists of traffic congestion measurements across 65 roadways in a major U.S. metropolitan area, recorded from April to September 1991. It includes a training set with features such as time, coordinates, direction of travel, and normalized congestion levels ranging from 0 to 100. The test set requires predictions for congestion on a specific day, September 30, 1991, based on similar features.",['congestion'],"[""reduce memory usage by optimizing data types"", ""drop holidays from the training data"", ""extract features such as weekday, hour, and minute from the time column"", ""one-hot encode categorical features"", ""split the data into training and validation sets"", ""convert data to PyTorch tensors and send to CUDA if available""]","model = Net().double().to(device)
loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
train_loop(train_dataloader, model, loss_fn, optimizer)
",DL,Pytorch,matthewszhang/tps-march-pytorch-with-gpu,https://www.kaggle.com/matthewszhang/tps-march-pytorch-with-gpu
tabular-playground-series-mar-2022,regression,continuous-regression,"The task is to predict traffic congestion levels for roadways based on time, space, and directional features.",Tabular,MAE – Mean Absolute Error,"The dataset consists of traffic congestion measurements across 65 roadways in a major U.S. metropolitan area, recorded from April to September 1991. It includes a training set with features such as time, east-west and north-south coordinates, direction of travel, and normalized congestion levels ranging from 0 to 100. The test set requires predictions for congestion on a specific day, September 30, 1991.",['congestion'],"[""drop the row_id column"", ""normalize the congestion values to the range 0 to 1"", ""create date features from the time column"", ""split the dataset into training and validation sets"", ""categorize categorical variables"", ""fill missing values"", ""normalize continuous variables""]","learn = tabular_learner(dls, y_range=model_config['y_range'], n_out=1, loss_func=F.mse_loss, metrics=mae, layers=model_config['layers'], config=learner_config)
learn.fit_one_cycle(model_config['epochs'], lr_max=model_config['lr_max'], wd=model_config['weight_decay'])",DL,Pytorch,masatomurakawamm/tps-mar2022-fastai-tabular-deep-learning-optuna,https://www.kaggle.com/masatomurakawamm/tps-mar2022-fastai-tabular-deep-learning-optuna
tabular-playground-series-mar-2022,regression,continuous-regression,"The task is to predict traffic congestion levels for roadways based on time, space, and directional features.",Tabular,MAE – Mean Absolute Error,"The dataset consists of traffic congestion measurements across 65 roadways in a major U.S. metropolitan area, recorded from April to September 1991. It includes a training set with features such as time, coordinates, direction of travel, and congestion levels, as well as a test set for making predictions. The congestion target is normalized between 0 and 100, and the dataset is derived from the Chicago Traffic Tracker historical congestion estimates.",['congestion'],"[""remove outliers for specific holidays"", ""convert time to datetime format"", ""extract hour, month, day of week, and other time-related features"", ""encode categorical variables using label encoding"", ""fill missing values with median""]","clf = TabNetRegressor(cat_idxs=cat_idxs)
clf.fit(X_tr.values, y_tr,
    eval_set=[(X_tr.values, y_tr),(X_val.values, y_val)],
    eval_name=['train','val'],
    eval_metric=['mae'],
    max_epochs=10,
    patience=3,)
clf.fit(X_train.values, y_train_new, eval_set = [(X_train.values, y_train_new)], max_epochs=20,
    patience=3, eval_metric = ['mae'])",DL,Pytorch,ifashion/tps-mar2022-tabnet,https://www.kaggle.com/ifashion/tps-mar2022-tabnet
tabular-playground-series-mar-2022,regression,continuous-regression,The task is to predict traffic congestion levels for roadways based on various features over a specified time period.,Tabular,MAE – Mean Absolute Error,"The dataset consists of traffic congestion measurements across 65 roadways in a major U.S. metropolitan area, recorded from April to September 1991. It includes a training set with features such as time, coordinates, direction of travel, and the target congestion levels normalized between 0 and 100. The test set requires predictions for congestion on a specific day, September 30, 1991.",['congestion'],"[""set index to row_id"", ""convert time to datetime"", ""create unique identifiers for x and y coordinates and direction"", ""extract weekday, hour, and minute from time"", ""one-hot encode categorical features""]","net = nn.Sequential(nn.Linear(193, 500), nn.BatchNorm1d(500), nn.ReLU(),
                    nn.Linear(500, 300), nn.BatchNorm1d(300), nn.ReLU(),
                    nn.Linear(300, 200), nn.BatchNorm1d(200), nn.ReLU(),
                    nn.Linear(200, 100), nn.BatchNorm1d(100), nn.ReLU(),
                    nn.Linear(100, 50), nn.BatchNorm1d(50), nn.ReLU(), 
                    nn.Linear(50, 1)
)

criterion = nn.L1Loss()
optimizer = torch.optim.SGD(params=net.parameters(), lr=1e-3, momentum=0.9)

for epoch in range(500):
    for x, target in train_dataloader:
        predict = net(x)
        loss = criterion(predict, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()",DL,Pytorch,zhangcheche/tps-2022-03-pytorch-score-4-923,https://www.kaggle.com/zhangcheche/tps-2022-03-pytorch-score-4-923
tabular-playground-series-jan-2022,regression,continuous-regression,The task is to predict the number of items sold for three products across two stores in three different countries over a year.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset consists of sales data for each date-country-store-item combination, including a training set with historical sales data and a test set for predictions. It captures various effects seen in real-world data, such as seasonality and holiday impacts. The training set is used to develop the model, while the test set is used for evaluation.",['num_sold'],"[""convert date column to datetime object"", ""split training data into training and validation sets"", ""create new features from the date column such as month, year, weekday, and weekend"", ""drop unnecessary columns""]","all_features = tf.keras.layers.concatenate(encoded_features)
x = tf.keras.layers.Dense(32, activation=""relu"")(all_features)
x = tf.keras.layers.Dropout(0.5)(x)
output = tf.keras.layers.Dense(1)(x)

model = tf.keras.Model(all_inputs, output)

model.compile(optimizer='rmsprop',loss=smape)

model.fit(train_ds, epochs=10, validation_data=val_ds)",DL,Tensorflow,usharengaraju/tensorflow-tf-data-keraspreprocessinglayers-w-b,https://www.kaggle.com/usharengaraju/tensorflow-tf-data-keraspreprocessinglayers-w-b
tabular-playground-series-jan-2022,regression,continuous-regression,"The task is to predict the number of items sold for three products across two stores in three countries over a year, based on historical sales data.",Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for three items sold at two stores located in three different countries. It includes a training set with historical sales data for each date-country-store-item combination and a test set for which predictions need to be made. The dataset captures various real-world effects such as seasonality and holiday impacts, allowing for diverse modeling approaches.",['num_sold'],"[""convert date to datetime format"", ""drop rows with date 29th February"", ""create features for public holidays"", ""create Fourier features for seasonality"", ""create interaction features"", ""one-hot encode categorical variables"", ""drop original date column""]","model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=100, batch_size=32)",DL,Tensorflow,samuelcortinhas/tps-jan-22-quick-eda-hybrid-model,https://www.kaggle.com/samuelcortinhas/tps-jan-22-quick-eda-hybrid-model
tabular-playground-series-jan-2022,regression,continuous-regression,"The task is to predict the number of items sold for three products across two stores in three countries over a year, based on historical sales data.",Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for three items sold at two stores located in three different countries over a year. It includes a training set with historical sales data and a test set for predictions. The dataset captures various real-world effects such as seasonality and holiday impacts, allowing for diverse modeling approaches.",['num_sold'],"[""convert date strings to datetime objects"", ""apply log transformation to target variable"", ""one-hot encode categorical features"", ""scale features using MinMaxScaler and StandardScaler""]","other = Input(shape=(len(features), ))
output = Dense(1, use_bias=True,bias_initializer=tf.keras.initializers.Constant(value=5.74))(other)
model = Model(other, output)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')
history = model.fit([X_tr_f[features]], np.log(y_tr), validation_data=validation_data, epochs=epochs, verbose=VERBOSE, batch_size=512, shuffle=True, callbacks=callbacks)",DL,Tensorflow,ambrosm/tpsjan22-05-keras-quickstart,https://www.kaggle.com/ambrosm/tpsjan22-05-keras-quickstart
tabular-playground-series-jan-2022,regression,continuous-regression,The task is to predict the number of items sold for three products across two stores in three countries over a year.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for three items sold at two stores located in three different countries over a period of three years. It includes features such as date, country, store, and product, allowing for the analysis of sales trends influenced by factors like seasonality and holidays. The training set includes historical sales data, while the test set requires predictions for future sales.",['num_sold'],"[""convert date to datetime"", ""one-hot encode categorical variables"", ""add holiday features"", ""extract date features (day, month, year, etc.)"", ""drop unnecessary columns""]","model = keras.Sequential()
model.add(layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=100, batch_size=32)",DL,Tensorflow,samuelcortinhas/tps-jan-22-eda-modelling,https://www.kaggle.com/samuelcortinhas/tps-jan-22-eda-modelling
tabular-playground-series-jan-2022,regression,continuous-regression,"The task is to predict the number of items sold for three products across two stores in three countries over a year, based on historical sales data and various features.",Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for three items at two stores located in three different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting future sales. The dataset captures real-world effects such as seasonality and holidays, allowing for various modeling approaches.",['num_sold'],"[""convert date column to datetime"", ""extract year, month, day, and day of the week from date"", ""convert month, day, and day of the week to categorical types"", ""scale GDP values using MinMaxScaler"", ""create binary feature for holidays"", ""drop redundant columns"", ""one-hot encode categorical columns"", ""split data into training and validation sets""]","inputs = layers.Input(shape=(x_train.shape[-1],))
x = tf.keras.layers.Dense(1024, activation=tf.nn.relu6)(inputs)
preprocess_features = tf.keras.layers.Dense(200, activation=tf.nn.relu6)(x)
m1_z1 = tf.keras.layers.Dense(256, activation=tf.nn.relu6)(preprocess_features)
m1_z1 = tf.keras.layers.Dropout(0.2)(m1_z1)
m1_z1 = tf.keras.layers.Dense(128, activation=tf.nn.relu6)(m1_z1)
m1_pred = tf.keras.layers.Dense(1)(m1_z1)
m2_z1 = tf.keras.layers.Dense(128, activation=tf.nn.swish)(preprocess_features)
m2_z1 = tf.keras.layers.Dropout(0.2)(m2_z1)
m2_z1 = tf.keras.layers.Dense(64, activation=tf.nn.swish)(m2_z1)
m2_pred = tf.keras.layers.Dense(1)(m2_z1)
ensemble_nn_only = tf.keras.models.Model(inputs, mean_nn_only)
ensemble_nn_only.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=smape, metrics=[smape])
history = ensemble_nn_only.fit(train_dataset, epochs=100, validation_data=valid_dataset, callbacks=[cb_es, cb_lr], verbose=2)",DL,Tensorflow,kavehshahhosseini/mix-tf-neural-networks-tf-decision-forests,https://www.kaggle.com/kavehshahhosseini/mix-tf-neural-networks-tf-decision-forests
tabular-playground-series-jan-2022,regression,continuous-regression,The task is to predict the number of items sold for three products across two stores in three countries over a year.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for three items sold at two stores located in Sweden, Finland, and Norway. It includes a training set with historical sales data and a test set for predictions. The training set spans multiple years, capturing seasonal effects and holiday impacts on sales. The dataset is fictional but reflects real-world sales patterns, allowing for various modeling approaches.",['num_sold'],"[""convert date column to datetime format"", ""merge holiday events with sales data"", ""one-hot encode holiday events""]","model = NeuralProphet(growth='linear', n_changepoints=10, changepoints_range=0.4, trend_reg=1, yearly_seasonality=True, weekly_seasonality=True, seasonality_mode='additive')
model.fit(train, freq='D')
train_predictions = model.predict(train)['yhat1']
val_predictions = model.predict(val)['yhat1']",DL,Pytorch,gunesevitan/tabular-playground-series-jan-2022-prophet,https://www.kaggle.com/gunesevitan/tabular-playground-series-jan-2022-prophet
tabular-playground-series-jan-2022,regression,continuous-regression,The task is to predict the number of items sold for three products across two stores in three different countries over a year.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for three items at two stores located in three different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures real-world effects such as seasonality and holiday impacts, allowing for various modeling approaches.",['num_sold'],"[""drop the 'row_id' column"", ""convert 'date' column to datetime format"", ""scale 'num_sold' values using MinMaxScaler""]","lstm = LSTM(num_classes, input_size, hidden_size, num_layers)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)",DL,Pytorch,toomuchsauce/tps-jan-part-2-lstm-pytorch,https://www.kaggle.com/toomuchsauce/tps-jan-part-2-lstm-pytorch
tabular-playground-series-jan-2022,regression,continuous-regression,"The task is to predict the number of items sold for three products across two stores in three countries over a year, based on historical sales data.",Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for three items at two stores located in three different countries, capturing various effects such as seasonality and holidays. It includes a training set with sales data for each date-country-store-item combination and a test set for predictions. The training set is complete, and the dataset is small enough to allow for various modeling approaches.",['num_sold'],"[""log-transform the target variable"", ""add date-related features"", ""merge with holiday data"", ""categorize categorical variables"", ""fill missing values"", ""normalize continuous variables""]","learn = tabular_learner(dls, n_out = dls.c, config = my_config, layers = [64,256,1024,1024,256,64,16], metrics = [smape, exp_rmspe])
learn.fit_one_cycle(300, 3e-3, cbs=SaveModelCallback(fname='kaggle_tps_jan_2022', with_opt=True))",DL,Pytorch,casati8/kaggle-tps-jan2022-fastai,https://www.kaggle.com/casati8/kaggle-tps-jan2022-fastai
tabular-playground-series-jan-2022,regression,continuous-regression,"The task is to predict the number of items sold for three products across two stores in three different countries over a year, based on historical sales data.",Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for three items at two stores located in three different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting future sales. The dataset captures various real-world effects such as seasonality and holiday impacts, allowing for diverse modeling approaches.",['num_sold'],"[""transform date features using trigonometric functions"", ""one-hot encode categorical features"", ""apply Fourier transforms to capture seasonal patterns"", ""encode holiday information""]","model.fit(train_data, train_targets)",DL,Pytorch,jaredfeng/tps-jan22-inprog-v5,https://www.kaggle.com/jaredfeng/tps-jan22-inprog-v5
tabular-playground-series-jan-2022,regression,continuous-regression,The task is to predict the number of items sold for three products across two stores in three different countries over a year.,Tabular,SMAPE – Symmetric Mean Absolute Percentage Error,"The dataset contains sales data for three items at two stores located in three different countries. It includes a training set with sales data for each date-country-store-item combination and a test set for predicting item sales. The dataset captures real-world effects such as seasonality and holiday impacts, allowing for various modeling approaches.",['num_sold'],"[""convert date to datetime format"", ""extract year, quarter, month, day, day of week, day of month, day of year, week of year, and weekend indicator from date"", ""drop the original date column"", ""one-hot encode categorical variables"", ""scale numerical features using standard scaling"", ""convert data to PyTorch tensors""]","model = NeuralNetwork().to(device)
model.apply(init_weights)

loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9,  weight_decay=0.00001)

model.fit(x_train, y_train)",DL,Pytorch,zhangcheche/a-simple-simple-simple-neural-network-pytorch,https://www.kaggle.com/zhangcheche/a-simple-simple-simple-neural-network-pytorch
tabular-playground-series-dec-2021,classification,multiclass-classification,The task is to predict the Cover_Type class for each instance in the test set based on various feature columns.,Tabular,multi-class classification accuracy,"The dataset consists of synthetic data generated by a GAN based on the original Forest Cover Type Prediction dataset. It includes a training set with a target column named Cover_Type and a test set for which predictions need to be made. The training data contains various feature columns, and the dataset is relatively large, with a total size of 693.17 MB in CSV format.",['Cover_Type'],"[""drop the Id column from both train and test datasets"", ""remove unwanted rows and columns based on target distribution"", ""scale features using StandardScaler"", ""create new features such as mean, std, min, and max from existing features""]","model = tf.keras.Sequential([tf.keras.layers.Dense(2048, activation = 'swish', input_shape = [X.shape[1]]),tf.keras.layers.Dense(1024, activation ='swish'),tf.keras.layers.Dense(512, activation ='swish'),tf.keras.layers.Dense(6, activation='softmax')])
model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate = LEARNING_RATE),loss='categorical_crossentropy',metrics=['acc'])
history = nn_model.fit(X_train , y_train,validation_data = (X_valid , y_valid),batch_size = BATCH_SIZE,epochs = EPOCHS,callbacks = [early_stopping , plateau])",DL,Tensorflow,odins0n/tps-dec-eda-modelling,https://www.kaggle.com/odins0n/tps-dec-eda-modelling
tabular-playground-series-dec-2021,classification,multiclass-classification,The task is to predict the Cover_Type class for each instance in the test set based on various feature columns.,Tabular,multi-class classification accuracy,"The dataset consists of synthetic data generated by a GAN based on the Forest Cover Type Prediction dataset. It includes a training set with a target column named Cover_Type and a test set for which predictions need to be made. The dataset is larger than the original and may have different relationships to the target. It contains various features related to soil types and wilderness areas, among others.",['Cover_Type'],"[""drop the Id column from both train and test datasets"", ""drop columns Soil_Type7 and Soil_Type15 due to constant values"", ""drop rows with Cover_Type=5 due to insufficient data"", ""rename long column names for simplicity"", ""encode Cover_Type labels to range from 0 to 5"", ""adjust Aspect values to lie within the range of 0 to 359"", ""create Manhattan distance and Euclidean distance features to Hydrology"", ""create new features for soil type and wilderness area counts"", ""replace invalid Hillshade values with appropriate limits"", ""scale features using RobustScaler"", ""reduce memory usage of the dataframes""]","model = Sequential([
    Dense(units=300, kernel_initializer=""lecun_normal"", activation=""selu"", input_shape=INPUT_SHAPE),
    BatchNormalization(),
    Dense(units=200, kernel_initializer=""lecun_normal"", activation=""selu""),
    BatchNormalization(),
    Dense(units=100, kernel_initializer=""lecun_normal"", activation=""selu""),
    BatchNormalization(),
    Dense(units=50, kernel_initializer=""lecun_normal"", activation=""selu""),
    BatchNormalization(),
    Dense(units=NUM_CLASSES, activation=""softmax"")
])

model.compile(
    optimizer=""adam"",
    loss=""sparse_categorical_crossentropy"",
    metrics=[""accuracy""]
)

model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, verbose=False)",DL,Tensorflow,gulshanmishra/tps-dec-21-tensorflow-nn-feature-engineering,https://www.kaggle.com/gulshanmishra/tps-dec-21-tensorflow-nn-feature-engineering
tabular-playground-series-dec-2021,classification,multiclass-classification,The task is to predict the Cover_Type class for each sample in the test set based on various feature columns.,Tabular,multi-class classification accuracy,"The dataset consists of synthetic data generated by a GAN, based on the original Forest Cover Type Prediction dataset. It includes a training file with a target column named Cover_Type and a test file for which predictions need to be made. The dataset is large, with a size of 693.17 MB, and is provided in CSV format.",['Cover_Type'],"[""impute missing values with 0 for numeric and 'None' for categorical"", ""reduce memory usage by downcasting numeric types"", ""feature engineering to create new distance and elevation features"", ""apply scaling using RobustScaler for numeric features""]","model = keras.Sequential([
    layers.Dense(units=300, input_shape=[INPUT_SHAPE], kernel_initializer=KERNEL_INIT, activation=ACTIVATION),
    layers.BatchNormalization(),
    layers.Dense(units=200, kernel_initializer=KERNEL_INIT, activation=ACTIVATION),
    layers.BatchNormalization(),
    layers.Dense(units=100, kernel_initializer=KERNEL_INIT, activation=ACTIVATION),
    layers.BatchNormalization(),
    layers.Dense(units=50, kernel_initializer=KERNEL_INIT, activation=ACTIVATION),
    layers.BatchNormalization(),
    layers.Dense(units=OUTPUT_SHAPE, activation='softmax', name='output')
])
model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[METRICS])
history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[early_stopping, plateau, modelCheckpoint], shuffle=True, verbose=VERBOSE)",DL,Tensorflow,teckmengwong/dcnv2-softmaxclassification,https://www.kaggle.com/teckmengwong/dcnv2-softmaxclassification
tabular-playground-series-dec-2021,classification,multiclass-classification,The task is to predict the Cover_Type class for each instance in the test set based on various feature columns.,Tabular,multi-class classification accuracy,"The dataset consists of training and test data for predicting a categorical target, Cover_Type, based on features. The training data includes the target column, while the test data does not. The dataset is synthetically generated by a GAN trained on original forest cover type data, making it larger and potentially different in relationships to the target.",['Cover_Type'],"[""reduce memory usage"", ""drop columns with constant values"", ""label encode target variable"", ""standardize feature values""]","model = keras.Sequential([
    layers.Dense(600,  activation='selu', kernel_initializer='lecun_normal', input_shape=[X.shape[1]]),
    layers.BatchNormalization(),
    layers.Dense(300,  activation='selu', kernel_initializer='lecun_normal'),   
    layers.BatchNormalization(),
    layers.Dense(150,  activation='selu', kernel_initializer='lecun_normal'),    
    layers.BatchNormalization(),
    layers.Dense(75,  activation='selu', kernel_initializer='lecun_normal'),    
    layers.BatchNormalization(),    
    layers.Dense(7, activation='softmax')])

model.compile(optimizer=""adam"", loss='sparse_categorical_crossentropy', metrics=['acc'])

history = model.fit(
        train_X, train_y,
        validation_data=(val_X, val_y), 
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        callbacks=[reduce_lr, early_stopping],        
        verbose=0)",DL,Tensorflow,bennyfung/keras-mlp-score-95-6,https://www.kaggle.com/bennyfung/keras-mlp-score-95-6
tabular-playground-series-dec-2021,classification,multiclass-classification,The task is to predict the Cover_Type class for each instance in the dataset based on various feature columns.,Tabular,multi-class classification accuracy,"The dataset consists of synthetic data generated by a GAN, based on the original Forest Cover Type Prediction dataset. It includes a training set with a target column named Cover_Type and a test set for which predictions need to be made. The training data is larger and may have different relationships to the target compared to the original dataset. The dataset is provided in CSV format and is approximately 693.17 MB in size.",['Cover_Type'],"[""drop irrelevant columns"", ""clip feature values to valid ranges"", ""reduce memory size by changing data types"", ""encode target labels""]","model = Sequential()
model.add(InputLayer(input_shape=(X.shape[-1])));
model.add(Dense(128, kernel_initializer='lecun_normal', activation='selu'));
model.add(Dense(64, kernel_initializer='lecun_normal', activation='selu'));
model.add(Dense(64, kernel_initializer='lecun_normal', activation='selu'));
model.add(Dense(len(le.classes_), activation='softmax'));
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']);
history = model.fit(X_tr, y_tr, validation_data=(X_va, y_va), epochs=EPOCHS, verbose=VERBOSE, batch_size=BATCH_SIZE, validation_batch_size=len(X_va), shuffle=True, callbacks=[lr, es]);",DL,Tensorflow,ambrosm/tpsdec21-01-keras-quickstart,https://www.kaggle.com/ambrosm/tpsdec21-01-keras-quickstart
tabular-playground-series-dec-2021,classification,multiclass-classification,The task is to predict the forest cover type based on various cartographic variables from a synthetic dataset.,Tabular,multi-class classification accuracy,"The dataset consists of training and test data for predicting forest cover types based on cartographic variables. The training data includes a target column indicating the cover type, while the test data is used for making predictions. The dataset is generated synthetically and is larger than the original dataset from the Forest Cover Type Prediction competition.",['Cover_Type'],"[""drop unnecessary columns"", ""convert boolean columns to integer"", ""fill missing values with appropriate methods"", ""scale features using standard scaling"", ""apply feature engineering techniques""]","model = Model(in_features=52, num_cls=6)
model.apply(init_weights)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=param['lr'], weight_decay=param['wd'])
scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=param['plateau_factor'], patience=param['plateau_patience'], mode='max', verbose=True)
train_loss, train_acc = train_loop(train_loader, model, criterion, optimizer, epoch, device)
valid_loss, valid_acc = val_loop(val_loader, model, criterion, epoch, device)",DL,Pytorch,sergiosaharovskiy/tps-dec-2021-a-complete-guide-eda-pytorch,https://www.kaggle.com/sergiosaharovskiy/tps-dec-2021-a-complete-guide-eda-pytorch
tabular-playground-series-dec-2021,classification,multiclass-classification,"The task is to predict the forest cover type based on various environmental features, with seven distinct classes to choose from.",Tabular,multi-class classification accuracy,"The dataset consists of 56 variables and 4 million observations related to different forest cover types. It includes features such as elevation, aspect, slope, distances to hydrology and roadways, hillshade indices, and binary indicators for wilderness areas and soil types. The target variable is the cover type, which is an integer class ranging from 1 to 7, representing different types of forest cover.",['Cover_Type'],"[""drop the Id column"", ""remove observations with cover type 5"", ""adjust Aspect values to be within 0-360 degrees"", ""clip Hillshade values to be within 0-255"", ""reduce memory usage by changing data types"", ""split the dataset into training and validation sets"", ""encode the target variable using label encoding""]","clf2 = TabNetClassifier(optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2), scheduler_params={""step_size"":4, ""gamma"":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR, mask_type='entmax')
clf2.fit(x_train,y_train,eval_set=[(x_train, y_train), (x_val, y_val)],eval_name=['train', 'valid'],eval_metric=['accuracy'],max_epochs=160 , patience=25)",DL,Pytorch,sisharaneranjana/semi-supervised-pre-training-with-tabnet,https://www.kaggle.com/sisharaneranjana/semi-supervised-pre-training-with-tabnet
tabular-playground-series-dec-2021,classification,multiclass-classification,The task is to predict the Cover_Type class based on various feature columns in the dataset.,Tabular,multi-class classification accuracy,"The dataset consists of synthetic data generated by a GAN, based on the original Forest Cover Type Prediction data. It includes a training set with a target column named Cover_Type and a test set for which predictions need to be made. The training data is larger and may have different relationships to the target compared to the original dataset. The dataset is provided in CSV format and is approximately 693.17 MB in size.",['Cover_Type'],"[""one-hot encode categorical features"", ""convert boolean values to integers"", ""split data into training and testing sets""]","model = Sequential()
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(train_loader, epochs=10, validation_data=test_loader)",DL,Pytorch,kishalmandal/pytorch-series-part-1-dataset-and-dataloader,https://www.kaggle.com/kishalmandal/pytorch-series-part-1-dataset-and-dataloader
tabular-playground-series-dec-2021,classification,multiclass-classification,The task is to predict the Cover_Type class for each instance in the test set based on various feature columns.,Tabular,multi-class classification accuracy,"The dataset consists of synthetic data generated by a GAN, based on the original Forest Cover Type Prediction dataset. It includes a training file with the target Cover_Type column and a test file for which predictions need to be made. The training data is larger and may have different relationships to the target compared to the original dataset.",['Cover_Type'],"[""subtract 1 from Cover_Type to adjust labels for cross entropy loss"", ""apply stratified k-fold cross-validation for model training""]","self.model = nn.Sequential(
            nn.Linear(CFG.N_FEATURES, CFG.HIDDEN_SIZE),
            nn.ReLU(),
            nn.Linear(CFG.HIDDEN_SIZE, CFG.NUM_CLASSES)
                     )

trainer.fit(model, dm)",DL,Pytorch,maxdiazbattan/tps-2021-pytorch-lightning-dataparallel-2t4,https://www.kaggle.com/maxdiazbattan/tps-2021-pytorch-lightning-dataparallel-2t4
tabular-playground-series-dec-2021,classification,multiclass-classification,The task is to predict the Cover_Type class for each instance in the dataset based on various feature columns.,Tabular,multi-class classification accuracy,"The dataset consists of synthetic data generated by a GAN, based on the original Forest Cover Type Prediction dataset. It includes a training set with a target column named Cover_Type and a test set for which predictions need to be made. The training data is provided in train.csv, while the test data is in test.csv. A sample submission file is also included to demonstrate the required format for predictions.",['Cover_Type'],"[""normalize continuous features using Quantile Transformer""]","model = DenoisingAutoEncoder()
model.to(CONFIG['device']);

optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])
scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['T_max'], eta_min=CONFIG['min_lr'])

run_training(model, optimizer, scheduler, device=CONFIG['device'], num_epochs=CONFIG['epochs'])",DL,Pytorch,debarshichanda/pytorch-dae-starter,https://www.kaggle.com/debarshichanda/pytorch-dae-starter
tabular-playground-series-nov-2021,classification,binary-classification,The task is to predict a binary target variable based on 100 continuous feature columns derived from a synthetic dataset.,Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem. The training data includes 100 feature columns and a target column, while the test data contains the same feature columns without the target. The features are continuous and were generated using a Generative Adversarial Network (GAN) trained on a real-world dataset related to spam email detection. The total size of the dataset is 1.04 GB, and it is provided in CSV format.",['target'],"[""drop the 'id' column from both train and test datasets"", ""fill missing values with the mean of the respective columns"", ""apply MinMax scaling to the feature columns""]","num_input = keras.Input(shape=(test.shape[1],), name='num_data')
out = keras.layers.Concatenate()([num_input])
for n_hidden in hidden_units:
    out = keras.layers.Dense(n_hidden, activation='swish')(out)
out = keras.layers.Dense(1, activation='sigmoid', name='prediction')(out)
model = keras.Model(inputs=[num_input], outputs=out)
model.compile(keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['AUC'])
model.fit([num_data], target, batch_size=2048, epochs=1000, validation_data=([num_data_test], y_test), callbacks=[es, plateau], validation_batch_size=len(y_test), shuffle=True, verbose=1)",DL,Tensorflow,chaudharypriyanshu/understanding-neural-net,https://www.kaggle.com/chaudharypriyanshu/understanding-neural-net
tabular-playground-series-nov-2021,classification,binary-classification,The task is to predict a binary target variable based on 100 continuous feature columns derived from a synthetic dataset.,Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem. The training data includes 100 feature columns and a target column, while the test data contains the same feature columns without the target. The data is synthetically generated by a GAN trained on a real-world dataset used for spam email identification. The total size of the dataset is 1.04 GB, and it is provided in CSV format.",['target'],"[""load data into dataframes"", ""split data into features and target"", ""standardize feature data""]","inp = Input(shape=X.shape[1], name='input')
h = Dense(128, activation='swish')(inp)
h = Dropout(0.25)(h)
h = Dense(64, activation='swish')(h)
h = Dropout(0.25)(h)
h = Dense(32, activation='swish')(h)
h = Dropout(0.25)(h)
h = Dense(1, activation='sigmoid')(h)
model = Model(inputs=inp, outputs=h)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-3), metrics=['AUC'])
history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, callbacks=[early_stopping, reduce_lr])",DL,Tensorflow,mlanhenke/tps-11-nn-baseline-keras,https://www.kaggle.com/mlanhenke/tps-11-nn-baseline-keras
tabular-playground-series-nov-2021,classification,binary-classification,The task is to predict a binary target variable based on 100 continuous feature columns derived from a synthetic dataset.,Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem, with 100 feature columns that are continuous. The data is synthetically generated using a GAN based on a real-world dataset that identifies spam emails. The training data includes a target column, while the test data is used to predict the target probabilities.",['target'],"[""drop the 'id' column from both train and test datasets"", ""standardize the feature columns using StandardScaler"", ""split the training data into training and validation sets""]","clf = ak.StructuredDataClassifier(max_trials=5,seed=1234)
clf.fit(x_train, y_train, epochs=100, validation_split=0.4)
predicted_y = clf.predict(x_test)
model = clf.export_model()",DL,Tensorflow,sisharaneranjana/deep-learning-with-autokeras,https://www.kaggle.com/sisharaneranjana/deep-learning-with-autokeras
tabular-playground-series-nov-2021,classification,binary-classification,The task is to predict a binary target variable based on 100 continuous feature columns derived from a synthetic dataset.,Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem. The training data includes a target column and 100 feature columns, all of which are continuous. The data is synthetically generated using a Generative Adversarial Network (GAN) trained on a real-world dataset that identifies spam emails based on various extracted features. The training file is named train.csv, while the test file is named test.csv, and a sample submission file is provided in sample_submission.csv.",['target'],"[""drop id and target columns from training data"", ""calculate median for skewed distributions"", ""calculate mean, std, median, skew, max, and var for bimodal distributions"", ""apply standard scaling and min-max normalization""]","model = keras.Sequential([
        layers.Dense(108, activation = ACTIVATION, input_shape = [train.shape[1]]),      
        layers.Dense(64, activation =ACTIVATION), 
        layers.Dense(32, activation =ACTIVATION),
        layers.Dense(1, activation='sigmoid'),
    ])

model.compile(
        optimizer= keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['AUC'],
    )

history = model.fit(  X_train, y_train,
                validation_data = (X_valid, y_valid),
                batch_size = BATCH_SIZE, 
                epochs = EPOCHS,
                callbacks = [early_stopping, plateau],
                shuffle = True,
                verbose = 0
              )",DL,Tensorflow,javiervallejos/simple-nn-with-good-results-tps-nov-21,https://www.kaggle.com/javiervallejos/simple-nn-with-good-results-tps-nov-21
tabular-playground-series-nov-2021,classification,binary-classification,The task is to predict a binary target variable based on 100 continuous feature columns derived from a synthetic dataset.,Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem. The training data includes 100 feature columns and a target column, while the test data contains the same feature columns without the target. The features are generated from a GAN trained on a real-world dataset related to spam email detection. The dataset is provided in CSV format and has a total size of 1.04 GB.",['target'],"[""drop unnecessary columns"", ""calculate statistical features for skewed distributions"", ""scale and normalize features using StandardScaler and MinMaxScaler""]","model = keras.Sequential([
        layers.Dense(108, activation = ACTIVATION, input_shape = [train.shape[1]]),      
        layers.Dense(64, activation =ACTIVATION), 
        layers.Dense(32, activation =ACTIVATION),
        layers.Dense(1, activation='sigmoid'),
    ])

model.compile(
        optimizer= keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['AUC'],
    )

history = model.fit(  X_train, y_train,
                validation_data = (X_valid, y_valid),
                batch_size = BATCH_SIZE, 
                epochs = EPOCHS,
                callbacks = [early_stopping, plateau],
                shuffle = True,
                verbose = 0
              )",DL,Tensorflow,adityasharma01/simple-nn-tps-nov-21,https://www.kaggle.com/adityasharma01/simple-nn-tps-nov-21
tabular-playground-series-nov-2021,classification,binary-classification,The task is to predict a binary target variable based on 100 continuous feature columns derived from a synthetic dataset.,Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem. The training data includes 100 feature columns and a target column, while the test data contains the same feature columns without the target. The data is synthetically generated by a GAN trained on a real-world dataset used for spam email identification. The total size of the dataset is 1.04 GB, and it is provided in CSV format.",['target'],"[""reduce memory usage by changing data types to float32 and uint8"", ""shuffle the training data""]","model = LogisticRegression(input_dim,output_dim)
error = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

for iteration in range(iteration_number):
    for (x, y) in generate_batches(X_train, y_train, batch_size):
        inputs = Variable(torch.from_numpy(x)).float()
        labels = Variable(torch.from_numpy(y))
        optimizer.zero_grad() 
        results = model(inputs)
        loss = error(results, labels)
        loss.backward()
        optimizer.step()",DL,Pytorch,lordozvlad/tps-nov-logistic-regression-with-pytorch,https://www.kaggle.com/lordozvlad/tps-nov-logistic-regression-with-pytorch
tabular-playground-series-nov-2021,classification,binary-classification,The task is to predict a binary target variable based on 100 continuous feature columns derived from a synthetic dataset generated to identify spam emails.,Tabular,AUCROC,"The dataset consists of training and test data in CSV format, with the training data containing a target column and 100 feature columns. The features are continuous and were synthetically generated by a GAN trained on a real-world dataset related to spam email identification. The training file is named train.csv, the test file is named test.csv, and there is a sample submission file provided as sample_submission.csv.",['target'],"[""drop the 'id' column"", ""downcast float64 and int64 columns to reduce memory usage"", ""check for and handle missing values""]","model = TabNetClassifier(optimizer_params=dict(lr=0.02))
model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=['auc'], max_epochs=40, patience=10, batch_size=1024, virtual_batch_size=128)",DL,Pytorch,sergiosaharovskiy/tps-nov-2021-a-complete-guide,https://www.kaggle.com/sergiosaharovskiy/tps-nov-2021-a-complete-guide
tabular-playground-series-nov-2021,classification,binary-classification,The task is to predict a binary target variable based on 100 continuous feature columns derived from a synthetic dataset.,Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem. The training data includes a target column and 100 feature columns, all of which are continuous. The data is synthetically generated using a Generative Adversarial Network (GAN) trained on a real-world dataset that identifies spam emails. The training set is provided in train.csv, while the test set is in test.csv, where predictions for the target variable must be made. A sample submission file is also included to demonstrate the required format.",['target'],"[""standardize feature values using StandardScaler""]","net_model = Net(len(FEATURES)).to(device)
net_model.apply(weights_init_uniform_rule)

criterion = nn.BCELoss()
optimizer = torch.optim.Adam(net_model.parameters(), lr=0.001)

for it in range(epochs):
    model.train()
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    optimizer.step()",DL,Pytorch,smsajideen/tps-nov-pytorch-baseline-nn,https://www.kaggle.com/smsajideen/tps-nov-pytorch-baseline-nn
tabular-playground-series-nov-2021,classification,binary-classification,The task is to predict a binary target variable based on 100 continuous feature columns derived from a synthetic dataset.,Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem. The training data includes 100 feature columns and a target column, while the test data contains the same feature columns without the target. The features are generated synthetically using a GAN trained on a real-world dataset related to spam email detection. The total size of the dataset is 1.04 GB, and it is provided in CSV format.",['target'],"[""standardize feature values""]","model = TPSModel(args)
model.compile(optimizer=optimizer, loss=loss_fn)
model.fit(train_loader, epochs=args.epochs)",DL,Pytorch,kishalmandal/pytorch-tps-dec-baseline,https://www.kaggle.com/kishalmandal/pytorch-tps-dec-baseline
tabular-playground-series-nov-2021,classification,binary-classification,The task is to predict a binary target variable based on 100 continuous feature columns derived from a synthetic dataset.,Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem. The training data includes 100 feature columns and a target column, while the test data contains the same feature columns without the target. The features are generated synthetically using a GAN trained on a real-world dataset related to spam email detection. The training file is named train.csv, and the test file is named test.csv, with a sample submission format provided in sample_submission.csv.",['target'],"[""drop the 'id' column from the training and test datasets"", ""standardize the feature columns using StandardScaler""]","model = nn.Sequential(
        nn.Linear(100,1),
        nn.Sigmoid()
    )
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.028)
",DL,Pytorch,mohammadkashifunique/tps-nov-logistic-regression-using-pytorch,https://www.kaggle.com/mohammadkashifunique/tps-nov-logistic-regression-using-pytorch
tabular-playground-series-oct-2021,classification,binary-classification,"The goal is to predict a binary target based on various feature columns, which include scaled continuous and binary features, using a synthetic dataset generated by a GAN.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target variable. The training data includes a target column and various feature columns, while the test data is used to predict the target for each row. The features are a mix of scaled continuous and binary types, and the dataset is synthetically generated based on real-world molecular response data.",['target'],"[""load data from CSV files"", ""convert dataset into TensorFlow dataset""]","train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train, label=""target"")
test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test)
model = tfdf.keras.RandomForestModel()
model.fit(train_ds)",DL,Tensorflow,usharengaraju/tensorflow-decision-forests-w-b,https://www.kaggle.com/usharengaraju/tensorflow-decision-forests-w-b
tabular-playground-series-oct-2021,classification,binary-classification,"The task is to predict a binary target variable based on a set of feature columns, which include scaled continuous and binary features.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target variable. The training data includes a target column and various feature columns, while the test data is used to predict the target for each row. The features are a mix of scaled continuous and binary types, and the data is synthetically generated by a GAN trained on real-world molecular response data. The dataset is provided in CSV format, with a total size of 3.49 GB.",['target'],"[""drop the id column"", ""convert features and target to numpy arrays""]","clf = ak.StructuredDataClassifier(overwrite=True, max_trials=5)
clf.fit(X_train, y_train, epochs=10, validation_split=0.3)
model = clf.export_model()
tmp = tf.keras.Sequential(model)
tmp.add(tf.keras.layers.Activation('tanh'))
tmp.compile(optimizer='adam', loss='mse')
tmp.fit(X_train, y_train, epochs=5, validation_split=0.3)",DL,Tensorflow,jeongbinpark/tps-oct-autokeras,https://www.kaggle.com/jeongbinpark/tps-oct-autokeras
tabular-playground-series-oct-2021,classification,binary-classification,"The task is to predict a binary target variable based on a set of feature columns, which include scaled continuous and binary features.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target variable. The training data includes a target column and various feature columns, which are a mix of scaled continuous features and binary features. The data is synthetically generated by a GAN trained on real-world molecular response data. The training file is named train.csv, the test file is named test.csv, and there is a sample submission file named sample_submission.csv.",['target'],"[""convert continuous features to float32"", ""convert categorical features to uint8"", ""categorize continuous features into bins""]","input_1 = layers.Input(shape=(x1.shape[-1]), name=""continuous"")
x_1 = layers.Embedding(input_dim=bins, output_dim=4)(input_1)
x_1 = layers.TimeDistributed(layers.Dense(64, activation=""relu""))(x_1)
x_1 = layers.TimeDistributed(layers.Dense(64, activation=""relu""))(x_1)
x_1 = layers.Flatten()(x_1)
x_1 = layers.Dense(128, activation=""relu"")(x_1)
x_1 = layers.Dense(128, activation=""relu"")(x_1)

input_2 = layers.Input(shape=x2.shape[-1], name=""categories"")
x_2 = layers.Dense(128, activation=""relu"")(input_2)
x_2 = layers.Dense(128, activation=""relu"")(x_2)

x = layers.Concatenate()([x_1,x_2])
x = layers.Dense(64, activation=""relu"")(x)
x = layers.Dense(128, activation=""relu"")(x)
output = layers.Dense(1, activation=""sigmoid"", name=""output"")(x)

model = tf.keras.Model([input_1,input_2], output)
model.compile(loss=""binary_crossentropy"", optimizer=tf.keras.optimizers.Adam(), metrics=[""AUC""])
history = model.fit((x1,x2), y, epochs=20, validation_split=0.2, batch_size=512, callbacks=[cb_es, cb_lr])",DL,Tensorflow,kavehshahhosseini/tps-oct-2021-multi-input-neural-network,https://www.kaggle.com/kavehshahhosseini/tps-oct-2021-multi-input-neural-network
tabular-playground-series-oct-2021,classification,binary-classification,"The task is to predict a binary target variable based on a set of feature columns, which include scaled continuous and binary features.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target variable. The training data includes a target column and various feature columns, while the test data contains only feature columns. The features are a mix of scaled continuous variables and binary variables, and the data is synthetically generated by a GAN trained on real-world molecular response data. The dataset is provided in CSV format and is approximately 3.49 GB in size.",['target'],"[""drop target and id columns"", ""identify categorical and numerical columns"", ""apply binning to numerical features""]","input_num = Input(shape = (train_num.shape[1]))
X = Embedding (input_dim=301, output_dim=8)(input_num)
X = Flatten()(X)
X = Dropout(0.3)(X)
input_cat = Input(shape = (train_cat.shape[1]))
X = Dense(64, kernel_initializer=tf.keras.initializers.GlorotNormal(), activation='swish')(X)
W = Concatenate()([X,input_cat])
W = Dense(64, kernel_initializer=tf.keras.initializers.GlorotNormal(), activation='swish')(W)
W = Dropout(0.3)(W)
W = Concatenate()([W,input_cat])
W = Dense(64, kernel_initializer=tf.keras.initializers.GlorotNormal(), activation='swish')(W)
W = Dropout(0.2)(W)
W= Dense(16, kernel_initializer=tf.keras.initializers.GlorotNormal(), activation='swish')(W)
output1 = Dense(1, kernel_initializer=tf.keras.initializers.GlorotNormal(), activation='sigmoid', name='output2')(W)
model = Model(inputs = [input_num,input_cat], outputs = output1, name='model')
model.compile(loss='binary_crossentropy', optimizer = OPTIMIZER, metrics=METRICS)
model.fit([xtrain_num,xtrain_cat],ytrain,epochs = EPOCH,batch_size = BATCH_SIZE,validation_data = ([xval_num,xval_cat],yval),verbose = 0,callbacks=[es,plateau])",DL,Tensorflow,pourchot/neural-network-2-inputs-numerical-categorical,https://www.kaggle.com/pourchot/neural-network-2-inputs-numerical-categorical
tabular-playground-series-oct-2021,classification,binary-classification,"The task is to predict a binary target variable based on a set of feature columns, which include scaled continuous and binary features.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target variable. The training data includes a target column and various feature columns, while the test data is used to predict the target for each row. The features are a mix of scaled continuous variables and binary variables, and the data is synthetically generated by a GAN trained on real-world molecular response data.",['target'],"[""convert boolean variables to 0/1"", ""drop rows with high outlier occurrences"", ""create new features based on standard deviation, minimum, and maximum"", ""run denoising autoencoder analysis on the data"", ""keep columns with variance greater than 0.05"", ""standardize the train and test data"", ""divide continuous variables into bins"", ""reduce memory usage of the dataset""]","visible = tf.keras.layers.Input(shape=(train_denoise.shape[1],), name='input_denoise')
encode_layer = tf.keras.layers.Dense(units=64, activation='relu', name='encoding_layer')(visible)
output = tf.keras.layers.Dense(units=train.shape[1], activation='linear', name='output_layer')(encode_layer)
model = tf.keras.models.Model(inputs=visible, outputs=output)
model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.RootMeanSquaredError()])
model.fit(train_denoise, train, validation_split=0.2, epochs=epochs, batch_size=batch_size, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss',patience=3,min_delta=0.0001,restore_best_weights=True), tf.keras.callbacks.ReduceLROnPlateau(factor=0.7,patience=3,min_delta=0.00001)], verbose=1)",DL,Tensorflow,fusioncenter/skip-connection-neural-network-architecture,https://www.kaggle.com/fusioncenter/skip-connection-neural-network-architecture
tabular-playground-series-oct-2021,classification,binary-classification,"The task is to predict a binary target variable based on a set of features, which include scaled continuous and binary features, derived from synthetic data generated by a GAN.",Tabular,AUCROC,"The dataset consists of three files: train.csv, which contains the training data along with the target column; test.csv, which is the test set for which predictions need to be made; and sample_submission.csv, which provides a template for the submission format. The training data has 1,000,000 rows and 287 columns, while the test data has 500,000 rows and 286 columns, with no missing values in either dataset.",['target'],"[""reduce memory usage"", ""normalize features using MinMaxScaler"", ""split data into training and validation sets""]","pretrainer = TabNetPretrainer(n_d=8, n_a=8, n_steps=1, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-3), mask_type='entmax', n_shared=1, n_independent=1)
pretrainer.fit(X_train=X_train, eval_set=[X_valid], max_epochs=100 , patience=10, batch_size=1024, virtual_batch_size=64, num_workers=0, drop_last=False, pretraining_ratio=0.55)
clf = TabNetRegressor(gamma=1.5, lambda_sparse=1e-2, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2,weight_decay=1e-5), scheduler_params={""step_size"":5, ""gamma"":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR, mask_type='entmax')
clf.fit(X_train=X_train, y_train=y_train, eval_set=[(X_valid, y_valid)], eval_name=['valid'], max_epochs=max_epochs, patience=10, batch_size=Bs, virtual_batch_size=vBs, num_workers=0, drop_last=False, from_unsupervised=pretrainer)",DL,Pytorch,mhslearner/tps-oct-eda-tabnet,https://www.kaggle.com/mhslearner/tps-oct-eda-tabnet
tabular-playground-series-oct-2021,classification,binary-classification,"The task is to predict a binary target variable based on a set of feature columns, which include scaled continuous and binary features.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target variable. The training data includes a target column and various feature columns, while the test data is used to predict the target for each row. The features are a mix of scaled continuous and binary types, and the data is synthetically generated by a GAN trained on real-world molecular response data. The dataset is provided in CSV format, with a total size of 3.49 GB.",['target'],"[""fill missing values with column means"", ""scale features using standard scaling"", ""split data into training and testing sets""]","model = TabularSept()
model.to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

model.train()
for e in range(1, EPOCHS+1):
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        y_pred = model(X_batch)
        loss = criterion(y_pred, y_batch.unsqueeze(1))
        loss.backward()
        optimizer.step()",DL,Pytorch,saurabhshahane/pytorch-nn-baseline-for-beginners,https://www.kaggle.com/saurabhshahane/pytorch-nn-baseline-for-beginners
tabular-playground-series-oct-2021,classification,binary-classification,"The task is to predict a binary target variable based on a set of features, which include both continuous and binary features, using a model that learns contextual embeddings.",Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem. The training data includes a target column and various feature columns that are a mix of scaled continuous features and binary features. The data is synthetically generated by a GAN trained on real-world molecular response data. The training set is provided in train.csv, while the test set is in test.csv, where predictions for the target variable must be made. A sample submission file is also included to demonstrate the required format.",['target'],"[""drop the 'id' column from both train and test datasets"", ""scale continuous features using QuantileTransformer"", ""discretize continuous features using KBinsDiscretizer""]","model = TabTransformer(**transformer_cfg).to(device)
model.compile(optimizer=torch.optim.AdamW(model.parameters(), lr=cfg.lr), loss=nn.BCEWithLogitsLoss())
model.fit(train_dl)",DL,Pytorch,cascadinglight/tabtransformer-gauss-rank-baseline-kfold,https://www.kaggle.com/cascadinglight/tabtransformer-gauss-rank-baseline-kfold
tabular-playground-series-oct-2021,classification,binary-classification,"The task is to predict a binary target variable based on various feature columns, which include scaled continuous and binary features, using synthetic data generated from a GAN.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target variable. The training data includes a target column and various feature columns, while the test data is used to predict the target for each row. The features are a mix of scaled continuous and binary types, and the data is synthetically generated based on real-world molecular response data.",['target'],"[""normalize feature values to a range between 0 and 1"", ""select the top 60 features based on statistical tests"", ""split the dataset into training and validation sets"", ""convert data to tensor format""]","model=Logistic_Reg_model(X_train.shape[1])
criterion=torch.nn.BCELoss()
optimizer=torch.optim.SGD(model.parameters(),lr=0.01)
for epoch in range(number_of_epochs):
 y_prediction=model(X_train)
 loss=criterion(y_prediction,y_train)
 loss.backward()
 optimizer.step()
 optimizer.zero_grad()",DL,Pytorch,tracyporter/oct-21-tabular-pytorch-selectkbest,https://www.kaggle.com/tracyporter/oct-21-tabular-pytorch-selectkbest
tabular-playground-series-oct-2021,classification,binary-classification,The task is to predict the probability of a binary target variable based on various feature columns.,Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target variable. The training data includes a target column and various feature columns, which are a mix of scaled continuous and binary features. The data is synthetically generated by a GAN trained on real-world molecular response data. The training file is named train.csv, while the test file is named test.csv, where predictions for the target variable are to be made. A sample submission file is also provided in the correct format.",['target'],"[""drop the id column from the test set""]","clf = TabNetClassifier()
clf.load_model(tabnet_zip)
test_pred = clf.predict_proba(test_arr)
sample_submission['target'] = test_pred[:,1]
sample_submission.to_csv('tabnet_prediction.csv', index=False)",DL,Pytorch,mitsuihayato/tps-oct-tabnet-load,https://www.kaggle.com/mitsuihayato/tps-oct-tabnet-load
tabular-playground-series-sep-2021,classification,binary-classification,The task is to predict the probability of a customer making a claim on an insurance policy based on anonymized features.,Tabular,AUCROC,"The dataset consists of training and test data for predicting insurance claims. The training data includes a target column indicating whether a claim was made, while the test data is used for making predictions. The features are anonymized and may contain missing values. The dataset is provided in CSV format and is approximately 1.37 GB in size.",['claim'],"[""fill missing values with zeros"", ""sample 20% of training data for training"", ""create a hold-out validation set from the training data""]","model = tfdf.keras.GradientBoostedTreesModel(num_trees=1500)
model.fit(train_ds)",DL,Tensorflow,carlmcbrideellis/classification-using-tensorflow-decision-forests,https://www.kaggle.com/carlmcbrideellis/classification-using-tensorflow-decision-forests
tabular-playground-series-sep-2021,classification,binary-classification,The task is to predict the probability of a customer making a claim on an insurance policy based on anonymized features.,Tabular,AUCROC,"The dataset consists of training and test data for predicting insurance claims. The training data includes a target column indicating whether a claim was made, while the test data is used for making predictions. The features are anonymized and may contain missing values. The dataset is provided in CSV format and is approximately 1.37 GB in size.",['claim'],"[""drop the 'id' column from both train and test datasets"", ""interpolate missing values using polynomial interpolation"", ""fill remaining missing values with the median"", ""scale features using RobustScaler""]","X_input = Input(input_shape)
X = Embedding(input_dim=BINS, output_dim=4, embeddings_initializer = ""glorot_uniform"")(X_input)
X = Flatten()(X)
X = Dense(64,  activation='swish')(X)
X = Dropout(0.6)(X)
X = Dense(32,  activation='swish')(X)
X = Dropout(0.4)(X)
X = Dense(8,  activation='swish')(X)
X = Dropout(0.2)(X)
X = Dense(1, activation='sigmoid')(X)
model = Model(inputs = X_input, outputs = X, name='simple_keras')
model.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=[auc])
model.fit(train_dataset, epochs = EPOCH, validation_data=val_dataset, callbacks=[model_checkpoint_callback])",DL,Tensorflow,shivansh002/tame-your-neural-network-once-for-all,https://www.kaggle.com/shivansh002/tame-your-neural-network-once-for-all
tabular-playground-series-sep-2021,classification,binary-classification,The task is to predict the probability of a customer making a claim on an insurance policy based on anonymized features.,Tabular,AUCROC,"The dataset consists of training and test data for predicting insurance claims. The training data includes a target column indicating whether a claim was made, while the test data is used for making predictions. The features are anonymized and may contain missing values. The dataset is provided in CSV format and is approximately 1.37 GB in size.",['claim'],"[""impute missing values using median"", ""scale features using quantile transformation"", ""discretize features into bins""]","model = Sequential([\n    Input(train[features].shape[1:]),\n    Embedding(input_dim=64, output_dim=4),\n    Flatten(),\n    Dense(64,  activation='relu'),\n    Dropout(0.5),\n    Dense(32,  activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid'),\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=[auc]) \n\nmodel.fit(x = np.float32(train[features]), y = np.float32(train.claim),\n          batch_size = 1024, shuffle = True, epochs = 25,callbacks=[callback])",DL,Tensorflow,firefliesqn/simple-keras-cpu-100-data,https://www.kaggle.com/firefliesqn/simple-keras-cpu-100-data
tabular-playground-series-sep-2021,classification,binary-classification,The task is to predict the probability of a customer making a claim on an insurance policy based on anonymized features.,Tabular,AUCROC,"The dataset consists of training and test data for predicting insurance claims. The training data includes a target column indicating whether a claim was made, while the test data is used for making predictions. The features are anonymized and may contain missing values. The dataset is provided in CSV format and is approximately 1.37 GB in size.",['claim'],"[""remove id column from train and test datasets"", ""extract target variable claim from training data"", ""calculate the sum of missing values for each row in both train and test datasets"", ""impute missing values using median strategy"", ""apply quantile transformation to scale features"", ""discretize features into bins using KBinsDiscretizer""]","X_input_bin = Input(input_shape)
X_input = Input(input_shape)
X = Embedding (input_dim=96, output_dim=18)(X_input_bin)
X = Flatten()(X)
X = Dense(30, activation='swish')(X)
X1 = Dense(30, kernel_initializer=tf.keras.initializers.GlorotNormal(), activation='swish')(X_input)
X = Add()([X,X1])
X = Dropout(0.5)(X)
X = Dense(30, kernel_initializer=tf.keras.initializers.GlorotNormal(), activation='swish')(X)
X = Dropout(0.5)(X)
X = Dense(1, kernel_initializer=tf.keras.initializers.GlorotNormal(),activation='sigmoid', name='output2')(X)
model = Model(inputs = [X_input_bin,X_input], outputs = X, name='ClassModel')
classmodel.compile(loss='binary_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=0.00012), metrics=[tf.keras.metrics.AUC(name='aucroc')])
classmodel.fit([xtrain_bin,xtrain],ytrain,batch_size=512,epochs = 15,validation_data=([xval_bin,xval],yval),callbacks=[model_checkpoint_callback])",DL,Tensorflow,lukaszborecki/tps-09-nn,https://www.kaggle.com/lukaszborecki/tps-09-nn
tabular-playground-series-sep-2021,classification,binary-classification,"The task is to predict whether a customer made a claim on an insurance policy, with the output being a probability between 0.0 and 1.0.",Tabular,AUCROC,"The dataset consists of anonymized features related to insurance claims, with a binary target variable indicating whether a claim was made. It includes a training set with the target variable and a test set for predictions. The dataset may contain missing values and is provided in CSV format, totaling 1.37 GB across three files.",['claim'],"[""drop missing values"", ""drop unnecessary columns""]","def build_generator(image_size=28, input_size=100):
    gen_input = keras.Input(shape=(input_size,))
    x = layers.Dense(7 * 7 * 128)(gen_input)
    x = layers.Reshape((7, 7, 128))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2DTranspose(128, kernel_size=[5,5], strides=2, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2DTranspose(64, kernel_size=[5,5], strides=2, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2DTranspose(32, kernel_size=[5,5], strides=1, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2DTranspose(1, kernel_size=[5,5], strides=1, padding='same')(x)
    x = layers.Activation('sigmoid')(x)
    generator = keras.Model(gen_input, x, name='generator')
    return generator
def build_discriminator(data_shape=[28,28,1,]):
    dis_input = keras.Input(data_shape)
    x = layers.LeakyReLU(alpha=0.2)(dis_input)
    x = layers.Conv2D(32, kernel_size=[5,5], strides=2, padding='same')(x)
    x = layers.LeakyReLU(alpha=0.2)(x)
    x = layers.Conv2D(64, kernel_size=[5,5], strides=2, padding='same')(x)
    x = layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Conv2D(128, kernel_size=[5,5], strides=2, padding='same')(x)
    x = layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Conv2D(256, kernel_size=[5,5], strides=1, padding='same')(x)
    x = layers.Flatten()(x)
    x = layers.Dense(1, activation='sigmoid')(x)
    discriminator = keras.Model(dis_input, x, name='discriminator')
    return discriminator
def build_models():
    noise_size = 100
    lr = 2e-4
    decay = 6e-8
    base_discriminator = build_discriminator(data_shape=(28,28,1,))
    discriminator = keras.Model(inputs=base_discriminator.inputs, outputs=base_discriminator.outputs)
    optimizer = keras.optimizers.RMSprop(lr=lr, decay=decay)
    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    generator = build_generator(image_size=28, input_size=noise_size)
    frozen_discriminator = keras.Model(inputs=base_discriminator.inputs, outputs=base_discriminator.outputs)
    frozen_discriminator.trainable = False
    optimizer = keras.optimizers.RMSprop(lr=lr * 0.6, decay=decay * 0.5)
    adversarial = keras.Model(generator.input, frozen_discriminator(generator.output))
    adversarial.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return generator, discriminator, adversarial
train_gan(G, D, A)",DL,Tensorflow,shivansh002/gentle-introduction-to-gan,https://www.kaggle.com/shivansh002/gentle-introduction-to-gan
tabular-playground-series-sep-2021,classification,binary-classification,The task is to predict the probability of a customer making a claim on an insurance policy based on anonymized features.,Tabular,AUCROC,"The dataset consists of training and test data for predicting insurance claims. The training data includes a target column indicating whether a claim was made, while the test data requires predictions for the same. The features are anonymized and may contain missing values. The dataset is provided in CSV format and is approximately 1.37 GB in size.",['claim'],"[""median-impute missing values"", ""add a feature for the sum of missing values"", ""apply quantile transformation to features"", ""discretize features into bins""]","self.emb = nn.Embedding(106,18)
self.fc = nn.Linear(119*18,20)
self.dropout = nn.Dropout(0.2)
self.dropout2 = nn.Dropout(0.5)
self.fc1 = nn.Linear(119,20)
self.fc2 = nn.Linear(30,30)
self.fc3 = nn.Linear(128*14,20)
self.out = nn.Linear(60,1)
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(train_loader, epochs=12)",DL,Pytorch,lukaszborecki/pytorch-fork-of-tps-09-nn,https://www.kaggle.com/lukaszborecki/pytorch-fork-of-tps-09-nn
tabular-playground-series-sep-2021,classification,binary-classification,The task is to predict the probability of a customer making a claim on an insurance policy based on anonymized features.,Tabular,AUCROC,"The dataset consists of training and test data for predicting insurance claims. The training data includes a target column indicating whether a claim was made, while the test data is used for making predictions. The features are anonymized and may contain missing values. The dataset is provided in CSV format and is approximately 1.37 GB in size.",['claim'],"[""drop unnecessary columns"", ""impute missing values with mean"", ""scale features using standard scaling"", ""create stratified folds for cross-validation""]","self.conv1 = nn.Conv1d(1, 8, kernel_size=1)
self.conv2 = nn.Conv1d(8, 16, kernel_size=1)
self.conv3 = nn.Conv1d(16, 32, kernel_size=1)
self.conv4 = nn.Conv1d(32, 64, kernel_size=1)
self.fc1 = nn.Linear(7552, 256)
self.fc2 = nn.Linear(256, 1)
model.compile(optimizer=torch.optim.RMSprop(self.parameters(), lr=self.lr))
trainer.fit(model, train_dl, val_dl)",DL,Pytorch,chamecall/tps-pytorch-lightning,https://www.kaggle.com/chamecall/tps-pytorch-lightning
tabular-playground-series-sep-2021,classification,binary-classification,The task is to predict the probability of a customer making a claim on an insurance policy based on anonymized features.,Tabular,AUCROC,"The dataset consists of training and test data for predicting insurance claims. The training data includes a target column indicating whether a claim was made, while the test data is used for making predictions. The features are anonymized and may contain missing values. The dataset is provided in CSV format and is approximately 1.37 GB in size.",['claim'],"[""drop the id column to prevent data leakage"", ""impute missing values using median"", ""scale features using robust scaling"", ""apply quantile transformation to standardize distribution"", ""bin features using KBinsDiscretizer""]","model1 = TabNetClassifier(optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2), scheduler_params={""step_size"":5,""gamma"":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR, mask_type='entmax')

model1.fit(Xtrain, y_train, eval_set=[(Xtrain, y_train), (Xvalid, y_valid)], eval_name=['train', 'valid'], eval_metric=['auc'], max_epochs=15, patience=10, batch_size=512, virtual_batch_size=256, num_workers=0, weights=1, from_unsupervised=unsupervised_model, drop_last=False)",DL,Pytorch,ninamaamary/tabnet-approach-semi-supervised-tps-09,https://www.kaggle.com/ninamaamary/tabnet-approach-semi-supervised-tps-09
tabular-playground-series-sep-2021,classification,binary-classification,The task is to predict the probability of a customer making a claim on an insurance policy based on anonymized features.,Tabular,AUCROC,"The dataset consists of training and test data for predicting insurance claims. The training data includes a target column indicating whether a claim was made, while the test data is used for making predictions. The features are anonymized and may contain missing values. The dataset is provided in CSV format and has a total size of 1.37 GB.",['claim'],"[""median-impute missing values"", ""apply quantile transformation to features"", ""discretize features into bins""]","class TabModel(nn.Module):
    def __init__(self, act_fn = nn.SiLU(), dropout_num = 1):
        super().__init__()
        self.emb = nn.Embedding(96,18)
        self.fc = nn.Linear(119*18, 30)
        self.dropouts = nn.ModuleList([nn.Dropout(0.4) 
                                       for _ in range(dropout_num)])
        self.fc1 = nn.Linear(119,30)
        self.fc2 = nn.Linear(30*2,30) # concat layers
        self.out = nn.Linear(30,1)
        self.act_fn = act_fn
        
        torch.nn.init.xavier_normal_(self.out.weight)
        torch.nn.init.xavier_normal_(self.emb.weight)
        torch.nn.init.xavier_normal_(self.fc.weight)
        torch.nn.init.xavier_normal_(self.fc1.weight)
        torch.nn.init.xavier_normal_(self.fc2.weight)

    def forward(self, x_bin, x):
        x_bin = self.emb(x_bin)
        x_bin = x_bin.view(-1,119*18)
        x_bin = self.act_fn(self.fc(x_bin))
        
        x = self.act_fn(self.fc1(x))
        x = torch.cat([x_bin,x], -1)
        x = self.act_fn(self.fc2(x))
        
        for i, dropout in enumerate(self.dropouts):
            if i == 0:
                out = dropout(x)
                out = self.out(out)
        
            else:
                temp_out = dropout(x)
                temp_out = self.out(temp_out)
                out += temp_out
                
        out /= len(self.dropouts)

        return torch.sigmoid(out) 

model = TabModel(dropout_num = 1).to(device)
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(train_data, train_labels)",DL,Pytorch,cascadinglight/tps-sep-pytorch-mlp-trainer,https://www.kaggle.com/cascadinglight/tps-sep-pytorch-mlp-trainer
tabular-playground-series-sep-2021,classification,binary-classification,The task is to predict the probability of a customer making a claim on an insurance policy based on anonymized features.,Tabular,AUCROC,"The dataset consists of training and test data for predicting insurance claims. The training data includes a target column indicating whether a claim was made, while the test data is used for making predictions. The features are anonymized and may contain missing values. The dataset is provided in CSV format and is approximately 1.37 GB in size.",['claim'],"[""drop the 'id' column from both train and test datasets"", ""impute missing values using mean strategy"", ""split the training data into training and validation sets""]","clf = TabNetRegressor(  verbose = 10 ,
                       optimizer_fn=torch.optim.Adam,
                       optimizer_params=dict(lr=2e-2),
                       scheduler_params={""step_size"":10, 
                                         ""gamma"":0.9},
                       scheduler_fn=torch.optim.lr_scheduler.StepLR,
                       mask_type='sparsemax'
                     )

clf.fit(
    X_train=X_train, y_train=y_train,
    eval_set=[(X_train, y_train), (X_valid, y_valid)],
    eval_name=['train', 'valid'],
    eval_metric=['rmsle','rmse'],
    max_epochs=max_epochs,
    patience=15,
    batch_size=Bs, virtual_batch_size=128,
    num_workers=0,
    drop_last=False,
    from_unsupervised=unsupervised_model
)",DL,Pytorch,ratorato/tabnet-tabular-playground-series-sep-2021,https://www.kaggle.com/ratorato/tabnet-tabular-playground-series-sep-2021
tabular-playground-series-jul-2021,regression,multi-output-regression,The task is to predict air pollution measurements over time based on weather information and sensor data.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains training data with weather information, sensor data, and values for three target variables: target_carbon_monoxide, target_benzene, and target_nitrogen_oxides. The training file includes these features and targets, while the test file has the same structure but lacks target values, which need to be predicted.","['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","[""normalize temperature and humidity values"", ""scale sensor readings"", ""handle missing values by imputation""]","model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(3, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)",DL,Tensorflow,bextuychiev/7-coolest-packages-top-kagglers-are-using,https://www.kaggle.com/bextuychiev/7-coolest-packages-top-kagglers-are-using
tabular-playground-series-jul-2021,regression,multi-output-regression,The task is to predict air pollution measurements over time based on weather information and sensor data.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and test data for predicting air pollution measurements, including values for carbon monoxide, benzene, and nitrogen oxides. The training data includes weather data and sensor readings, while the test data is in the same format but lacks target values. The dataset is provided in CSV format and is approximately 826.96 kB in size.","['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","[""normalize sensor readings"", ""impute missing weather data"", ""convert date_time to datetime format""]","model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(3, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_logarithmic_error')
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)",DL,Tensorflow,bextuychiev/no-bs-guide-to-hyperparameter-tuning-with-optuna,https://www.kaggle.com/bextuychiev/no-bs-guide-to-hyperparameter-tuning-with-optuna
tabular-playground-series-jul-2021,regression,multi-output-regression,"The task is to predict air pollution measurements for carbon monoxide, benzene, and nitrogen oxides based on weather data and sensor inputs over time.",Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and testing data for air pollution measurements, including weather data (temperature and humidity) and sensor readings from five different sensors. The training data contains the target values for three pollutants: carbon monoxide, benzene, and nitrogen oxides, while the test data is structured similarly but lacks these target values. The dataset is provided in CSV format with a total size of approximately 826.96 kB.","['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","[""convert date_time to datetime format"", ""drop target columns from test set"", ""apply MinMax scaling to features"", ""create cyclical features for month, day, and hour"", ""split data into sequences using sliding window approach""]","model_tcm = Sequential()
model_tcm.add(LSTM(100, activation='tanh', input_shape=(n_steps, n_features)))
model_tcm.add(RepeatVector(n_lookup))
model_tcm.add(LSTM(100, activation='tanh', return_sequences=True))
model_tcm.add(TimeDistributed(Dense(1)))
model_tcm.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.02), loss= rmsle)
history_tcm = model_tcm.fit(Xtrain_seq_tcm, ytrain_seq_tcm, validation_data = (Xtest_seq_tcm, ytest_seq_tcm), epochs=100, verbose = 0, batch_size = 16, callbacks=[es, red_lr])",DL,Tensorflow,remekkinas/lstm-seq2seq-encoder-decoder,https://www.kaggle.com/remekkinas/lstm-seq2seq-encoder-decoder
tabular-playground-series-jul-2021,regression,multi-output-regression,"The task is to predict air pollution measurements for carbon monoxide, benzene, and nitrogen oxides based on weather data and sensor inputs over time.",Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of air pollution measurements over time, including weather data such as temperature and humidity, along with readings from five sensors. The training data includes the target values for carbon monoxide, benzene, and nitrogen oxides, while the test data is in the same format but lacks these target values. The dataset is provided in CSV format and is approximately 826.96 kB in size.","['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","[""convert date_time to datetime format"", ""apply seasonal decomposition"", ""generate lag features"", ""normalize features"", ""reshape data for LSTM input""]","INPUT = ks.layers.Input(batch_input_shape=(1, train.shape[1], train.shape[2]), name=""input"")
L = ks.layers.LSTM(128, kernel_initializer='LecunUniform', activation = ""tanh"", return_sequences = True, stateful=True, name=""L1"")(INPUT)
L = ks.layers.LSTM(32, kernel_initializer='LecunUniform', activation = ""sigmoid"", return_sequences = True, stateful=True, name=""L2"")(L)
L = ks.layers.Dense(3, activation = ks.layers.PReLU(), kernel_initializer='LecunUniform', name=""L_out"")(L)
m = ks.Model(inputs=INPUT, outputs=L)
model.compile(optimizer = optimizer, loss = rmsle, metrics=""mae"")
history = model.fit(x=train, y=y, validation_data=(val, y_val), epochs = eps, batch_size = bs, shuffle = False, callbacks=[lrReducer, ks.callbacks.LearningRateScheduler(lr_schaker, verbose=0)], verbose=1)",DL,Tensorflow,frankmollard/tps-eda-lstm-pseudolabels-interpolate,https://www.kaggle.com/frankmollard/tps-eda-lstm-pseudolabels-interpolate
tabular-playground-series-jul-2021,regression,multi-output-regression,The task is to predict air pollution measurements over time based on weather information and sensor data.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and test data for predicting air pollution levels. The training data includes weather data, sensor readings, and three target variables: carbon monoxide, benzene, and nitrogen oxides. The test data is in the same format as the training data but does not include target values, which need to be predicted. The dataset is provided in CSV format and is approximately 826.96 kB in size.","['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","[""convert date_time to datetime object"", ""extract year from date_time"", ""extract month from date_time"", ""extract hour from date_time"", ""extract day from date_time"", ""normalize feature columns""]","model = tf.keras.Sequential()
model.add(tf.keras.layers.LSTM(64, activation='relu', input_shape=(timesteps, features)))
model.add(tf.keras.layers.Dense(3))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=50, batch_size=32)",DL,Tensorflow,godzill22/tps-07-eda-statistical-analysis,https://www.kaggle.com/godzill22/tps-07-eda-statistical-analysis
tabular-playground-series-jul-2021,regression,multi-output-regression,The task is to predict air pollution measurements over time based on weather information and sensor data.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains training data with weather information, sensor data, and values for three targets: target_carbon_monoxide, target_benzene, and target_nitrogen_oxides. The training file includes these features along with a timestamp, while the test file has the same structure but lacks target values. A sample submission file is also provided to illustrate the required format for predictions.","['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","[""log-transform target variables"", ""convert training data to numpy arrays"", ""convert numpy arrays to PyTorch tensors"", ""create TensorDataset from features and targets"", ""create DataLoader for training data""]","model=MODEL(x_train.size(1),32,64,128,y_train.size(1))
model.compile(optimizer=torch.optim.Adam(model.parameters()), loss=F.mse_loss)
model.fit(train_dl)",DL,Pytorch,balajinagappan/tabular-playground-series-beginners-jul-2021,https://www.kaggle.com/balajinagappan/tabular-playground-series-beginners-jul-2021
tabular-playground-series-jul-2021,regression,multi-output-regression,"The task is to predict air pollution measurements over time based on weather information and sensor inputs, specifically targeting carbon monoxide, benzene, and nitrogen oxides levels.",Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset consists of training and testing data for predicting air pollution measurements. The training data includes weather data, sensor data, and values for three target pollutants: carbon monoxide, benzene, and nitrogen oxides. The test data is in the same format as the training data but does not include target values, which need to be predicted. The dataset is provided in CSV format and is approximately 826.96 kB in size.","['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","[""extract hour from date_time"", ""extract day of the week from date_time"", ""extract day of the year from date_time"", ""standardize feature values"", ""normalize target values""]","self.layers = nn.Sequential(
            nn.Linear(11, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Linear(32, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(256, 3),
            nn.Sigmoid())

optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.75, patience=6, verbose = 1,mode = 'min', cooldown = 0, min_lr = 10e-7)
trainer.fit(model)",DL,Pytorch,kirillshmilovich1995/pytorch-lightning-simple-mlp,https://www.kaggle.com/kirillshmilovich1995/pytorch-lightning-simple-mlp
tabular-playground-series-jun-2021,classification,multiclass-classification,The task is to predict the probability that each product belongs to one of several class labels based on its features.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It involves predicting the category of an eCommerce product given various attributes about the listing. The features are anonymized but relate to real-world features. The dataset includes training data with product IDs, associated features, and class labels, as well as test data for which predictions must be made.",['target'],"[""scale features using MinMaxScaler"", ""one-hot encode target labels""]","conv_inputs = layers.Input(shape = (75))
knn_inputs = layers.Input(shape = (9))
embed = layers.Embedding (input_dim = 353, output_dim = emb_units, embeddings_regularizer='l2')(conv_inputs)
embed = layers.Conv1D(conv_kernel, 1, activation = 'relu')(embed)
embed = layers.Flatten()(embed)
hidden = layers.Dropout(dropout1)(embed)
hidden = tfa.layers.WeightNormalization(layers.Dense(units = l1_nodes, activation = act1, kernel_initializer = ker_init1))(hidden)
output = layers.Dropout(dropout2)(layers.Concatenate()([embed, hidden, knn_inputs]))
output = tfa.layers.WeightNormalization(layers.Dense(units = l2_nodes, activation = act2, kernel_initializer = ker_init2))(output)
output = layers.Dropout(dropout3)(layers.Concatenate()([embed, hidden, output]))
output = tfa.layers.WeightNormalization(layers.Dense(units = l3_nodes, activation = act3, kernel_initializer = ker_init3))(output)
conv_outputs = layers.Dense(units = 9, activation = 'softmax', kernel_initializer = ker_init4)(output)
model = Model([conv_inputs, knn_inputs], conv_outputs)
model.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.RMSprop(learning_rates), metrics = custom_metric)
model.fit([X_train, X_train_knn], y_train, batch_size = 128, epochs = EPOCH, validation_data=([X_test, X_test_knn], y_test), callbacks=[es, plateau], verbose = 0)",DL,Tensorflow,remekkinas/keras-tuner-knn-features-simplex-optimization,https://www.kaggle.com/remekkinas/keras-tuner-knn-features-simplex-optimization
tabular-playground-series-jun-2021,classification,multiclass-classification,The task is to predict the probability distribution over multiple class labels for eCommerce products based on their attributes.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It involves predicting the category of an eCommerce product given various anonymized attributes. The training data consists of one product per row with associated features and class labels, while the test data requires predicting the probabilities for each class. The dataset has increased observations, features, and class labels compared to similar datasets.",['target'],"[""normalize feature values"", ""encode categorical features using one-hot encoding"", ""split data into training and validation sets""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(9, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))",DL,Tensorflow,yus002/blending-tool-comparative-method,https://www.kaggle.com/yus002/blending-tool-comparative-method
tabular-playground-series-jun-2021,classification,multiclass-classification,The task is to predict the probability of each product belonging to multiple class labels based on its features.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It involves predicting the category of an eCommerce product given various attributes about the listing. The features are anonymized but relate to real-world properties. The dataset includes training data with product IDs, associated features, and class labels, as well as test data for predictions and a sample submission file.",['target'],"[""one-hot encode target labels""]","conv_inputs = layers.Input(shape = (75))
embed = layers.Embedding (input_dim = 354, output_dim = 7, embeddings_regularizer='l2')(conv_inputs)
embed = layers.Conv1D(12,1,activation = 'relu')(embed)
embed = layers.Flatten()(embed)
hidden = layers.Dropout(0.3)(embed)
hidden = tfa.layers.WeightNormalization(layers.Dense(units=32, activation ='selu', kernel_initializer = ""lecun_normal""))(hidden)
output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))
output = tfa.layers.WeightNormalization(layers.Dense(units = 32, activation='relu', kernel_initializer = ""lecun_normal""))(output)
output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))
output = tfa.layers.WeightNormalization(layers.Dense(units = 32, activation = 'elu', kernel_initializer = ""lecun_normal""))(output)
conv_outputs = layers.Dense(units = 9, activation ='softmax', kernel_initializer =""lecun_normal"")(output)
model = Model(conv_inputs,conv_outputs)
model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=2e-4), metrics=custom_metric)
model.fit(X_train, y_train, batch_size = 256, epochs = EPOCH, validation_data=(X_test, y_test), callbacks=[es, plateau], verbose = 0)",DL,Tensorflow,pourchot/simple-neural-network,https://www.kaggle.com/pourchot/simple-neural-network
tabular-playground-series-jun-2021,classification,multiclass-classification,The task is to predict the probability of each product belonging to one of several class labels based on various attributes of the product listing.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It involves predicting the category of an eCommerce product given various anonymized attributes. The training data contains one product per row with associated features and class labels, while the test data requires predicting the probabilities for each class. The dataset has increased observations, features, and class labels compared to similar datasets.",['target'],"[""drop unnecessary columns"", ""scale features using min-max scaling"", ""label encode categorical features""]","inputs = layers.Input(shape = (75,))
embed = layers.Embedding(360, 8)(inputs)
embed = layers.Flatten()(embed)
hidden = layers.Dropout(0.2)(embed)
hidden = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='selu', kernel_initializer=""lecun_normal""))(hidden)
output = layers.Dropout(0.2)(layers.Concatenate()([embed, hidden]))
output = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='relu'))(output)
output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden, output]))
output = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='elu'))(output)
output = layers.Dense(9, activation = 'softmax')(output)
model = keras.Model(inputs=inputs, outputs=output, name=""res_nn_model"")
model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=2e-4), metrics=custom_metric)
model.fit(X_train, y_train, batch_size = 256, epochs = EPOCH, validation_data=(X_test, y_test), callbacks=[es, plateau], verbose = 0)",DL,Tensorflow,mehrankazeminia/1-tps-jun-21-histgradient-catboost-nn,https://www.kaggle.com/mehrankazeminia/1-tps-jun-21-histgradient-catboost-nn
tabular-playground-series-jun-2021,classification,multiclass-classification,The task is to predict the probability distribution over multiple product categories based on various product attributes.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It involves predicting the category of an eCommerce product given various anonymized attributes. The training data contains one product per row with associated features and class labels, while the test data requires predicting the probabilities for each class. The dataset has increased observations, features, and class labels compared to similar datasets.",['target'],"[""one-hot encode target variable"", ""label encode target variable""]","conv_inputs = layers.Input(shape = (75))
embed = layers.Embedding (input_dim = 354, output_dim = 7, embeddings_regularizer='l2')(conv_inputs)
embed = layers.Conv1D(12,1,activation = 'relu')(embed)
embed = layers.Flatten()(embed)
hidden = layers.Dropout(0.3)(embed)
hidden = tfa.layers.WeightNormalization(layers.Dense(units=32, activation ='selu', kernel_initializer = ""lecun_normal""))(hidden)
output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))
output = tfa.layers.WeightNormalization(layers.Dense(units = 32, activation='relu', kernel_initializer = ""lecun_normal""))(output)
output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))
output1 = tfa.layers.WeightNormalization(layers.Dense(units = 32, activation = 'relu', kernel_initializer = ""lecun_normal""))(output)
conv_outputs = layers.Dense(units = 9, activation ='softmax', kernel_initializer =""lecun_normal"")(output1)
model_conv = Model(conv_inputs,conv_outputs)
model_conv.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=2e-4), metrics=custom_metric)
model_conv.fit(X_train, y_train, batch_size = 256, epochs = EPOCH, validation_data=(X_test, y_test), callbacks=[es, plateau], verbose = 0)",DL,Tensorflow,pourchot/a-neural-network-improved-by-a-gradient-boosting,https://www.kaggle.com/pourchot/a-neural-network-improved-by-a-gradient-boosting
tabular-playground-series-jun-2021,classification,multiclass-classification,The task is to predict the probability distribution over multiple class labels for eCommerce products based on their attributes.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It involves predicting the category of an eCommerce product given various anonymized attributes. The training data consists of one product per row with associated features and a class label, while the test data requires predicting the probability of each product belonging to each class.",['target'],"[""label encode target variable"", ""convert features to float32""]","net = NeuralNetClassifier(TPSResidual, device = device, lr = 0.001, max_epochs = 50, callbacks = [lr_scheduler, early_stopping])",DL,Pytorch,remekkinas/pytorch-skorch-residual-hyperparameter,https://www.kaggle.com/remekkinas/pytorch-skorch-residual-hyperparameter
tabular-playground-series-jun-2021,classification,multiclass-classification,The task is to predict the probability distribution over multiple product categories based on various product attributes.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It involves predicting the category of an eCommerce product given various anonymized attributes. The training data consists of one product per row with associated features and class labels, while the test data requires predicting the probability of each product belonging to each class.",['target'],"[""label encode target variable"", ""split data into training and validation sets"", ""create data loaders for training and validation""]","model = TPSResidual(NUM_FEATURES, NUM_CLASSES, FEATURE_DICT_SIZE, batch_norm=False, dropout=0.3, emb_output=4, linear_nodes=32, linear_out=16, num_block=3)
model.to(device)
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)
model.train()",DL,Pytorch,remekkinas/pytorch-residual-connection-100-nn-together,https://www.kaggle.com/remekkinas/pytorch-residual-connection-100-nn-together
tabular-playground-series-jun-2021,classification,multiclass-classification,The task is to predict the probability that each product belongs to one of several class labels based on its features.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It involves predicting the category of an eCommerce product given various attributes about the listing. The features are anonymized but relate to real-world features. The dataset includes training data with product IDs, associated features, and class labels, as well as test data for making predictions and a sample submission file.",['target'],"[""label encode categorical features""]","model = TabNetClassifier(**tabnet_params)
model.fit(X_train=X_train, y_train=y_train,
          from_unsupervised=pretrainer if PRETRAIN else None,
          eval_set=[(X_train, y_train), (X_valid, y_valid)],
          eval_name=[""train"", ""valid""],
          eval_metric=[""logloss""],
          batch_size=BS,
          virtual_batch_size=256,
          max_epochs=MAX_EPOCH,
          drop_last=True,
          pin_memory=True,
          patience=10)",DL,Pytorch,optimo/tabnetbaseline,https://www.kaggle.com/optimo/tabnetbaseline
tabular-playground-series-jun-2021,classification,multiclass-classification,The task is to predict the probability of each product belonging to one of several predefined classes based on its features.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It involves predicting the category of an eCommerce product given various attributes about the listing. The features are anonymized but relate to real-world properties. The dataset includes training data with product IDs, associated features, and class labels, as well as test data for predictions.",['target'],"[""label encode target variable"", ""standardize feature values""]","model = classification_model(75, 9, layers_, p=dropout_)
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.RMSprop(model.parameters(), lr=l_rate)
for epoch in range(1, n_epochs + 1):
    for phase in phases:
        if phase == ""train"":
            model.train()
        else:
            model.eval()
        for _, data in enumerate(loaders[phase], 0):
            features, y_true = data[0], data[1]
            features = features.to(device, dtype=torch.float)
            y_true = y_true.to(device, dtype=torch.float)
            optimizer.zero_grad()
            with torch.set_grad_enabled(phase == ""train""):
                y_pred = model(features)
                loss = criterion(y_pred, y_true)
                if phase == ""train"":
                    loss.backward()
                    optimizer.step()",DL,Pytorch,altinsoyemrecan/tps-june-optuna-pytorch-starter,https://www.kaggle.com/altinsoyemrecan/tps-june-optuna-pytorch-starter
tabular-playground-series-jun-2021,classification,multiclass-classification,The task is to predict the probability that an eCommerce product belongs to each of several class labels based on its attributes.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It involves predicting the category of an eCommerce product given various anonymized attributes. The training data consists of one product per row with associated features and class labels, while the test data requires predicting class probabilities for each product. The dataset has increased observations, features, and class labels compared to similar datasets.",['target'],"[""normalize feature values"", ""split dataset into training, validation, and test sets"", ""encode categorical features using label encoding"", ""fill missing values with mean for numerical features""]","unsupervised_model = TabNetPretrainer(optimizer_fn = opt, optimizer_params = opt_params, mask_type = mask)

clf = TabNetClassifier(gamma = 1.5, lambda_sparse = 1e-4, optimizer_fn = opt, optimizer_params = opt_params, scheduler_fn = sch, scheduler_params = sch_params, mask_type = mask)

unsupervised_model.fit(X_train=X_train, eval_set=[X_valid], pretraining_ratio=0.8)

clf.fit(X_train=X_train, y_train=y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_name=['train', 'val'], eval_metric=[""logloss"", 'balanced_accuracy'], max_epochs=max_epochs , patience=15, batch_size=batch_size, virtual_batch_size=virtual_batch, num_workers=workers, weights=sample_type, drop_last=False, from_unsupervised=unsupervised_model)",DL,Pytorch,sauravmaheshkar/tps-june-2021-simple-tabnet-starter,https://www.kaggle.com/sauravmaheshkar/tps-june-2021-simple-tabnet-starter
tabular-playground-series-may-2021,classification,multiclass-classification,The task is to predict the probability of each product belonging to multiple class labels based on various attributes of the product listing.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It includes training data with product IDs, associated features, and class labels, as well as test data for which predictions must be made. The features are anonymized but relate to real-world attributes.",['target'],"[""label-encode target variable"", ""convert categorical features to category type"", ""split data into training and testing sets"", ""one-hot encode categorical variables""]","input_layers = list()
embedding_layers = list()
for feature in categorical_variables:
    n_labels = df_all[feature].nunique()
    input_layer = Input(shape=(1,))
    embedding_layer = Embedding(n_labels, 3)(input_layer)
    input_layers.append(input_layer)
    embedding_layers.append(embedding_layer)
merge = concatenate(embedding_layers)
dense_1 = Dense(128, kernel_initializer='normal', activation='relu')(merge)
x = BatchNormalization()(dense_1)
x = Dropout(0.5)(x)
dense_2 = Dense(32, kernel_initializer='normal', activation='relu')(x)
x = BatchNormalization()(dense_2)
x = Dropout(0.25)(x)
flatten = Flatten()(x)
output = Dense(4, activation='softmax')(flatten)
model = Model(inputs=input_layers, outputs=output)
model.compile(loss = ""categorical_crossentropy"", optimizer = tf.keras.optimizers.Adam(), metrics=['accuracy'])
model.fit(X_train_enc, y_train_enc, validation_data=(X_test_enc, y_test_enc), epochs=20, batch_size=64, verbose=2)",DL,Tensorflow,remekkinas/autoencoder-dae-embeddings-lightautoml-mljar,https://www.kaggle.com/remekkinas/autoencoder-dae-embeddings-lightautoml-mljar
tabular-playground-series-may-2021,classification,multiclass-classification,The task is to predict the probability of each product belonging to multiple class labels based on various attributes of the product listing.,Tabular,Multi-class Log Loss,"The dataset consists of synthetic data generated using a CTGAN, based on a real dataset. It includes a training file with product IDs, associated features, and class labels, as well as a test file for predicting class probabilities. The features are anonymized but relate to real-world attributes.",['target'],"[""label encode target variable"", ""split data into training and validation sets""]","model = Sequential()
model.add(Dense(64, input_dim=NUM_FEATURES, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(NUM_CLASS, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=HEAD_EPOCHS, validation_data=(X_valid, y_valid), batch_size=128)",DL,Tensorflow,remekkinas/tps-5-hydra-keras-weighted-embedding-mix,https://www.kaggle.com/remekkinas/tps-5-hydra-keras-weighted-embedding-mix
tabular-playground-series-may-2021,classification,multiclass-classification,The task is to predict the probability that each product belongs to one of several predefined categories based on its attributes.,Tabular,Multi-class Log Loss,"The dataset consists of synthetic data generated from a real dataset using a CTGAN. It includes a training file with product IDs, features, and class labels, as well as a test file for which predictions must be made. The features are anonymized but relate to real-world eCommerce product attributes. The dataset is provided in CSV format and is approximately 18.19 MB in size.",['target'],"[""remove duplicate rows"", ""encode target labels using label encoding"", ""split data into training and testing sets"", ""reshape input data for neural network"", ""normalize input data by scaling pixel values to the range [0, 1]""]","model = Sequential()
model.add(Conv2D(256, kernel_size=(2, 2), padding='same', activation='relu', input_shape=input_shape))
model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))
model.add(Conv2D(64, (2, 2), padding='same', activation='relu'))
model.add(Conv2D(32, (2, 2), padding='same', activation='relu'))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(63, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test), callbacks=earlystop)",DL,Tensorflow,remekkinas/cnn-2d-convolution-for-solving-tps-05,https://www.kaggle.com/remekkinas/cnn-2d-convolution-for-solving-tps-05
tabular-playground-series-may-2021,classification,multiclass-classification,"The task is to predict the category of eCommerce products based on various attributes, providing predicted probabilities for each class label.",Tabular,Multi-class Log Loss,"The dataset consists of synthetic data generated from a real dataset using a CTGAN. It includes a training file with product IDs, associated features, and class labels, as well as a test file for which predictions must be made. The features are anonymized but relate to real-world attributes.",['target'],"[""label encode target variable"", ""scale features using standard scaler""]","model = models.Sequential()
model.add(layers.Dense(16, input_shape=X.shape,input_dim=50,activation='relu', name=""Hidden-1""))
layers.BatchNormalization()
model.add(layers.Dropout(0.2))
model.add(layers.Dense(8, activation='relu'))
layers.BatchNormalization()
model.add(layers.Dropout(0.2))
model.add(layers.Dense(4, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X, Y1, epochs=50,validation_split=0.3)",DL,Tensorflow,pranjalverma08/tps-may-21-the-ensemble-approach-with-optuna,https://www.kaggle.com/pranjalverma08/tps-may-21-the-ensemble-approach-with-optuna
tabular-playground-series-may-2021,classification,multiclass-classification,The task is to predict the probability that a product belongs to each of several class labels based on its features.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It contains training data with product IDs, associated features, and class labels, as well as test data for which predictions must be made. The features are anonymized but relate to real-world attributes of eCommerce products. The dataset includes train.csv, test.csv, and a sample_submission.csv file.",['target'],"[""drop target and id columns from training features"", ""split training data into training and validation sets""]","model = keras.Sequential()
model.add(keras.layers.Dense(128, activation='relu', input_shape=(input_shape,)))
model.add(keras.layers.Dense(64, activation='relu'))
model.add(keras.layers.Dense(num_classes, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=32)",DL,Tensorflow,ryanbarretto/boring-blend-of-stack-and-weights,https://www.kaggle.com/ryanbarretto/boring-blend-of-stack-and-weights
tabular-playground-series-may-2021,classification,multiclass-classification,"The task is to predict the category of eCommerce products based on various attributes, providing predicted probabilities for each class label.",Tabular,Multi-class Log Loss,"The dataset consists of synthetic data generated using a CTGAN, based on a real dataset. It includes a training file with product IDs, features, and class labels, as well as a test file for predictions. The features are anonymized but relate to real-world attributes.",['target'],"[""remove duplicate rows"", ""encode target labels using label encoding"", ""split dataset into training and validation sets"", ""scale features using MinMaxScaler"", ""convert datasets to numpy arrays""]","model = TPS05ClassificationSeq(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)
train_nn()",DL,Pytorch,remekkinas/tps-5-pytorch-nn-for-tabular-step-by-step,https://www.kaggle.com/remekkinas/tps-5-pytorch-nn-for-tabular-step-by-step
tabular-playground-series-may-2021,classification,multiclass-classification,The task is to predict the probability of each product belonging to multiple class labels based on various attributes.,Tabular,Multi-class Log Loss,"The dataset is synthetic and generated using a CTGAN, based on a real dataset that predicts the category of eCommerce products from their attributes. It includes a training file with product IDs, features, and class labels, and a test file for which predictions must be made. The dataset size is 18.19 MB and is in CSV format, licensed under Attribution 4.0 International (CC BY 4.0).",['target'],"[""label encode target variable"", ""standardize feature values""]","net = NeuralNetClassifier(TPS05Classification, device = device, lr = 0.001, max_epochs = 50, callbacks = [lr_scheduler, early_stopping, TPS05CustomCallback])
result = grid_net.fit(X,y)",DL,Pytorch,remekkinas/skorch-tutorial-simple-pytorch-nn-with-scikit,https://www.kaggle.com/remekkinas/skorch-tutorial-simple-pytorch-nn-with-scikit
tabular-playground-series-may-2021,classification,multiclass-classification,"The goal is to predict the category of an eCommerce product based on various attributes of the listing, using a multi-class classification approach.",Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It contains training data with product IDs, associated features, and class labels, as well as test data for predicting class probabilities. The features are anonymized but relate to real-world attributes.",['target'],"[""map target labels to integers"", ""scale features using standard scaling"", ""perform stratified K-fold cross-validation"", ""select important features using univariate feature selection""]","classifier = TabNetClassifier(n_d=64, n_a=64, n_steps=5, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2), verbose=1, seed=42)
classifier.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_name=['valid'], patience=20, max_epochs=EPOCHS, eval_metric=['logloss'])",DL,Pytorch,manabendrarout/deep-learning-tabnet-starter-tps-may21,https://www.kaggle.com/manabendrarout/deep-learning-tabnet-starter-tps-may21
tabular-playground-series-may-2021,classification,multiclass-classification,The task is to predict the probability of each product belonging to multiple class labels based on various attributes.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It contains training data with product IDs, associated features, and class labels, as well as test data for predicting class probabilities. The features are anonymized but relate to real-world attributes.",['target'],"[""remove duplicate rows in training data"", ""encode target labels using label encoding"", ""split the data into training and validation sets""]","model = network(input_size, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
scheduler = ReduceLROnPlateau(optimizer, 'min')
losses = trainer(model, criterion, optimizer, train_loader, valid_loader, epochs=100)",DL,Pytorch,prithviraj7387/tabular-playground-may-pytorch,https://www.kaggle.com/prithviraj7387/tabular-playground-may-pytorch
tabular-playground-series-may-2021,classification,multiclass-classification,The task is to predict the probability of each product belonging to multiple class labels based on various attributes of the product listing.,Tabular,Multi-class Log Loss,"The dataset is synthetic but based on a real dataset generated using a CTGAN. It contains training data with product IDs, associated features, and class labels, as well as test data for which predictions must be made. The features are anonymized but relate to real-world attributes.",['target'],"[""label encode target variable"", ""split data into features and target"", ""standardize feature values""]","net = NeuralNetClassifier(TPS05Classification, device = device, lr = 0.001, max_epochs = 50, callbacks = [lr_scheduler, early_stopping, TPS05CustomCallback])
pipeline = Pipeline(steps)
grid_net = GridSearchCV(pipeline, grid_params, refit = True, cv = 3, scoring = 'neg_log_loss', verbose = 1)
result = grid_net.fit(X,y)",DL,Pytorch,tkoyama010/skorch-tutorial-simple-pytorch-nn-with-scikit,https://www.kaggle.com/tkoyama010/skorch-tutorial-simple-pytorch-nn-with-scikit
tabular-playground-series-apr-2021,classification,binary-classification,The task is to predict whether a passenger survived the sinking of the Synthanic based on various features.,Tabular,Accuracy,"The dataset is synthetic but based on the actual Titanic dataset, generated using a CTGAN. It includes a training set with known outcomes and a test set for predictions. Features include passenger demographics and ticket information. The training set is used to build models, while the test set is for evaluating model performance on unseen data.",['Survived'],"[""fill missing values in 'Embarked' with 'No'"", ""fill missing values in 'Cabin' with '_'"", ""extract cabin type from 'Cabin'"", ""map 'Ticket' to a simplified format"", ""fill missing 'Age' with median and round"", ""fill missing 'Fare' with median based on 'Pclass'"", ""extract first and second names from 'Name'"", ""create 'SameFirstName' and 'SameSecondName' features"", ""convert 'Sex' to binary"", ""calculate 'FamilySize' from 'SibSp' and 'Parch'"", ""apply log transformation to certain numerical features"", ""standardize numerical features"", ""apply label encoding to categorical features"", ""apply target encoding to categorical features"", ""transform features using Denoising AutoEncoder""]","dae = DAE(cat_cols=cat_cols, num_cols=num_cols, encoding_dim=encoding_dim)
X = dae.fit_transform(df[feature_cols])",DL,Tensorflow,jeongyoonlee/dae-with-2-lines-of-code-with-kaggler,https://www.kaggle.com/jeongyoonlee/dae-with-2-lines-of-code-with-kaggler
tabular-playground-series-apr-2021,classification,binary-classification,The task is to predict whether a passenger survived the sinking of the Synthanic based on various features.,Tabular,Accuracy,"The dataset is synthetic but based on the actual Titanic dataset, generated using a CTGAN. It includes a training set with known outcomes and a test set for predictions. Features include passenger demographics and ticket information. The training set is used to build models, while the test set evaluates model performance on unseen data.",['Survived'],"[""fill missing values in 'Embarked' with 'No'"", ""fill missing values in 'Cabin' with '_'"", ""extract first character from 'Cabin' to create 'CabinType'"", ""map 'Ticket' to first part or 'X'"", ""fill missing values in 'Age' with median and round"", ""fill missing values in 'Fare' with median and round"", ""extract first and second names from 'Name'"", ""create 'SameFirstName' and 'SameSecondName' features based on name counts"", ""convert 'Sex' to binary"", ""create 'FamilySize' feature by summing 'SibSp' and 'Parch'""]","num_input = keras.layers.Input((num_dim,), name='num_input')
cat_input = keras.layers.Input((1,), name='cat_input')
cat_emb = keras.layers.Embedding(input_dim=df[cat_col].max() + 1, output_dim=emb_dim)(cat_input)
merged_inputs = keras.layers.Concatenate()([num_input, cat_emb])
encoded = keras.layers.Dense(encoding_dim * 3, activation='relu')(merged_inputs)
encoded = keras.layers.Dense(encoding_dim * 2, activation='relu')(encoded)
encoded = keras.layers.Dense(encoding_dim, activation='relu')(encoded)
decoded = keras.layers.Dense(encoding_dim * 2, activation='relu')(encoded)
decoded = keras.layers.Dense(encoding_dim * 3, activation='relu')(decoded)
decoded = keras.layers.Dense(num_dim + emb_dims, activation='linear')(decoded)
ae.compile(optimizer='adam')
ae.fit(inputs, inputs, epochs=100, batch_size=16384, shuffle=True, validation_split=.2)",DL,Tensorflow,jeongyoonlee/autoencoder-pseudo-label-autolgb,https://www.kaggle.com/jeongyoonlee/autoencoder-pseudo-label-autolgb
tabular-playground-series-apr-2021,classification,binary-classification,The task is to predict whether a passenger survived the sinking of the Synthanic based on various features.,Tabular,Accuracy,"The dataset is synthetic but based on the actual Titanic dataset, generated using a CTGAN. It includes a training set with known outcomes and a test set for predictions. Features include passenger demographics and ticket information. The training set is used to build models, while the test set evaluates performance on unseen data.",['Survived'],"[""fill missing values in 'Cabin' with 'U'"", ""map 'Cabin' to numerical values"", ""map 'Sex' to binary values"", ""fill missing values in 'Age' with median"", ""fill missing values in 'Fare' with median"", ""fill missing values in 'Embarked' with 'C'"", ""create 'FamilySize' feature"", ""drop unnecessary columns""]","classifier = Sequential()
classifier.add(Dense(units=10,kernel_initializer='uniform',activation='relu',input_dim=8))
classifier.add(Dropout(rate = 0.2))
classifier.add(Dense(units=64,kernel_initializer='uniform',activation='relu'))
classifier.add(Dropout(rate = 0.2))
classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))
classifier.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])
classifier.fit(X_train, y_train)",DL,Tensorflow,pranjalverma08/tps-april-21-ann-pseudo-label-score-81-101,https://www.kaggle.com/pranjalverma08/tps-april-21-ann-pseudo-label-score-81-101
tabular-playground-series-apr-2021,classification,binary-classification,The task is to predict whether a passenger survived the sinking of the Synthanic based on various features.,Tabular,Accuracy,"The dataset is synthetic but based on the actual Titanic dataset, generated using a CTGAN. It includes a training set with known outcomes and a test set for predictions. Features include passenger demographics and ticket information. The training set is used to build models, while the test set is for evaluating model performance on unseen data.",['Survived'],"[""drop unnecessary columns"", ""fill missing values with median or mode"", ""map categorical variables to numerical values"", ""create new features based on existing data"", ""normalize or scale features if necessary""]","classifier = Sequential()
classifier.add(Dense(units=10,kernel_initializer='uniform',activation='relu',input_dim=7))
classifier.add(Dropout(rate = 0.2))
classifier.add(Dense(units=64,kernel_initializer='uniform',activation='relu'))
classifier.add(Dropout(rate = 0.2))
classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))
classifier.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])
classifier.fit(X_train,y_train)",DL,Tensorflow,pranjalverma08/tps-april-21-the-ann-approach-score-80-782,https://www.kaggle.com/pranjalverma08/tps-april-21-the-ann-approach-score-80-782
tabular-playground-series-apr-2021,classification,binary-classification,The task is to predict whether a passenger survived the sinking of the Synthanic based on various features.,Tabular,Accuracy,"The dataset is synthetic but based on the actual Titanic dataset, generated using a CTGAN. It contains a training set with known outcomes and a test set for predictions. Features include passenger demographics and ticket information. The training set is used to build models, while the test set is for evaluating model performance on unseen data.",['Survived'],"[""fill missing values in 'Embarked' with 'No'"", ""fill missing values in 'Cabin' with '_'"", ""extract cabin type from 'Cabin'"", ""map 'Ticket' to first part or 'X'"", ""fill missing values in 'Age' with median and round"", ""fill missing values in 'Fare' with median"", ""extract first and second names from 'Name'"", ""create 'SameFirstName' and 'SameSecondName' features"", ""convert 'Sex' to binary"", ""calculate 'FamilySize' as sum of 'SibSp' and 'Parch' plus one"", ""apply log transformation to selected numerical features"", ""standardize numerical features using StandardScaler"", ""encode categorical features using LabelEncoder""]","num_input = keras.layers.Input((num_dim,), name='num_input')
cat_input = keras.layers.Input((1,), name='cat_input')
encoded = keras.layers.Dense(128, activation='relu')(num_input)
encoded = keras.layers.Dropout(0.2)(encoded)
clf_output = keras.layers.Dense(1, activation='sigmoid')(encoded)
model = keras.Model(inputs=[num_input, cat_input], outputs=clf_output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])
model.fit(inputs, targets, epochs=30, batch_size=16384, validation_split=0.2)",DL,Tensorflow,jeongyoonlee/supervised-emphasized-denoising-autoencoder,https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder
tabular-playground-series-apr-2021,classification,binary-classification,The task is to predict whether a passenger survived the sinking of the Synthanic based on various features.,Tabular,Accuracy,"The dataset is synthetic but based on the actual Titanic dataset, generated using a CTGAN. It contains a training set with known outcomes and a test set for predictions. Features include passenger demographics and ticket information, such as gender, age, class, and fare. The training set is used to build models, while the test set evaluates performance on unseen data.",['Survived'],"[""drop irrelevant features"", ""drop rows with missing values"", ""fill missing fare values with mean"", ""encode categorical features for sex and embarked""]","self.fc1 = nn.Linear(self.input_size, 1024)
self.fc2 = nn.Linear(1024, 768)
self.fc3 = nn.Linear(768, 128)
self.fc4 = nn.Linear(128, classes)
model.compile(optimizer='adam', loss='cross_entropy')
trainer.fit(model)",DL,Pytorch,heyytanay/tps-april-eda-pytorch-lightning-on-tabular-data,https://www.kaggle.com/heyytanay/tps-april-eda-pytorch-lightning-on-tabular-data
tabular-playground-series-apr-2021,classification,binary-classification,The task is to predict whether a passenger survived the sinking of the Synthanic based on various features.,Tabular,Accuracy,"The dataset is synthetic but based on the actual Titanic dataset, generated using a CTGAN. It includes a training set with ground truth outcomes and a test set for predictions. Features include passenger demographics and ticket information. The training set is used to build models, while the test set evaluates model performance on unseen data.",['Survived'],"[""drop PassengerId column"", ""fill missing Cabin values and create has_Cabin and Deck features"", ""fill missing Age and Fare values based on Pclass and Deck"", ""create FamilySize feature"", ""log-transform Fare values"", ""fill missing Embarked and Ticket values"", ""create Name_length, Last_name, and First_name features"", ""apply age categorization"", ""encode categorical features using LabelEncoder"", ""split training data into k-folds for cross-validation""]","self.layer_1 = nn.Linear(15, 64)
self.layer_2 = nn.Linear(64, 64)
self.layer_out = nn.Linear(64, 1)
model.compile(optimizer=optim.Adam(model.parameters(), lr=LEARNING_RATE), loss=nn.BCEWithLogitsLoss())
model.fit(train_loader)",DL,Pytorch,godzill22/basic-pytorch-model-tps-april,https://www.kaggle.com/godzill22/basic-pytorch-model-tps-april
tabular-playground-series-apr-2021,classification,binary-classification,The task is to predict whether a passenger survived the sinking of the Synthanic based on various features.,Tabular,Accuracy,"The dataset is synthetic but based on the actual Titanic dataset, generated using a CTGAN. It includes a training set with known outcomes and a test set for predictions. Features include passenger demographics and ticket information. The training set is used to build models, while the test set evaluates model performance on unseen data.",['Survived'],"[""fill missing age values based on passenger class"", ""fill missing cabin values with 'X' and extract the first letter"", ""fill missing ticket values with 'X' and extract the first part"", ""fill missing fare values with the mean fare"", ""fill missing embarked values with 'X'"", ""extract the first name from the full name""]","clf = TabNetClassifier(n_a=BEST_PARAMS['n_a'], n_d=BEST_PARAMS['n_d'], n_steps=BEST_PARAMS['n_steps'], n_independent=BEST_PARAMS['n_independent'], seed=42)
clf.fit(X_train=X_train, y_train=y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_name=['train', 'valid'], eval_metric=['accuracy'], max_epochs=50, patience=10, batch_size=BEST_PARAMS['batch_size'], virtual_batch_size=BEST_PARAMS['virtual_batch_size'])",DL,Pytorch,khoongweihao/tps-apr-2021-tabnet,https://www.kaggle.com/khoongweihao/tps-apr-2021-tabnet
tabular-playground-series-apr-2021,classification,binary-classification,The task is to predict whether a passenger survived the sinking of the Synthanic based on various features.,Tabular,Accuracy,"The dataset is synthetic but based on the actual Titanic dataset, generated using a CTGAN. It includes a training set with known outcomes and a test set for predictions. Features include passenger demographics and ticket information. The training set is used to build models, while the test set evaluates model performance on unseen data.",['Survived'],"[""normalize text and reorder names and surnames"", ""load data into torch tensors""]","encoder = LSTMEncoder(**args).to(args['device'])
predictor = Predictor(**args).to(args['device'])
optimizer = torch.optim.Adam(list(encoder.parameters())+list(predictor.parameters()),lr=args['learning_rate'])
criterion = nn.NLLLoss(reduction='mean').to(args['device'])",DL,Pytorch,saztorralba/synthetictitanicdata-lstmfulldataencoder,https://www.kaggle.com/saztorralba/synthetictitanicdata-lstmfulldataencoder
tabular-playground-series-apr-2021,classification,binary-classification,The task is to predict whether a passenger survived the sinking of the Synthanic based on various features.,Tabular,Accuracy,"The dataset is synthetic but based on the actual Titanic dataset, generated using a CTGAN. It contains a training set with ground truth outcomes and a test set for predictions. Features include passenger demographics and ticket information. The training set is used to build models, while the test set evaluates model performance on unseen data.",['Survived'],"[""fill missing values in Age with mean"", ""fill missing values in Fare with mean"", ""fill missing values in Embarked with 'X'"", ""fill missing values in Cabin with 'X-1'"", ""create interaction features between numerical columns"", ""normalize Age and Fare"", ""one-hot encode categorical features""]","self.fc1 = nn.Linear(input_shape, 32)
self.bn1 = nn.BatchNorm1d(32)
self.drop1 = nn.Dropout(0.3)
self.fc2 = nn.Linear(32, 32)
self.bn2 = nn.BatchNorm1d(32)
self.drop2 = nn.Dropout(0.3)
self.out = nn.Linear(32, 1)
model.compile(optimizer='adam', loss='BCEWithLogitsLoss')
model.fit(train_dl, epochs=epochs)",DL,Pytorch,barteksadlej123/pytorch-model-feature-engineering,https://www.kaggle.com/barteksadlej123/pytorch-model-feature-engineering
tabular-playground-series-mar-2021,classification,binary-classification,"The task is to predict the probability of a binary target based on various feature columns, which include both categorical and continuous types.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target. The training data includes feature columns cat0 to cat18, which are categorical, and cont0 to cont10, which are continuous. The training file is named train.csv and contains the target column, while the test file is named test.csv and requires predictions for the target. A sample submission file is also provided in the correct format.",['target'],"[""drop the 'id' column from train and test datasets"", ""fill missing values in categorical features with the mode"", ""clip continuous features in the test set to the range of the training set"", ""create quantile features for continuous variables"", ""apply ordinal encoding to categorical features""]","inputs = []
outputs = []
for c in catcols:
    inp = layers.Input(shape=(1,))
    out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)
    out = layers.SpatialDropout1D(0.25)(out)
    out = layers.Reshape(target_shape=(embed_dim, ))(out)
    inputs.append(inp)
    outputs.append(out)

x = layers.Concatenate()(outputs)
x = layers.BatchNormalization()(x)
x = layers.Dense(300, activation='relu')(x)
x = layers.Dropout(0.3)(x)
x = layers.BatchNormalization()(x)
y = layers.Dense(1, activation='sigmoid')(x)
model = Model(inputs=inputs, outputs=y)
model.compile(optimizer=tfa.optimizers.SWA(tf.keras.optimizers.Adam(learning_rate=learning_rate)), loss=losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=metrics.AUC(name=""AUC"") )
model.fit(X_train, y_train, validation_data=(X_test, y_test), verbose=Verbose, batch_size=1024, callbacks=[es, sb, plateau], epochs=100)",DL,Tensorflow,siavrez/kerasembeddings,https://www.kaggle.com/siavrez/kerasembeddings
tabular-playground-series-mar-2021,classification,binary-classification,"The task is to predict the probability of a binary target based on various feature columns, which include both categorical and continuous types.",Tabular,AUCROC,"The dataset consists of training and test data for a binary classification problem. The training data includes a target column and multiple feature columns, with categorical features labeled from cat0 to cat18 and continuous features labeled from cont0 to cont10. The test set is used to predict the target for each row, and a sample submission file is provided to illustrate the required format.",['target'],"[""encode categorical features using label encoding"", ""shuffle the data"", ""split features and targets""]","cat0_inp = tf.keras.Input(shape=(1,))
cat0_out = tf.keras.layers.Embedding(input_dim=2, output_dim=1, name='cat0')(cat0_inp)
cat0_out = tf.keras.layers.Reshape(target_shape=(1,))(cat0_out)

cat1_inp = tf.keras.Input(shape=(1,))
cat1_out = tf.keras.layers.Embedding(input_dim=15, output_dim=3, name='cat1')(cat1_inp)
cat1_out = tf.keras.layers.Reshape(target_shape=(3,))(cat1_out)

# ... (similar code for other categorical inputs)

con_inputs = tf.keras.Input(shape=(con_data.shape[1]))
fc1 = tf.keras.layers.Dense(512, activation='relu', name=""cont_fc1"")(con_inputs)

# ... (similar code for continuous inputs)

all_inputs = cat_inputs + [con_inputs]
model = tf.keras.Model(inputs=all_inputs, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(training_data[0], training_data[1], epochs=5, validation_data=validation_data, callbacks=[early_stopper, checkpoint])",DL,Tensorflow,heyytanay/tps-eda-5-folds-tf-model-dabl-plot,https://www.kaggle.com/heyytanay/tps-eda-5-folds-tf-model-dabl-plot
tabular-playground-series-mar-2021,classification,binary-classification,"The task is to predict the probability of a binary target based on various feature columns, which include both categorical and continuous types.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target. The training data includes a target column, while the test data is used to predict the target probabilities. The feature columns are divided into categorical columns (cat0 to cat18) and continuous columns (cont0 to cont10). The dataset is provided in CSV format, with a total size of 131.65 MB.",['target'],"[""one-hot encode categorical features""]","inputs = Input((11,))
x = Dense(1500, activation='relu')(inputs)
x = Dense(1500, activation='relu', name=""feature"")(x)
x = Dense(1500, activation='relu')(x)
outputs = Dense(11, activation='relu')(x)
model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='mse')
autoencoder.fit(alldata[continous_cols], alldata[continous_cols], epochs=20, batch_size=256, shuffle=True)",DL,Tensorflow,pradeepboopathy/lightgbm-10-folds,https://www.kaggle.com/pradeepboopathy/lightgbm-10-folds
tabular-playground-series-mar-2021,classification,binary-classification,"The task is to predict the probability of a binary target based on various feature columns, which include both categorical and continuous variables.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target. The training data includes a target column, while the test data is used to predict the target probabilities. The feature columns are categorized into categorical features (cat0 to cat18) and continuous features (cont0 to cont10). The dataset is provided in CSV format and is approximately 131.65 MB in size.",['target'],"[""label-encode categorical features"", ""apply rank gauss normalization to continuous features""]","AUTOENCODER.compile(optimizer = optimizer, loss = rmse)

history = AUTOENCODER.fit(X_Train, 
                Xx.drop(catVars, axis=1), 
                epochs = eps, 
                batch_size = bs, 
                shuffle = False,
                callbacks=[history])",DL,Tensorflow,frankmollard/encoder-decoder-combined,https://www.kaggle.com/frankmollard/encoder-decoder-combined
tabular-playground-series-mar-2021,classification,binary-classification,"The task is to predict the probability of a binary target based on various feature columns, which include both categorical and continuous types.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target. The training data includes a target column and multiple feature columns, with categorical features labeled from cat0 to cat18 and continuous features labeled from cont0 to cont10. The test set is used to predict the target for each row, and a sample submission file is provided to illustrate the required format.",['target'],"[""one-hot encode categorical features"", ""drop original categorical columns after encoding""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_shape,)))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[EarlyStopping(patience=5)])",DL,Tensorflow,kesavsivakumar/tab-pgrnd-k-best-features-xgboost-vs-catboost,https://www.kaggle.com/kesavsivakumar/tab-pgrnd-k-best-features-xgboost-vs-catboost
tabular-playground-series-mar-2021,classification,binary-classification,"The task is to predict the probability of a binary target based on various feature columns, which include both categorical and continuous types.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target. The training data includes feature columns categorized as categorical (cat0 to cat18) and continuous (cont0 to cont10). The test set is used to predict the target for each row, and a sample submission file is provided to illustrate the required format.",['target'],"[""normalize continuous features"", ""one-hot encode categorical features"", ""combine categorical and continuous features""]","model = Model(num_features=3000, num_targets=1, hidden_size=1000)
model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'],eps=0.00001)
scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, max_lr=1e-2, epochs=CFG['epochs'], steps_per_epoch=len(trainloader))
loss_fn = nn.BCEWithLogitsLoss()
train_fn(model, optimizer, scheduler, loss_fn, trainloader, epoch, device)",DL,Pytorch,davidedwards1/tabularmarch21-dae-starter-cv-inference,https://www.kaggle.com/davidedwards1/tabularmarch21-dae-starter-cv-inference
tabular-playground-series-mar-2021,classification,binary-classification,The task is to predict the probability of a binary target based on various feature columns.,Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target. The training data includes feature columns categorized as categorical (cat0 to cat18) and continuous (cont0 to cont10). The test set is used to predict the target for each row, and a sample submission file is provided to illustrate the required format.",['target'],"[""label encode categorical features"", ""group low frequency categorical values into one category"", ""clip numerical features in test set to match training set"", ""quantile bin continuous features""]","model = TabNetClassifier(**tabnet_params)
model.fit(X_train=X_train, y_train=y_train,
          from_unsupervised=pretrainer if PRETRAIN else None,
          eval_set=[(X_train, y_train), (X_valid, y_valid)],
          eval_name=[""train"", ""valid""],
          eval_metric=[""auc""],
          batch_size=BS,
          virtual_batch_size=256,
          max_epochs=MAX_EPOCH,
          drop_last=True,
          pin_memory=True,
          patience=10,)",DL,Pytorch,optimo/tabnet-baseline,https://www.kaggle.com/optimo/tabnet-baseline
tabular-playground-series-mar-2021,classification,binary-classification,"The task is to predict the probability of a binary target based on various feature columns, which include both categorical and continuous types.",Tabular,AUCROC,"The dataset consists of training and test data in CSV format. The training data includes a target column and multiple feature columns, with 19 categorical features and 11 continuous features. The test set is used to predict the target for each row, and a sample submission file is provided to illustrate the required format for predictions.",['target'],"[""label encode categorical features"", ""apply transformations to reduce distinct values in categorical features""]","layers = []
in_features = 30
for i in range(2):
    out_features = 50
    layers.append(torch.nn.Linear(in_features, out_features))
    layers.append(torch.nn.ReLU())
    p = 0.4928102110636615
    layers.append(nn.Dropout(p))
    in_features = out_features
classes = 1
layers.append(torch.nn.Linear(in_features, classes))
layers.append(torch.nn.Sigmoid())
model = torch.nn.Sequential(*layers).to(DEVICE)
lr = 0.001879813193722479
optimizer = getattr(optim, 'Adam')(model.parameters(), lr=lr)
for epoch in range(50):
    model.train()
    for batch_idx, (data, target) in enumerate(train_generator):
        data, target = data.to(DEVICE), target.to(DEVICE)
        optimizer.zero_grad()
        output = model(data.float())
        msloss = nn.MSELoss()
        loss = msloss(output.float(),target.float())
        loss.backward()
        optimizer.step()",DL,Pytorch,jyotmakadiya/eda-pytorch-nn-with-hpo-using-optuna,https://www.kaggle.com/jyotmakadiya/eda-pytorch-nn-with-hpo-using-optuna
tabular-playground-series-mar-2021,classification,binary-classification,"The task is to predict the probability of a binary target based on various feature columns, which include both categorical and continuous types.",Tabular,AUCROC,"The dataset consists of training and test data for predicting a binary target. The training data includes feature columns cat0 to cat18, which are categorical, and cont0 to cont10, which are continuous. The training file is named train.csv and contains the target column, while the test file is named test.csv and requires predictions for the target. A sample submission file is provided in sample_submission.csv.",['target'],"[""label encode categorical features""]","model = TabNetClassifier(**tabnet_params)
model.fit(X_train=train_df,
          y_train=train_target,
          eval_set=[(val_df, val_target)],
          eval_name = [""val""],
          eval_metric = ['auc'],
          max_epochs=MAX_EPOCH,
          patience=20, 
          batch_size=BATCH_SIZE, virtual_batch_size=128,
          num_workers=1, 
          drop_last=False,
          from_unsupervised=unsupervised_model)",DL,Pytorch,rapela/tps-03-21-tabnet-classifier,https://www.kaggle.com/rapela/tps-03-21-tabnet-classifier
tabular-playground-series-mar-2021,classification,binary-classification,The task is to predict the probability of a binary target based on various feature columns.,Tabular,AUCROC,The dataset consists of training and test data for predicting a binary target. The training data includes feature columns categorized as categorical (cat0 to cat18) and continuous (cont0 to cont10). The test set is used to predict the target for each row. The dataset is provided in CSV format with a total size of 131.65 MB.,['target'],"[""normalize continuous features"", ""encode categorical features""]","self.output = nn.Linear(self.hid_dim,self.out_dim)
self.softmax = nn.LogSoftmax(dim=1)
optimizer = torch.optim.Adam(predictor.parameters(),lr=args['learning_rate'])
loss = train_model(trainset,traintargets,predictor,optimizer,criterion,**args)",DL,Pytorch,saztorralba/tabulardata-pytorchmlp,https://www.kaggle.com/saztorralba/tabulardata-pytorchmlp
tabular-playground-series-feb-2021,regression,continuous-regression,"The task is to predict a continuous target variable based on various feature columns, which include both categorical and continuous types.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training file and a test file, where the training file contains feature columns and a target column. The feature columns are divided into categorical columns (cat0 to cat9) and continuous columns (cont0 to cont13). The test file is used to predict the target for each row. The dataset is in CSV format and has a size of 154.66 MB.",['target'],"[""combine minority categories into composite categories"", ""one-hot encode categorical features"", ""standardize numerical features"", ""add PCA features for dimensionality reduction""]","model.add(layers.Dense(200, activation='elu', input_shape=(input_feat_dim,), kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(MonteCarloDropout(dropout_val))
model.add(layers.Dense(100, activation='elu', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(MonteCarloDropout(dropout_val))
model.add(layers.Dense(50, activation='elu', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(MonteCarloDropout(dropout_val))
model.add(layers.Dense(1))
model.compile(optimizer=keras.optimizers.Nadam(lr=lr), loss='mse', metrics=['mse'])
history = model.fit(X_train, y_train, epochs=50, batch_size=256, validation_data=(X_val, y_val), callbacks=trg_callbacks)",DL,Tensorflow,benfraser/eda-and-dnn-regression-models,https://www.kaggle.com/benfraser/eda-and-dnn-regression-models
tabular-playground-series-feb-2021,regression,continuous-regression,"The task is to predict a continuous target variable based on various feature columns, which include both categorical and continuous types.",Tabular,RMSE – Root Mean Squared Error,The dataset consists of training and test data for predicting a continuous target variable. The training data includes feature columns categorized as categorical (cat0 to cat9) and continuous (cont0 to cont13). The test set is used to predict the target for each row. The dataset is provided in CSV format and is approximately 154.66 MB in size.,['target'],"[""drop unnecessary columns"", ""transform categorical data to numeric using column encoder""]","model=M.Sequential(name='Determinator_Model')
model.add(L.Dense(1000,input_dim=input_shape,activation='relu'))
model.add(L.Dense(100,'relu'))
model.add(L.Dense(1,'sigmoid'))
model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')
train_model(g_model,d_model,gan_model,data,epochs=1000,n_counter=200,latent_dim=16,n=100,batch_size=128)",DL,Tensorflow,accountstatus/making-synthetic-data-using-gan,https://www.kaggle.com/accountstatus/making-synthetic-data-using-gan
tabular-playground-series-feb-2021,regression,continuous-regression,"The task is to predict a continuous target variable based on various feature columns, which include both categorical and continuous types.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test data for predicting a continuous target variable. The training data includes feature columns categorized as categorical (cat0 to cat9) and continuous (cont0 to cont13). The test set is used to predict the target for each row. The dataset is provided in CSV format, with a total size of 154.66 MB, and is licensed under Attribution 4.0 International (CC BY 4.0).",['target'],"[""drop id column from train and test datasets"", ""label encode categorical features"", ""remove skewed categorical feature"", ""standardize continuous features""]","model = Sequential()
model.add(Dense(23, activation=""relu""))
model.add(Dense(23, activation=""relu""))
model.add(Dense(23, activation=""relu""))
model.add(Dense(23, activation=""relu""))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mae')
model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val) , epochs=10, batch_size=128)",DL,Tensorflow,godzill22/tabular-playground-feb-with-keras,https://www.kaggle.com/godzill22/tabular-playground-feb-with-keras
tabular-playground-series-feb-2021,regression,continuous-regression,"The task is to predict a continuous target variable based on various feature columns, which include both categorical and continuous types.",Tabular,RMSE – Root Mean Squared Error,The dataset consists of training and test data for predicting a continuous target variable. The training data includes feature columns categorized as categorical (cat0 to cat9) and continuous (cont0 to cont13). The test set is provided for making predictions on the target variable. The dataset is in CSV format and has a size of 154.66 MB.,['target'],"[""median-impute missing values"", ""one-hot encode categorical features"", ""scale continuous features using standard scaler""]","inputs = tf.keras.Input(shape=(70), name=""input"")
x = inputs
for i_layer in range(3): 
    x = tf.keras.layers.Dense(1024, activation=tf.keras.layers.LeakyReLU(0.1))(x)
    x = tf.keras.layers.Dropout(0)(x)
x = tf.keras.layers.Dense(64, activation=None)(x)
x = tf.keras.layers.Dense(64, activation=None)(x)
x = tf.keras.layers.Dense(64, activation=None)(x)
outputs = tf.keras.layers.Dense(1, activation=None)(x)
model = tf.keras.Model(inputs, outputs)
model.compile(optimizer=tf.keras.optimizers.Adam(0.00001), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.RootMeanSquaredError()])
nn_model.fit(x=train_X, y=train_y, epochs=50, batch_size=4096, validation_data=(test_X, test_y), callbacks=[early_stopping], verbose=False)",DL,Tensorflow,gregorycalvez/sklearn-and-tensorflow,https://www.kaggle.com/gregorycalvez/sklearn-and-tensorflow
tabular-playground-series-feb-2021,regression,continuous-regression,"The task is to predict a continuous target variable based on multiple feature columns, which include both categorical and continuous types.",Tabular,RMSE – Root Mean Squared Error,The dataset consists of training and test data in CSV format. The training data includes a target column and various feature columns categorized as categorical (cat0 to cat9) and continuous (cont0 to cont13). The test set requires predictions for the target variable for each row. The dataset size is 154.66 MB and is licensed under Attribution 4.0 International (CC BY 4.0).,['target'],"[""one-hot encode categorical features"", ""drop original categorical columns"", ""normalize continuous features""]","model = Sequential()
model.add(Dense(53, input_dim=20, kernel_initializer='normal', activation='relu'))
model.add(Dense(2670, activation='relu'))
model.add(Dense(1345, activation='relu'))
model.add(Dense(1, activation='linear'))
model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])
history_pl = model.fit(X_train_fs,y, batch_size=128, epochs=30, verbose=1, callbacks=[earlyStopping, mcp_save,reduce_lr_loss], validation_split=0.25)",DL,Tensorflow,kesavsivakumar/f-regression-30-xgboost-catboost-mlp,https://www.kaggle.com/kesavsivakumar/f-regression-30-xgboost-catboost-mlp
tabular-playground-series-feb-2021,regression,continuous-regression,"The task is to predict a continuous target variable based on multiple feature columns, which include both categorical and continuous types.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test data for predicting a continuous target variable. The training data includes feature columns labeled cat0 to cat9 for categorical features and cont0 to cont13 for continuous features. The test data is structured similarly, and a sample submission file is provided to illustrate the required format for predictions.",['target'],"[""convert categorical features using label encoding"", ""reshape target variable for training""]","model = MLPModel(3 * dae_hidden_size, hidden_size=mlp_hidden_size, input_dropout=mlp_input_dropout, dropout_rate=mlp_dropout).cuda()
optimizer = torch.optim.Adam(model.parameters(), lr=mlp_init_lr, weight_decay=mlp_l2_reg)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=1/3, patience=10, verbose=0, cooldown=2, min_lr=1e-7)
for epoch in range(777):
    model.train()
    for i, (x, target) in enumerate(train_dl):
        x, target = x.cuda(), target.cuda()
        with torch.no_grad(): x = dae.feature(x)
        optimizer.zero_grad()
        loss = torch.nn.functional.mse_loss(model.forward(x), target)
        loss.backward()
        optimizer.step()",DL,Pytorch,ryanzhang/pytorch-dae-starter-code,https://www.kaggle.com/ryanzhang/pytorch-dae-starter-code
tabular-playground-series-feb-2021,regression,continuous-regression,The task is to predict a continuous target variable based on various categorical and continuous feature columns provided in the dataset.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test data files. The training data contains 10 categorical features and 14 continuous features, with a target column that needs to be predicted. The test data is used to evaluate the model's performance. The dataset is in CSV format and has a size of 154.66 MB.",['target'],"[""label-encode categorical features"", ""quantile-transform continuous features""]","self.layer1 = self.batch_linear_drop(self.emb_size+self.cont_size,hidden_sizes[0],0.1,activation=nn.ReLU)
self.layer2 = self.batch_linear_drop(hidden_sizes[0],hidden_sizes[1],0.1,activation=nn.ReLU)
self.layer3 = self.batch_linear_drop(hidden_sizes[1],hidden_sizes[2],0.1,activation=nn.ReLU)
self.layer4 = self.batch_linear_drop(hidden_sizes[2],hidden_sizes[3],0.1,activation=nn.ReLU)
self.layer5 = self.batch_linear(hidden_sizes[3],output_size)

optimizer = optim.Adam(model.parameters(),lr=config['learning_rate'])

train_loss = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler=lr_scheduler)
valid_loss,predictions = valid_loop(valid_dl,model,loss_fn,device)",DL,Pytorch,maunish/tps-feb-super-cool-eda-embeddings-pytorch,https://www.kaggle.com/maunish/tps-feb-super-cool-eda-embeddings-pytorch
tabular-playground-series-feb-2021,regression,continuous-regression,"The task is to predict a continuous target variable based on various feature columns, which include both categorical and continuous types.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test data in CSV format. The training data includes a target column and multiple feature columns, with categorical features labeled as cat0 to cat9 and continuous features labeled as cont0 to cont13. The test set requires predictions for the target variable for each row, and a sample submission file is provided to illustrate the required format.",['target'],"[""label encode categorical features""]","model = TabNetRegressor(**tabnet_params)
model.fit(X_train=train_df,
          y_train=train_target,
          eval_set=[(val_df, val_target)],
          eval_name = [""val""],
          eval_metric = ['mse'],
          max_epochs=MAX_EPOCH,
          patience=20, batch_size=BATCH_SIZE,
          num_workers=1, drop_last=False)",DL,Pytorch,rapela/tps-02-21-tabnet-regressor,https://www.kaggle.com/rapela/tps-02-21-tabnet-regressor
tabular-playground-series-feb-2021,regression,continuous-regression,"The task is to predict a continuous target variable based on various feature columns, which include both categorical and continuous types.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test data for predicting a continuous target variable. The training data includes a target column and multiple feature columns, with categorical features labeled cat0 to cat9 and continuous features labeled cont0 to cont13. The test set is provided for making predictions, and a sample submission file is included to demonstrate the required format.",['target'],"[""one-hot encode categorical features"", ""scale numerical features using quantile transformation""]","model = NeuralNetwork(inp,hid)
loss = RMSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)
model.cuda()
nn_model = torch_fit(x_tensor, y_tensor, x_val_tensor, y_val_tensor, model=model, loss=loss, lr=0.00728, num_epochs=9550)",DL,Pytorch,jobyingramdodd/nn-tab,https://www.kaggle.com/jobyingramdodd/nn-tab
tabular-playground-series-feb-2021,regression,continuous-regression,"The task is to predict a continuous target variable based on various feature columns, which include both categorical and continuous types.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test data for predicting a continuous target variable. The training data includes feature columns labeled cat0 to cat9 for categorical features and cont0 to cont13 for continuous features. The test set is provided for making predictions on the target variable. The dataset is in CSV format and has a size of 154.66 MB, licensed under Attribution 4.0 International (CC BY 4.0).",['target'],"[""label encode categorical features""]","re = TabNetRegressor()
re.fit(
  X_train, y_train,
  eval_set=[(X_test, y_test)],
   eval_name=['train'],
    eval_metric=[ 'rmse'],
    max_epochs=250,
    patience=50,
    batch_size=1042, virtual_batch_size=128,
    num_workers=2,
    drop_last=False
)",DL,Pytorch,kingabzpro/tabnet-tps-feb,https://www.kaggle.com/kingabzpro/tabnet-tps-feb
tabular-playground-series-jan-2021,regression,continuous-regression,The task is to predict a continuous target variable based on 14 continuous feature columns from the dataset.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training file with 300,000 rows and 16 columns, including a target column, and a test file with 200,000 rows and 15 columns. The features are continuous and labeled as cont1 to cont14. There are no missing values in either dataset.",['target'],"[""normalize feature values"", ""split dataset into training and validation sets""]","model = keras.Sequential([normalizer,layers.Dense(64, activation='relu'),layers.Dense(64, activation='relu'),layers.Dense(1)])
model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.001))
history = model.fit(X_train, y_train, validation_split=0.2,verbose=0, epochs=100)",DL,Tensorflow,dwin183287/tps-jan-2021-eda-models,https://www.kaggle.com/dwin183287/tps-jan-2021-eda-models
tabular-playground-series-jan-2021,regression,continuous-regression,The task is to predict a continuous target variable based on multiple continuous feature columns.,Tabular,RMSE – Root Mean Squared Error,The dataset consists of training and test data in CSV format. The training data includes a target column and 14 continuous feature columns labeled cont1 to cont14. The test data is used to predict the target for each row. The total size of the dataset is 144.43 MB and it is licensed under Attribution 4.0 International (CC BY 4.0).,['target'],"[""abs transform feature values""]","inputs = Input((14,))
x = Dense(500, activation='relu')(inputs)
x = Dense(500, activation='relu', name=""feature"")(x)
x = Dense(500, activation='relu')(x)
outputs = Dense(14, activation='relu')(x)
model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='mse')
autoencoder.fit(alldata[cont_features], alldata[cont_features], epochs=15, batch_size=256, shuffle=True)",DL,Tensorflow,fatihozturk/models-stacking-3rd-place-solution,https://www.kaggle.com/fatihozturk/models-stacking-3rd-place-solution
tabular-playground-series-jan-2021,regression,continuous-regression,The task is to predict a continuous target variable based on 14 continuous feature columns.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set and a test set, where the training set includes a target column and 14 continuous feature columns named cont1 to cont14. The test set is used to predict the target for each row. The training set has a size of 144.43 MB and is in CSV format, with no missing values in the feature columns.",['target'],"[""drop outliers from target"", ""create cross-validation folds"", ""standardize continuous features""]","inputs = Input(shape=(14,))
x = Dense(256, activation='elu')(inputs)
x = Dropout(0.5)(x)
x = Dense(256, activation='elu')(x)
outputs = Dense(1, activation='linear')(x)
model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32)",DL,Tensorflow,gunesevitan/tabular-playground-series-jan-2021-models,https://www.kaggle.com/gunesevitan/tabular-playground-series-jan-2021-models
tabular-playground-series-jan-2021,regression,continuous-regression,The task is to predict a continuous target variable based on multiple continuous feature columns.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of training and test data in CSV format. The training data includes a target column and 14 continuous feature columns labeled cont1 to cont14. The test set is used to predict the target for each row. The total size of the dataset is 144.43 MB, and it is licensed under Attribution 4.0 International (CC BY 4.0).",['target'],"[""round continuous features to lower decimals"", ""create categorical features by binning continuous features"", ""apply label encoding to categorical features""]","inputs = []
flatten_layers = []
for e, c in enumerate(cont_features):
    input_c = Input(shape=(1, ), dtype='int32')
    num_c = max_cat_values[e]
    embed_c = Embedding(
        num_c,
        3,
    )(input_c)
    embed_c = Dropout(0.6)(embed_c)
    flatten_c = Flatten()(embed_c)
    inputs.append(input_c)
    flatten_layers.append(flatten_c)

input_num = Input(shape=(14,), dtype='float32')
inputs.append(input_num)
flatten_layers.append(input_num)
flatten = Concatenate()(flatten_layers)
x = Dense(1024, activation='relu',bias_initializer=""normal"",kernel_regularizer=regularizers.l2(0.0001))(flatten)
outputs = Dense(1, activation='linear')(x)
model = Model(inputs=inputs, outputs=outputs)
opt = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss=""mse"", optimizer=opt)
model.fit(X_train_list, y_train,validation_data=(X_val_list,y_val), callbacks=[lr_callback,estop], epochs=10000, batch_size=1024, verbose=1)",DL,Tensorflow,fatihozturk/nn-with-embedding-part-of-3rd-place-solution,https://www.kaggle.com/fatihozturk/nn-with-embedding-part-of-3rd-place-solution
tabular-playground-series-jan-2021,regression,continuous-regression,The task is to predict a continuous target variable based on multiple continuous feature columns.,Tabular,RMSE – Root Mean Squared Error,The dataset consists of training and test data in CSV format. The training data includes a target column and 14 continuous feature columns labeled cont1 to cont14. The test set is provided for making predictions on the target variable. The dataset size is 144.43 MB and is licensed under Attribution 4.0 International (CC BY 4.0).,['target'],"[""standardize feature columns using StandardScaler""]","model = keras.Sequential([
        layers.Dense(4096, activation='relu'),
        layers.Reshape((256, 16)),
        layers.Conv1D(filters=16, kernel_size=5, strides=1, activation='relu'),
        layers.MaxPooling1D(pool_size=2),
        layers.Flatten(),
        layers.Dense(16, activation='relu'),
        layers.Dense(1, activation='linear'),
    ])

model.compile(
        optimizer='adam',
        loss='mse',
        metrics=[keras.metrics.RootMeanSquaredError()]
    )

model.fit(
        X_tr, y_tr,
        validation_data=(X_val, y_val),
        batch_size=30000,
        epochs=1000,
        callbacks=[early_stopping],
    )",DL,Tensorflow,sishihara/1dcnn-for-tabular-from-moa-2nd-place,https://www.kaggle.com/sishihara/1dcnn-for-tabular-from-moa-2nd-place
tabular-playground-series-jan-2021,regression,continuous-regression,The task is to predict a continuous target variable based on multiple continuous feature columns.,Tabular,RMSE – Root Mean Squared Error,The dataset consists of training and test data in CSV format. The training data includes a target column and 14 continuous feature columns. The test set is used to predict the target for each row. The dataset size is 144.43 MB and is licensed under Attribution 4.0 International (CC BY 4.0).,['target'],"[""drop the target and id columns from the training data"", ""reshape the target variable to a 2D array""]","regressor = TabNetRegressor(**final_params)
regressor.fit(X_train=X, y_train=y,
          patience=TabNet_params['patience'], max_epochs=epochs,
          eval_metric=['rmse'])",DL,Pytorch,neilgibbons/tuning-tabnet-with-optuna,https://www.kaggle.com/neilgibbons/tuning-tabnet-with-optuna
tabular-playground-series-jan-2021,regression,continuous-regression,The task is to predict a continuous target variable based on multiple continuous feature columns.,Tabular,RMSE – Root Mean Squared Error,The dataset consists of training and test data in CSV format. The training data includes a target column and 14 continuous feature columns. The test set is used to predict the target for each row. The dataset size is 144.43 MB and is licensed under Attribution 4.0 International (CC BY 4.0).,['target'],"[""log1p transform the target variable""]","regressor = TabNetRegressor(verbose=1,seed=42)
regressor.fit(X_train=X_train, y_train=y_train,
              eval_set=[(X_valid, y_valid)],
              patience=1, max_epochs=2,
              eval_metric=['rmse'])",DL,Pytorch,essefiahlem/get-started-jan-tabular-playground-competition,https://www.kaggle.com/essefiahlem/get-started-jan-tabular-playground-competition
tabular-playground-series-jan-2021,regression,continuous-regression,The task is to predict a continuous target variable based on multiple continuous feature columns.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training file containing feature columns named cont1 to cont14 and a target column, as well as a test file where predictions need to be made. The training data is provided in train.csv and the test data in test.csv, with a sample submission format in sample_submission.csv.",['target'],"[""standardize feature values using MinMaxScaler""]","model = TPSnn(input_size, hidden_size1=hidden_size1, hidden_size2=hidden_size2, hidden_size3=hidden_size3, hidden_size4=hidden_size4, num_classes=num_classes)
optimizer = torch.optim.SGD(model.parameters(), lr)
history = fit(12, 0.02, model, train_loader, valid_loader)",DL,Pytorch,mdhamani/tps-getting-better-eda-pytorch-neural-net,https://www.kaggle.com/mdhamani/tps-getting-better-eda-pytorch-neural-net
tabular-playground-series-jan-2021,regression,continuous-regression,The task is to predict a continuous target variable based on multiple continuous feature columns.,Tabular,RMSE – Root Mean Squared Error,The dataset consists of training and test data in CSV format. The training data includes a target column and 14 continuous feature columns. The test data is used to predict the target for each row. The dataset size is 144.43 MB and is licensed under Attribution 4.0 International (CC BY 4.0).,['target'],"[""remove id column from training data"", ""remove id column from test data"", ""apply log transformation to target variable"", ""convert feature columns to numpy arrays""]","regressor = TabNetRegressor(verbose=1,seed=42)
regressor.fit(X_train=X_train, y_train=y_train,
              eval_set=[(X_valid, y_valid)],
              patience=1, max_epochs=2,
              eval_metric=['rmse'])",DL,Pytorch,elvinagammed/tabnet-regression-baseline,https://www.kaggle.com/elvinagammed/tabnet-regression-baseline
tabular-playground-series-jan-2021,regression,continuous-regression,The task is to predict a continuous target variable based on multiple continuous feature columns.,Tabular,RMSE – Root Mean Squared Error,The dataset consists of training and test data in CSV format. The training data includes a target column and 14 continuous feature columns. The test data is used to predict the target for each row. The dataset size is 144.43 MB and is licensed under Attribution 4.0 International (CC BY 4.0).,['target'],"[""remove rows with zero target values""]","regressor = TabNetRegressor(n_d=16, n_a=16, n_steps=4, n_independent=2, n_shared=2, lambda_sparse=0, optimizer_params=dict(lr=1e-2, weight_decay=1e-5), mask_type='entmax', scheduler_params=dict(mode='min', patience=5, min_lr=1e-4, factor=0.8), scheduler_fn=ReduceLROnPlateau, verbose=1, seed=rnd_seed_reg)
regressor.fit(X_train=X_train, y_train=y_train, eval_set=[(X_valid, y_valid)], max_epochs=100, patience=15, batch_size=1024, eval_metric=['rmse'])",DL,Pytorch,docxian/tabular-playground-1-visual-eda-tabnet,https://www.kaggle.com/docxian/tabular-playground-1-visual-eda-tabnet
cat-in-the-dat,classification,binary-classification,The task is to predict the probability of a binary target variable based on various features.,Tabular,AUCROC,"The dataset consists of binary, nominal, and ordinal features, along with cyclical day and month features. It contains no missing values and the test set does not include unseen feature values. The training set is provided in train.csv, while test.csv is used for making predictions. A sample submission file is also included to demonstrate the required format.",['target'],"[""label-encode categorical features""]","inputs = []
outputs = []
for c in catcols:
    num_unique_values = int(data[c].nunique())
    embed_dim = int(min(np.ceil((num_unique_values)/2), 50))
    inp = layers.Input(shape=(1,))
    out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)
    out = layers.SpatialDropout1D(0.3)(out)
    out = layers.Reshape(target_shape=(embed_dim, ))(out)
    inputs.append(inp)
    outputs.append(out)

x = layers.Concatenate()(outputs)
x = layers.BatchNormalization()(x)
x = layers.Dense(300, activation=""relu"")(x)
x = layers.Dropout(0.3)(x)
x = layers.BatchNormalization()(x)
x = layers.Dense(300, activation=""relu"")(x)
x = layers.Dropout(0.3)(x)
x = layers.BatchNormalization()(x)
y = layers.Dense(2, activation=""softmax"")(x)
model = Model(inputs=inputs, outputs=y)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])
model.fit(X_train, utils.to_categorical(y_train), validation_data=(X_test, utils.to_categorical(y_test)), verbose=1, batch_size=1024, callbacks=[es, rlr], epochs=100)",DL,Tensorflow,abhishek/entity-embeddings-to-handle-categories,https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories
cat-in-the-dat,classification,binary-classification,The task is to predict the probability of a binary target variable based on various features.,Tabular,AUCROC,"The dataset consists of binary, nominal, and ordinal features, along with cyclical day and month features. It contains no missing values and the test set does not include unseen feature values. The training set is provided in train.csv, while test.csv is used for making predictions. A sample submission file is available in sample_submission.csv.",['target'],"[""label-encode categorical features"", ""concatenate train and test data""]","inputs = []
outputs = []
for c in catcols:
    num_unique_values = int(data[c].nunique())
    embed_dim = int(min(np.ceil((num_unique_values)/2), 50))
    inp = layers.Input(shape=(1,))
    out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)
    out = layers.SpatialDropout1D(0.3)(out)
    out = layers.Reshape(target_shape=(embed_dim, ))(out)
    inputs.append(inp)
    outputs.append(out)

x = layers.Concatenate()(outputs)
x = layers.BatchNormalization()(x)
x = layers.Dense(300)(x)
x = Activation('Mish', name=""act1"")(x)
x = layers.Dropout(0.3)(x)
x = layers.BatchNormalization()(x)
x = layers.Dense(300)(x)
x = Activation('Mish', name=""act2"")(x)
x = layers.Dropout(0.3)(x)
x = layers.BatchNormalization()(x)
y = layers.Dense(2, activation=""softmax"")(x)
model = Model(inputs=inputs, outputs=y)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])
model.fit(X_train, utils.to_categorical(y_train), validation_data=(X_test, utils.to_categorical(y_test)), verbose=1, batch_size=1024, callbacks=[es, rlr], epochs=100)",DL,Tensorflow,abhishek/entity-embeddings-to-handle-categories-using-mish,https://www.kaggle.com/abhishek/entity-embeddings-to-handle-categories-using-mish
cat-in-the-dat,classification,binary-classification,"The task is to predict the probability of a binary target variable based on various features, including binary, nominal, and ordinal types, as well as cyclical day and month features.",Tabular,AUCROC,"The dataset consists of a training set and a test set, with features that include binary, nominal, and ordinal types. There are no missing values, and the test set does not contain unseen feature values. The training set includes a target column that indicates the binary outcome to be predicted. The dataset is provided in CSV format and is approximately 67.96 MB in size.",['target'],"[""label-encode categorical features""]","inputs = []
outputs = []
for c in catcols:
    num_unique_values = int(data[c].nunique())
    inp = layers.Input(shape=(1,))
    out = layers.Embedding(num_unique_values + 1, config.EMBEDDING_DIM, name=c)(inp)
    out = layers.SpatialDropout1D(config.SPATIAL_DROPOUT)(out)
    out = layers.Reshape(target_shape=(config.EMBEDDING_DIM, ))(out)
    inputs.append(inp)
    outputs.append(out)

x = layers.Concatenate()(outputs)
x = layers.BatchNormalization()(x)

for n_cells, act_fn, dropout in config.HIDDEN_LAYERS:
    x = layers.Dense(n_cells, activation=act_fn)(x)
    x = layers.Dropout(dropout)(x)
    x = layers.BatchNormalization()(x)

y = layers.Dense(config.OUTPUT_CELLS, activation=config.OUTPUT_ACTIVATION)(x)

model = Model(inputs=inputs, outputs=y)
model.compile(loss=config.LOSS, optimizer=config.OPT, metrics=config.METRICS)
history =  model.fit(X, y, validation_split=0.1, batch_size=config.BATCH_SIZE, callbacks=config.CALL_BACKS, epochs=config.MAX_EPOCHS)",DL,Tensorflow,ritvik1909/entity-embeddings,https://www.kaggle.com/ritvik1909/entity-embeddings
cat-in-the-dat,classification,binary-classification,"The task is to predict the probability of a binary target variable based on various features including binary, nominal, and ordinal types.",Tabular,AUCROC,"The dataset consists of a training set and a test set, with features that include binary, nominal, and ordinal types, as well as cyclical day and month features. The training set does not contain missing values, and the test set includes only known feature values. The data is structured in CSV format, with a total size of 67.96 MB.",['target'],"[""drop the 'id' column"", ""replace nominal values with integers"", ""apply label encoding to object type columns"", ""log-transform specific columns"", ""scale features using MinMaxScaler""]","net = ResMLP(initial_filters=32, block_list=[2, 2, 2], num_classes=2)
net.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
            metrics=['sparse_categorical_accuracy'])
history = net.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), callbacks=[early_stopping])",DL,Tensorflow,jiaowoguanren/categorical-feature-encoding-challenge-tf-resmlp,https://www.kaggle.com/jiaowoguanren/categorical-feature-encoding-challenge-tf-resmlp
cat-in-the-dat,classification,binary-classification,The task is to predict the probability of a binary target variable based on various features.,Tabular,AUCROC,"The dataset consists of a training set and a test set, containing binary, nominal, and ordinal features, as well as cyclical day and month features. There are no missing values, and the test set does not contain unseen feature values. The training set includes a target column that indicates the binary outcome to be predicted.",['target'],"[""label-encode categorical features""]","inputs = []
outputs = []
for c in catcols:
    num_unique_values = int(data[c].nunique())
    embed_dim = int(min(np.ceil((num_unique_values)/2), 50))
    inp = layers.Input(shape=(1,))
    out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)
    out = layers.SpatialDropout1D(0.3)(out)
    out = layers.Reshape(target_shape=(embed_dim, ))(out)
    inputs.append(inp)
    outputs.append(out)

x = layers.Concatenate()(outputs)
x = layers.BatchNormalization()(x)
x = layers.Dense(300, activation=""relu"")(x)
x = layers.Dropout(0.3)(x)
x = layers.BatchNormalization()(x)
x = layers.Dense(300, activation=""relu"")(x)
x = layers.Dropout(0.3)(x)
x = layers.BatchNormalization()(x)
y = layers.Dense(2, activation=""softmax"")(x)
model = Model(inputs=inputs, outputs=y)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])
model.fit(X_train, utils.to_categorical(y_train), validation_data=(X_test, utils.to_categorical(y_test)), verbose=1, batch_size=1024, callbacks=[es, rlr], epochs=100)",DL,Tensorflow,jagannathrk/categorical-feature-entity-embeddings,https://www.kaggle.com/jagannathrk/categorical-feature-entity-embeddings
cat-in-the-dat,classification,binary-classification,The task is to predict the probability of a binary target variable based on various features.,Tabular,AUCROC,"The dataset consists of a training set and a test set, containing binary, nominal, and ordinal features, as well as cyclical day and month features. There are no missing values, and the test set does not contain unseen feature values. The training set includes a target column that indicates the binary outcome to be predicted.",['target'],"[""label encode categorical features""]","self.linear_1 = nn.Linear(no_of_embed, 300)
self.relu = nn.ReLU()
self.dropout = nn.Dropout(0.3)
self.linear_2 = nn.Linear(300, 1)
self.sigmoid = nn.Sigmoid()

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
loss_fn = nn.BCELoss()
model.train()   
train_fn(model, train_dataloader, epochs=20)",DL,Pytorch,adarsh415/entity-embedding-pytorch,https://www.kaggle.com/adarsh415/entity-embedding-pytorch
dont-overfit-ii,classification,binary-classification,The task is to predict a binary target variable associated with each row in the dataset based on provided features.,Tabular,AUCROC,"The dataset consists of a training set with 250 rows and a test set with 19,750 rows. Each row contains an id and a binary target variable, along with 300 continuous variables. The training set is used to train the model, while the test set is used for making predictions. There are no missing values or duplicates in either dataset.",['target'],"[""scale features using MinMaxScaler"", ""split data into training and validation sets""]","model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), tf.keras.layers.Dense(7, activation=tf.nn.relu), tf.keras.layers.Dense(2, activation=tf.nn.softmax)])
model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])
model.fit(X, Y, epochs=10)",DL,Tensorflow,sarahesham/don-t-overfit,https://www.kaggle.com/sarahesham/don-t-overfit
dont-overfit-ii,classification,binary-classification,"The task is to predict a binary target variable associated with each row in the dataset, based on the provided features.",Tabular,AUCROC,"The dataset consists of a training set with 250 rows and a test set with 19,750 rows. The training set includes an 'id' column for sample identification and a 'target' column which is a binary variable. The features are continuous variables ranging from 0 to 299. The dataset is in CSV format and has a total size of 38.6 MB.",['target'],"[""robustly scale features"", ""add noise to training data""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(x_train0.shape[1],)))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])
model.fit(x_train0, y_cat_train0, epochs=50, batch_size=32, validation_split=0.2)",DL,Tensorflow,wordroid/get-best-features-and-paramaters-at-the-same-time,https://www.kaggle.com/wordroid/get-best-features-and-paramaters-at-the-same-time
dont-overfit-ii,classification,binary-classification,The task is to predict a binary target variable associated with each row in the dataset.,Tabular,AUCROC,"The dataset consists of a training set with 250 rows and a test set with 19,750 rows. The training set includes an 'id' column for sample identification and a 'target' column which is a binary variable. The features are continuous variables ranging from 0 to 299. The dataset is provided in CSV format and is subject to competition rules.",['target'],"[""select relevant features based on correlation with target"", ""process training and test datasets to retain selected features""]","model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), tf.keras.layers.Dense(7, activation=tf.nn.relu), tf.keras.layers.Dense(2, activation=tf.nn.softmax)])
model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])
model.fit(dftrain_processed, y_train, epochs=10)",DL,Tensorflow,adamabidi/eda-not-overfit,https://www.kaggle.com/adamabidi/eda-not-overfit
dont-overfit-ii,classification,binary-classification,The task is to predict a binary target variable associated with each row in the dataset.,Tabular,AUCROC,"The dataset consists of a training set with 250 rows and a test set with 19,750 rows. The training set includes an 'id' column for sample identification and a 'target' column which is a binary variable. The features are continuous variables, totaling 300 in number. The dataset is provided in CSV format and is 38.6 MB in size.",['target'],"[""check for missing values"", ""split data into features and target"", ""split data into training and testing sets""]","input_layer = tf.keras.Input(shape=(300,))
hidden_layer_1 = tf.keras.layers.Dense(1024, activation='relu')(input_layer)
hidden_layer_2 = tf.keras.layers.Dense(512, activation='relu')(hidden_layer_1)
hidden_layer_3 = tf.keras.layers.Dense(256, activation='relu')(hidden_layer_2)
output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_layer_3)
model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.25)",DL,Tensorflow,gokhangerdan/how-to-avoid-overfitting-for-beginners-keras,https://www.kaggle.com/gokhangerdan/how-to-avoid-overfitting-for-beginners-keras
dont-overfit-ii,classification,binary-classification,The task is to predict a binary target variable associated with each row in the dataset.,Tabular,AUCROC,"The dataset consists of a training set with 250 rows and a test set with 19,750 rows. The training set includes an 'id' column for sample identification and a 'target' column which is a binary variable. The features are continuous variables ranging from 0 to 299. The dataset is provided in CSV format and is subject to competition rules.",['target'],"[""normalize data using MinMaxScaler""]","kmodel = kr.models.Sequential()
kmodel.add(kr.layers.Dense(32, input_dim=np.size(X_train,1), activation='relu'))
kmodel.add(kr.layers.Dense(32, activation='relu'))
kmodel.add(kr.layers.Dense(16, activation='relu'))
kmodel.add(kr.layers.Dense(1, activation='sigmoid'))
kmodel.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')
history = kmodel.fit(X_train,Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT, verbose=LOGS, shuffle=True, class_weight=None, sample_weight=None)",DL,Tensorflow,sheik0/logistic-regression,https://www.kaggle.com/sheik0/logistic-regression
dont-overfit-ii,classification,binary-classification,The task is to predict a binary target variable associated with each row in the dataset based on provided features.,Tabular,AUCROC,"The dataset consists of a training set with 250 rows and a test set with 19,750 rows. The training set includes an 'id' column for sample identification and a 'target' column which is a binary variable. The features are continuous variables indexed from 0 to 299.",['target'],"[""normalize the data using MinMaxScaler""]","underfit_net = UnderfitNet(100)
loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(underfit_net.parameters(), lr=1.0e-3)
for epoch in range(1000):
    order = np.random.permutation(len(X_train))
    for start_index in range(0, len(X_train), batch_size):
        optimizer.zero_grad()
        batch_indexes = order[start_index:start_index+batch_size]
        x_batch = X_train[batch_indexes]
        y_batch = y_train[batch_indexes]
        preds = underfit_net.forward(x_batch)
        loss_value = loss(preds, y_batch)
        loss_value.backward()
        optimizer.step()",DL,Pytorch,georgyzubkov/dont-overfit-top-10-cor-params-and-pytorch,https://www.kaggle.com/georgyzubkov/dont-overfit-top-10-cor-params-and-pytorch
dont-overfit-ii,classification,binary-classification,The task is to predict a binary target variable associated with each row in the dataset.,Tabular,AUCROC,"The dataset consists of a training set with 250 rows and a test set with 19,750 rows. The training set includes an id column and a binary target column, while the test set contains only the id column. The features are continuous variables, and the dataset is provided in CSV format.",['target'],"[""scale features using RobustScaler""]","self.conv1= nn.Conv2d(1,8,5,padding=2)
self.conv2 = nn.Conv2d(8,16,(3,3),padding = 1)
self.fc1 = nn.Linear(16*17*17 ,500)
self.fc2 = nn.Linear(500,1)
criterion=nn.BCEWithLogitsLoss()
optimizer = optim.SGD(model1.parameters(), lr=0.003)
model1=model1.float()
model1(image.float())
sub['target'] = y_pred_list",DL,Pytorch,mrinath/pytorch-cnn,https://www.kaggle.com/mrinath/pytorch-cnn
dont-overfit-ii,classification,binary-classification,"The task is to predict a binary target variable associated with each row in the dataset, based on the provided features.",Tabular,AUCROC,"The dataset consists of a training set with 250 rows and a test set with 19,750 rows. The training set includes an 'id' column for sample identification and a 'target' column which is a binary variable. The features are continuous variables, and the dataset is in CSV format with a total size of 38.6 MB.",['target'],"[""seed the random number generator for reproducibility"", ""load the training data from CSV"", ""drop the 'id' and 'target' columns from the training data to create feature set"", ""load the test data from CSV"", ""drop the 'id' column from the test data to create feature set""]","model = OverfitModel(activation,size)
optimizer = eval(o)(model.parameters(),lr=lr,weight_decay=decay)
model.train()
for epoch in range(20):
    y_pred,loss = train(model, train_loader,optimizer,loss_function)
    model.load_state_dict(torch.load('checkpoint.pt'))
    y_pred,_ = eval_on_set(model,train_loader, loss_function)",DL,Pytorch,xsakix/third-overfit,https://www.kaggle.com/xsakix/third-overfit
dont-overfit-ii,classification,binary-classification,"The task is to predict a binary target variable associated with each row in the dataset, based on the provided features.",Tabular,AUCROC,"The dataset consists of a training set with 250 rows and a test set with 19,750 rows. The training set includes an 'id' column for sample identification and a 'target' column which is a binary variable. The features are continuous variables ranging from 0 to 299. The dataset is in CSV format and has a total size of 38.6 MB.",['target'],"[""drop the 'id' column from the training and test datasets"", ""convert the training and test datasets to numpy arrays""]","model = OverfitModel().cuda()
loss_function = nn.BCEWithLogitsLoss().cuda()
optimizer = optim.Adam(model.parameters(),lr=1e-3,weight_decay=1e-5)
early_stop = EarlyStopping(patience=5)
for epoch in range(20):
    y_pred,loss = train(model, train_loader,optimizer,loss_function)
    y_pred, val_loss = eval_on_set(model,val_loader, loss_function)
    print('EPOCH: ',epoch,': loss :',loss,': val loss: ',val_loss)",DL,Pytorch,xsakix/first-overfit,https://www.kaggle.com/xsakix/first-overfit
tmdb-box-office-prediction,regression,continuous-regression,The task is to predict the international box office revenue for each movie based on various metadata features.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains information on 7398 movies, including metadata from The Movie Database (TMDB). Each movie is identified by an id, and the dataset includes details such as cast, crew, plot keywords, budget, release dates, languages, production companies, and countries. The goal is to predict the worldwide revenue for 4398 movies in the test set.",['revenue'],"[""convert string columns to dictionary"", ""drop unnecessary columns"", ""fill missing values with mean"", ""apply label encoding to categorical features"", ""create new features based on text length""]","model = Sequential()
model.add(Dense(198, input_shape = (198, ), kernel_initializer = 'normal', activation='elu'))
model.add(Normalization())
model.add(Dense(396, kernel_initializer = 'normal', activation='elu'))
model.add(Normalization())
model.add(Dense(1028, kernel_initializer = 'normal', activation='elu'))
model.add(Dense(396, kernel_initializer = 'normal', activation='elu'))
model.add(Dense(1, kernel_initializer='normal'))

model.compile(loss='mean_absolute_error', optimizer='adam')

history = model.fit(X, Y, batch_size=20, epochs = 40, validation_split=0.1)",DL,Tensorflow,dhainiksuthar/movie-box-office-prediction,https://www.kaggle.com/dhainiksuthar/movie-box-office-prediction
tmdb-box-office-prediction,regression,continuous-regression,The task is to predict the international box office revenue for each movie based on various metadata features.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains information on 7398 movies, including metadata from The Movie Database (TMDB). Each movie is identified by an id, and the dataset includes features such as cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. The goal is to predict the worldwide revenue for 4398 movies in the test set.",['revenue'],"[""load data from CSV files"", ""create boolean feature for collection membership"", ""concatenate train and test datasets"", ""generate TF-IDF features for cast and crew"", ""generate TF-IDF features for production companies"", ""generate TF-IDF features for production countries"", ""generate TF-IDF features for genres"", ""combine various text features into a single feature"", ""extract date features from release dates"", ""fill missing values with median for numerical features"", ""scale features using PowerTransformer""]","model = keras.Sequential([layers.Dense(200, activation=tf.nn.leaky_relu, kernel_initializer='normal', input_shape=[len(train_dataset.keys())]),layers.Dropout(.8),layers.Dense(100, activation=tf.nn.leaky_relu, kernel_initializer='normal'),layers.Dropout(.6),layers.Dense(50, activation=tf.nn.leaky_relu, kernel_initializer='normal'),layers.Dropout(.4),layers.Dense(20, activation=tf.nn.leaky_relu, kernel_initializer='normal'),layers.Dropout(.2),layers.Dense(1, activation='linear', kernel_initializer='normal')]);optimizer = tf.keras.optimizers.RMSprop(0.0001);model.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['mean_absolute_error', 'mean_squared_error']);history = model.fit(normed_train_data, train_labels,batch_size = 100,epochs=EPOCHS, validation_split = 0.1, verbose=1, callbacks=[PrintDot()])",DL,Tensorflow,hieugaboy199x/tmdb-khaipha,https://www.kaggle.com/hieugaboy199x/tmdb-khaipha
tmdb-box-office-prediction,regression,continuous-regression,The task is to predict the international box office revenue for each movie based on various metadata features.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains information on 7398 movies, including metadata from The Movie Database (TMDB). Each movie is identified by an id, and the dataset includes features such as cast, crew, plot keywords, budget, release dates, languages, production companies, and countries. The goal is to predict the worldwide revenue for 4398 movies in the test set.",['revenue'],"[""median-impute missing values"", ""convert date strings to datetime objects"", ""split date into year, month, and day"", ""filter numerical columns"", ""filter categorical columns"", ""extract information from nested fields"", ""encode text fields using TF-IDF"", ""scale numerical features using MinMaxScaler""]","ann_model = keras.models.Sequential()
ann_model.add(keras.layers.Dense(5000 , activation=""relu"", input_shape=X_train_pp.shape[1:]))
ann_model.add(keras.layers.Dense(1000, activation=""relu""))
ann_model.add(keras.layers.Dense(2000, activation=""relu""))
ann_model.add(keras.layers.Dense(100, activation=""relu""))
ann_model.add(keras.layers.Dense(500, activation=""relu""))
ann_model.add(keras.layers.Dense(X_train_pp.shape[1], activation=""relu""))
ann_model.add(keras.layers.Dense(1))
ann_model.compile(loss=""mean_squared_logarithmic_error"", optimizer=keras.optimizers.SGD(lr=1e-1))
history = ann_model.fit(X_train_pp, y_train, epochs=100, batch_size=32, validation_split=0.2)",DL,Tensorflow,hoangdang89/ticket-box-prediction,https://www.kaggle.com/hoangdang89/ticket-box-prediction
tmdb-box-office-prediction,regression,continuous-regression,The task is to predict the international box office revenue for each movie based on various metadata features.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains 7398 movies with metadata from The Movie Database (TMDB), including cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. The goal is to predict the worldwide revenue for 4398 movies in the test set. Some movies may have the same title but are different films, and some may be remakes.",['revenue'],"[""fill missing budget values with median"", ""fill missing runtime values with median"", ""drop irrelevant columns"", ""encode categorical variables using label encoding"", ""log-transform skewed features""]","onelayer = keras.Sequential([\n        layers.BatchNormalization(),\n        layers.Dense(512, input_shape=[X_train.shape[1]], activation = 'relu'),\n        layers.Dense(1)\n    ])\n\nonelayer.compile(\n    optimizer = 'adam',\n    loss = 'mae',\n    metrics = ['mean_squared_logarithmic_error']\n)\nhistory = onelayer.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=256,\n    epochs=500,\n    callbacks=[early_stopping],\n    verbose = False\n)",DL,Tensorflow,alpayariyak/cs4342-final-project-team-8,https://www.kaggle.com/alpayariyak/cs4342-final-project-team-8
tmdb-box-office-prediction,regression,continuous-regression,Predict the international box office revenue for each movie based on various metadata.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains 7398 movies with metadata from The Movie Database (TMDB), including cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. The task is to predict the worldwide revenue for 4398 movies in the test set. Some movies may have similar titles or be remakes, but they should be treated as separate entities. The dataset is sourced from TMDB and includes various attributes relevant to movie revenue prediction.",['revenue'],"[""fill missing budget values with median"", ""fill missing runtime values with median"", ""encode categorical variables using label encoding"", ""drop irrelevant columns"", ""extract date features from release date""]","model_1 = keras.Sequential([layers.BatchNormalization(),layers.Dense(100, input_shape=[14]),layers.Dense(1, activation='relu')])
model_1.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])
history = model_1.fit(X_train, y_train,validation_data=(X_valid, y_valid),batch_size=256,epochs=100,callbacks=[early_stopping])",DL,Tensorflow,alpayariyak/tmdbteam8-2,https://www.kaggle.com/alpayariyak/tmdbteam8-2
tmdb-box-office-prediction,regression,continuous-regression,The task is to predict the international box office revenue for each movie based on various metadata features.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains information on 7398 movies, including metadata from The Movie Database (TMDB). Each movie is identified by an id, and the dataset includes features such as cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. The goal is to predict the worldwide revenue for 4398 movies in the test set.",['revenue'],"[""check for missing values"", ""replace 0 values in runtime with mean"", ""extract release year, month, and day from release date"", ""replace NaN in homepage with 0"", ""replace NaN in poster_path with 0"", ""replace 0 values in budget with average budget of the release year"", ""fill NaN in belongs_to_collection with 0"", ""fill NaN in genres with most common value"", ""fill NaN in title with original_title"", ""fill NaN in production_companies with 'none'"", ""fill NaN in spoken_languages with most common"", ""fill NaN in Keywords with 'none'"", ""fill NaN in cast with 'none'"", ""fill NaN in crew with 'none'"", ""fill NaN in overview with ''"", ""fill NaN in tagline with ''"", ""fill NaN in production_countries with most common"", ""fill NaN in status with 'Released'"", ""convert string lists to actual lists""]","class Classifier(nn.Module):
    def __init__(self, in_classes, dropout=0.5):
        super().__init__()
        self.input_dim = in_classes
        self.hidden_1 = int(self.input_dim)
        self.fc1 = nn.Linear(self.input_dim, int(self.hidden_1))
        self.fc2 = nn.Linear(int(self.hidden_1), int(self.hidden_1/2))
        self.fc3 = nn.Linear(int(self.hidden_1/2), 1)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x):
        x = x.view(x.shape[0], -1)
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.dropout(F.relu(self.fc2(x)))
        out = self.fc3(x)
        return out

ff_classifier = Classifier(in_classes=in_classes, dropout=dropout)

criterion = nn.MSELoss()
optimizer = torch.optim.SGD(ff_classifier.parameters(), lr = lr, momentum=0.5, nesterov=True)

for epoch in range(epochs):
    # Training loop
    # ...
    # Validation loop
    # ...

# Fit the model
train(ff_classifier, train_loader, valid_loader, epochs=epochs, lr=lr)",DL,Pytorch,wanderdust/lgbm-pytorch-catboost,https://www.kaggle.com/wanderdust/lgbm-pytorch-catboost
tmdb-box-office-prediction,regression,continuous-regression,The task is to predict the international box office revenue for each movie based on various metadata features.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains information on 7398 movies, including metadata from The Movie Database (TMDB). Each movie is identified by an id, and the dataset includes details such as cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. The goal is to predict the worldwide revenue for 4398 movies in the test set. Note that some movies may have the same title but are different films, and some may be remakes.",['revenue'],"[""clean runtime values"", ""transform JSON columns"", ""extract release date features"", ""one-hot encode genres"", ""calculate additional budget features"", ""scale numerical features""]","model = WithPosterEmbeddings(features_size = fs, dp=dropout).cuda()
dataloader_train = DataLoader(get_dataset(idx=train_idx), batch_size=bs, shuffle=True, num_workers=4)
dataloader_valid = DataLoader(get_dataset(idx=valid_idx), batch_size=bs, shuffle=True, num_workers=4)
criterion = nn.MSELoss()
opt = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
lr_sch = lr_scheduler.ReduceLROnPlateau(opt,'min', factor=0.1, patience=10, verbose=True)
for epoch_i in range(epochs):
    for sample_batch in tqdm(dataloader_train):
        images = sample_batch['images'].cuda()
        features = sample_batch['features'].cuda()
        targets = sample_batch['targets'].float().cuda()
        preds = model(images, features)
        loss = criterion(preds,targets.unsqueeze(1))
        opt.zero_grad()
        loss.backward()
        opt.step()",DL,Pytorch,ostamand/tmdb-with-posters-embeddings-cnn,https://www.kaggle.com/ostamand/tmdb-with-posters-embeddings-cnn
bike-sharing-demand,regression,continuous-regression,"The task is to predict the total count of bikes rented during each hour based on various features such as weather, temperature, and time.",Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains hourly rental data for bikes over two years. It includes features such as datetime, season, holiday status, working day status, weather conditions, temperature, humidity, wind speed, and counts of rentals by casual and registered users. The training set consists of the first 19 days of each month, while the test set includes the 20th to the end of the month.",['count'],"[""check for missing values"", ""visualize data distributions"", ""split datetime into year, month, day, and time"", ""map humidity and wind speed values to averages based on ranges""]","model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(units=32, activation='relu'))
model.add(tf.keras.layers.Dense(units=64, activation='relu'))
model.add(tf.keras.layers.Dense(units=128, activation='relu'))
model.add(tf.keras.layers.Dense(units=256, activation='relu'))
model.add(tf.keras.layers.Dense(units=64, activation='relu'))
model.add(tf.keras.layers.Dense(units=32, activation='relu'))
model.add(tf.keras.layers.Dense(units=16, activation='relu'))
model.add(tf.keras.layers.Dense(units=1,activation=""linear""))
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
history = model.fit(x_train, y_train, epochs=200, verbose = 0)",DL,Tensorflow,kagleo123/bike-sharing-eda-regression-model-predict,https://www.kaggle.com/kagleo123/bike-sharing-eda-regression-model-predict
bike-sharing-demand,regression,continuous-regression,"The task is to predict the total count of bikes rented during each hour based on historical rental data and various features such as weather, season, and time.",Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains hourly rental data for bikes over two years. It includes features such as datetime, season, holiday status, working day status, weather conditions, temperature, humidity, wind speed, and counts of casual and registered user rentals. The training set consists of the first 19 days of each month, while the test set includes the 20th to the end of the month. The target variable is the total count of bike rentals.",['count'],"[""one-hot encode categorical features"", ""standardize numeric features""]","model = Sequential()
model.add(Dense(100, input_dim=X_train_encoded.shape[1], activation='relu'))
model.add(Dense(1, activation=None))
model.compile(optimizer='adam', loss='mse')
history = model.fit(X_train_encoded, y_train, epochs=20, batch_size=32, validation_data=(X_validation_encoded, y_validation), callbacks=[early_stopping])",DL,Tensorflow,homayoonkhadivi/2-creative-deep-learning-models-bike-prediction,https://www.kaggle.com/homayoonkhadivi/2-creative-deep-learning-models-bike-prediction
bike-sharing-demand,regression,continuous-regression,"The task is to predict the total count of bikes rented during each hour based on historical rental data and various features such as weather, season, and time.",Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains hourly rental data for bikes over two years. It includes features such as datetime, season, holiday status, working day status, weather conditions, temperature, humidity, wind speed, and counts of casual and registered rentals. The training set consists of the first 19 days of each month, while the test set covers the 20th to the end of the month. The target variable is the total count of bike rentals.",['count'],"[""drop casual and registered columns"", ""extract year, month, day, and hour from datetime"", ""apply min-max normalization to continuous features"", ""shift previous label for time series forecasting"", ""adjust season and weather features to start from 0""]","inp = tf.keras.Input((None, train.shape[1]-1), name = 'Input')
cont = K.stack([inp[:, :, i] for i in cont_feat_tf], axis = 2)
cats_emb = []
for d_f in disc_feat_tf:
    cat_inp = inp[:, :, d_f]
    d_fm6 = d_f - 6
    cats_emb.append(Embedding(cat_nums[d_fm6], embedding_nums[d_fm6], name = ""Embedding_{0}"".format(disc_feat[d_fm6]))(cat_inp))
sub_embs = tf.concat([cont, K.stack(cats_emb, axis = 2)[:, :, :, 0]], axis = 2)
full_embs = Dropout(dropout_rates)(sub_embs)
LSTM_out = tf.keras.layers.LSTM(lstm_units, return_sequences=True, name = ""LSTM"")(full_embs)
Output_dense = Dense(1, activation='relu', name='Output', kernel_regularizer=l1(l1_penalty))
Output = TimeDistributed(Output_dense, name='Output_timedistributed')(LSTM_out)
lstm_model = tf.keras.Model(inputs=inp, outputs=Output, name='Basic_LSTM')
lstm_model.compile(optimizer='adam', loss=rmse)
epoch_num = 20
val_loss = []
for epoch in range(epoch_num):
    for train_batch in train_dat.as_numpy_iterator():
        tr_x = train_batch[:, :, :-1]
        tr_y = np.expand_dims(train_batch[:, :, -1], axis=-1)
        lstm_model.fit(tr_x, tr_y, epochs=1, verbose=0, shuffle=False)",DL,Tensorflow,juyoungwang/tensorflow-deepar-implementation,https://www.kaggle.com/juyoungwang/tensorflow-deepar-implementation
bike-sharing-demand,regression,continuous-regression,"The task is to predict the total count of bikes rented during each hour based on historical rental data and various features such as weather, season, and time.",Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains hourly rental data for bikes over two years. It includes features such as datetime, season, holiday status, working day status, weather conditions, temperature, humidity, wind speed, and counts of casual and registered rentals. The training set consists of the first 19 days of each month, while the test set covers the 20th to the end of the month. The target variable is the total count of bike rentals.",['count'],"[""one-hot encode season and weather features"", ""drop original season and weather columns"", ""extract hour, day, month, and year from datetime"", ""drop datetime column"", ""split dataset into training and testing sets""]","model = Sequential()
model.add(Dense(1024, kernel_regularizer = 'l2', activation = 'relu', input_shape=(1*18,)))
model.add(Dense(512, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1))

model.compile(optimizer='adam',
              loss='msle',
              metrics=['mae', 'msle'])

history = model.fit(x_train, y_train, epochs=150, validation_split=0.2, verbose=1, batch_size = 32)",DL,Tensorflow,michalwiese/neural-network-bikes,https://www.kaggle.com/michalwiese/neural-network-bikes
bike-sharing-demand,regression,continuous-regression,The task is to predict the total count of bike rentals per hour based on historical rental data and various environmental and temporal factors.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains hourly bike rental data spanning two years. The training set consists of the first 19 days of each month, while the test set includes the 20th to the end of each month. The dataset includes fields such as datetime, season, holiday, working day, weather conditions, temperature, humidity, wind speed, and counts of rentals from registered and casual users, with the target variable being the total count of bike rentals.",['count'],"[""convert datetime to datetime object"", ""extract date, hour, weekday, and month from datetime"", ""map season and weather to descriptive labels"", ""remove outliers based on count quantiles"", ""one-hot encode categorical variables""]","model = Sequential()
model.add(Dense(120, activation='relu'))
model.add(Dense(80, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(30, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='relu'))
model.compile(loss='mse', optimizer='adam')
history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, verbose=1)",DL,Tensorflow,senasudemir/bike-sharing-demand-prediction,https://www.kaggle.com/senasudemir/bike-sharing-demand-prediction
bike-sharing-demand,regression,multi-output-regression,Predict the total count of bikes rented during each hour using hourly rental data and various features.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains hourly rental data for a bike-sharing system over two years. It includes features such as datetime, season, holiday status, working day status, weather conditions, temperature, humidity, wind speed, and counts of casual and registered user rentals. The training set consists of the first 19 days of each month, while the test set includes the 20th to the end of the month. The target variable is the total count of bike rentals.","['casual', 'registered', 'count']","[""load dataset"", ""concatenate train and test data"", ""check for missing values"", ""extract year, month, day, hour from datetime"", ""drop unnecessary variables"", ""convert integer variables to categorical"", ""one-hot encode categorical variables"", ""scale continuous variables using MinMaxScaler"", ""split the dataset into training and test sets""]","class MLP(nn.Module):
    def __init__(self, num_inputs=X_train.shape[1], num_outputs=2, layer1=512, layer2=256, dropout1=0, dropout2=0):
        super(MLP, self).__init__()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(num_inputs, layer1),
            nn.LeakyReLU(),
            nn.Dropout(dropout1),
            nn.Linear(layer1, layer2),
            nn.LeakyReLU(),
            nn.Dropout(dropout2),
            nn.Linear(layer2, num_outputs)
            )
    def forward(self, x):
        x = self.linear_relu_stack(x)
        return x  
mlp = NeuralNetRegressor(
    MLP(num_inputs=X_train.shape[1], num_outputs=2),
    optimizer=torch.optim.Adam,
    criterion=nn.MSELoss,
    iterator_train__shuffle=True,
    device=try_gpu(),
    verbose=0,
    callbacks=[EpochScoring(target_metric, lower_is_better=False, on_train=False, name='valid_neg_rmsle_custom'),
               EarlyStopping(monitor='valid_neg_rmsle_custom', patience=5,
                             threshold=1e-3, lower_is_better=False),
               Checkpoint(monitor='valid_neg_rmsle_custom_best')]
                          )
tune_search.fit(X, y)",DL,Pytorch,jayahn0104/tutorial-multi-output-regression-skorch-tune,https://www.kaggle.com/jayahn0104/tutorial-multi-output-regression-skorch-tune
bike-sharing-demand,regression,continuous-regression,The task is to predict the total count of bikes rented during each hour based on historical rental data and various features such as weather and time.,Tabular,RMSLE – Root Mean Squared Logarithmic Error,"The dataset contains hourly rental data for bikes over two years. It includes features such as datetime, season, holiday status, working day status, weather conditions, temperature, humidity, wind speed, and the total count of rentals. The training set consists of the first 19 days of each month, while the test set covers the 20th to the end of the month.",['count'],"[""drop unnecessary columns"", ""shift target variable for prediction"", ""split training data into training and validation sets"", ""normalize temperature, humidity, and wind speed features""]","model = RegressionLSTM(len(features), num_hidden_dim, output_dim)
loss_function = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
model.to(device)
earlystopping = EarlyStopping(patience=5, verbose=True, path=""/kaggle/working/checkpoint_model.pth"")
for epoch in range(100000):
    model.train()
    for data in train_loader:
        X = data[0].to(device)
        y = data[1].to(device)
        a = model(X)
        loss = loss_function(a, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    model.eval()",DL,Pytorch,marimochi/usingpytorchlstm,https://www.kaggle.com/marimochi/usingpytorchlstm
reducing-commercial-aviation-fatalities,classification,multiclass-classification,"The goal is to predict the probability of each cognitive state (baseline, SS, CA, DA) for pilots based on physiological data during flight simulations.",Tabular,Multi-class Log Loss,"The dataset contains physiological data from eighteen pilots subjected to various distractions. The training set includes controlled experiments outside of a flight simulator, while the test set consists of a full flight simulation. The pilots experienced distractions inducing cognitive states: Channelized Attention (CA), Diverted Attention (DA), and Startle/Surprise (SS). Each experiment records a pair of pilots over time, with data collected at a sample rate of 256 Hz. The dataset includes EEG recordings, ECG signals, respiration data, and galvanic skin response measurements, with noise and artifacts present due to the physiological nature of the data.",['event'],"[""smooth eeg data using rolling mean"", ""clip eeg data to percentiles"", ""smooth ecg data using rolling mean"", ""smooth respiration data using rolling mean"", ""smooth galvanic skin response data using rolling mean"", ""scale all values between 0 and 1""]","model = tf.keras.Sequential()
model.add(layers.Dense(128, activation='relu', input_shape=(input_shape,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(4, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32)",DL,Tensorflow,theryan/competition-kernel-for-rcaf,https://www.kaggle.com/theryan/competition-kernel-for-rcaf
conways-reverse-game-of-life-2020,classification,binary-classification,The task is to predict the starting board configuration of a game based on the stopping board configuration after a specified number of steps.,Tabular,MAE – Mean Absolute Error,"The dataset consists of 50,000 training games and 50,000 test games, each represented as a 25x25 grid with 625 cells. Each game has a unique identifier and includes variables for the number of steps between the start and stop boards, as well as the state of each cell in both the starting and stopping boards. The starting board is recorded after five warmup steps to allow the cells to stabilize into a more life-like state. The stopping board is recorded after a random number of steps, delta, which ranges from 1 to 5. The dataset allows for the creation of additional training games if desired.","['start_0', 'start_1', 'start_2', 'start_3', 'start_4', 'start_5', 'start_6', 'start_7', 'start_8', 'start_9', 'start_10', 'start_11', 'start_12', 'start_13', 'start_14', 'start_15', 'start_16', 'start_17', 'start_18', 'start_19', 'start_20', 'start_21', 'start_22', 'start_23', 'start_24', 'start_25', 'start_26', 'start_27', 'start_28', 'start_29', 'start_30', 'start_31', 'start_32', 'start_33', 'start_34', 'start_35', 'start_36', 'start_37', 'start_38', 'start_39', 'start_40', 'start_41', 'start_42', 'start_43', 'start_44', 'start_45', 'start_46', 'start_47', 'start_48', 'start_49', 'start_50', 'start_51', 'start_52', 'start_53', 'start_54', 'start_55', 'start_56', 'start_57', 'start_58', 'start_59', 'start_60', 'start_61', 'start_62', 'start_63', 'start_64', 'start_65', 'start_66', 'start_67', 'start_68', 'start_69', 'start_70', 'start_71', 'start_72', 'start_73', 'start_74', 'start_75', 'start_76', 'start_77', 'start_78', 'start_79', 'start_80', 'start_81', 'start_82', 'start_83', 'start_84', 'start_85', 'start_86', 'start_87', 'start_88', 'start_89', 'start_90', 'start_91', 'start_92', 'start_93', 'start_94', 'start_95', 'start_96', 'start_97', 'start_98', 'start_99', 'start_100', 'start_101', 'start_102', 'start_103', 'start_104', 'start_105', 'start_106', 'start_107', 'start_108', 'start_109', 'start_110', 'start_111', 'start_112', 'start_113', 'start_114', 'start_115', 'start_116', 'start_117', 'start_118', 'start_119', 'start_120', 'start_121', 'start_122', 'start_123', 'start_124', 'start_125', 'start_126', 'start_127', 'start_128', 'start_129', 'start_130', 'start_131', 'start_132', 'start_133', 'start_134', 'start_135', 'start_136', 'start_137', 'start_138', 'start_139', 'start_140', 'start_141', 'start_142', 'start_143', 'start_144', 'start_145', 'start_146', 'start_147', 'start_148', 'start_149', 'start_150', 'start_151', 'start_152', 'start_153', 'start_154', 'start_155', 'start_156', 'start_157', 'start_158', 'start_159', 'start_160', 'start_161', 'start_162', 'start_163', 'start_164', 'start_165', 'start_166', 'start_167', 'start_168', 'start_169', 'start_170', 'start_171', 'start_172', 'start_173', 'start_174', 'start_175', 'start_176', 'start_177', 'start_178', 'start_179', 'start_180', 'start_181', 'start_182', 'start_183', 'start_184', 'start_185', 'start_186', 'start_187', 'start_188', 'start_189', 'start_190', 'start_191', 'start_192', 'start_193', 'start_194', 'start_195', 'start_196', 'start_197', 'start_198', 'start_199', 'start_200', 'start_201', 'start_202', 'start_203', 'start_204', 'start_205', 'start_206', 'start_207', 'start_208', 'start_209', 'start_210', 'start_211', 'start_212', 'start_213', 'start_214', 'start_215', 'start_216', 'start_217', 'start_218', 'start_219', 'start_220', 'start_221', 'start_222', 'start_223', 'start_224', 'start_225', 'start_226', 'start_227', 'start_228', 'start_229', 'start_230', 'start_231', 'start_232', 'start_233', 'start_234', 'start_235', 'start_236', 'start_237', 'start_238', 'start_239', 'start_240', 'start_241', 'start_242', 'start_243', 'start_244', 'start_245', 'start_246', 'start_247', 'start_248', 'start_249', 'start_250', 'start_251', 'start_252', 'start_253', 'start_254', 'start_255', 'start_256', 'start_257', 'start_258', 'start_259', 'start_260', 'start_261', 'start_262', 'start_263', 'start_264', 'start_265', 'start_266', 'start_267', 'start_268', 'start_269', 'start_270', 'start_271', 'start_272', 'start_273', 'start_274', 'start_275', 'start_276', 'start_277', 'start_278', 'start_279', 'start_280', 'start_281', 'start_282', 'start_283', 'start_284', 'start_285', 'start_286', 'start_287', 'start_288', 'start_289', 'start_290', 'start_291', 'start_292', 'start_293', 'start_294', 'start_295', 'start_296', 'start_297', 'start_298', 'start_299', 'start_300', 'start_301', 'start_302', 'start_303', 'start_304', 'start_305', 'start_306', 'start_307', 'start_308', 'start_309', 'start_310', 'start_311', 'start_312', 'start_313', 'start_314', 'start_315', 'start_316', 'start_317', 'start_318', 'start_319', 'start_320', 'start_321', 'start_322', 'start_323', 'start_324', 'start_325', 'start_326', 'start_327', 'start_328', 'start_329', 'start_330', 'start_331', 'start_332', 'start_333', 'start_334', 'start_335', 'start_336', 'start_337', 'start_338', 'start_339', 'start_340', 'start_341', 'start_342', 'start_343', 'start_344', 'start_345', 'start_346', 'start_347', 'start_348', 'start_349', 'start_350', 'start_351', 'start_352', 'start_353', 'start_354', 'start_355', 'start_356', 'start_357', 'start_358', 'start_359', 'start_360', 'start_361', 'start_362', 'start_363', 'start_364', 'start_365', 'start_366', 'start_367', 'start_368', 'start_369', 'start_370', 'start_371', 'start_372', 'start_373', 'start_374', 'start_375', 'start_376', 'start_377', 'start_378', 'start_379', 'start_380', 'start_381', 'start_382', 'start_383', 'start_384', 'start_385', 'start_386', 'start_387', 'start_388', 'start_389', 'start_390', 'start_391', 'start_392', 'start_393', 'start_394', 'start_395', 'start_396', 'start_397', 'start_398', 'start_399', 'start_400', 'start_401', 'start_402', 'start_403', 'start_404', 'start_405', 'start_406', 'start_407', 'start_408', 'start_409', 'start_410', 'start_411', 'start_412', 'start_413', 'start_414', 'start_415', 'start_416', 'start_417', 'start_418', 'start_419', 'start_420', 'start_421', 'start_422', 'start_423', 'start_424', 'start_425', 'start_426', 'start_427', 'start_428', 'start_429', 'start_430', 'start_431', 'start_432', 'start_433', 'start_434', 'start_435', 'start_436', 'start_437', 'start_438', 'start_439', 'start_440', 'start_441', 'start_442', 'start_443', 'start_444', 'start_445', 'start_446', 'start_447', 'start_448', 'start_449', 'start_450', 'start_451', 'start_452', 'start_453', 'start_454', 'start_455', 'start_456', 'start_457', 'start_458', 'start_459', 'start_460', 'start_461', 'start_462', 'start_463', 'start_464', 'start_465', 'start_466', 'start_467', 'start_468', 'start_469', 'start_470', 'start_471', 'start_472', 'start_473', 'start_474', 'start_475', 'start_476', 'start_477', 'start_478', 'start_479', 'start_480', 'start_481', 'start_482', 'start_483', 'start_484', 'start_485', 'start_486', 'start_487', 'start_488', 'start_489', 'start_490', 'start_491', 'start_492', 'start_493', 'start_494', 'start_495', 'start_496', 'start_497', 'start_498', 'start_499', 'start_500', 'start_501', 'start_502', 'start_503', 'start_504', 'start_505', 'start_506', 'start_507', 'start_508', 'start_509', 'start_510', 'start_511', 'start_512', 'start_513', 'start_514', 'start_515', 'start_516', 'start_517', 'start_518', 'start_519', 'start_520', 'start_521', 'start_522', 'start_523', 'start_524', 'start_525', 'start_526', 'start_527', 'start_528', 'start_529', 'start_530', 'start_531', 'start_532', 'start_533', 'start_534', 'start_535', 'start_536', 'start_537', 'start_538', 'start_539', 'start_540', 'start_541', 'start_542', 'start_543', 'start_544', 'start_545', 'start_546', 'start_547', 'start_548', 'start_549', 'start_550', 'start_551', 'start_552', 'start_553', 'start_554', 'start_555', 'start_556', 'start_557', 'start_558', 'start_559', 'start_560', 'start_561', 'start_562', 'start_563', 'start_564', 'start_565', 'start_566', 'start_567', 'start_568', 'start_569', 'start_570', 'start_571', 'start_572', 'start_573', 'start_574', 'start_575', 'start_576', 'start_577', 'start_578', 'start_579', 'start_580', 'start_581', 'start_582', 'start_583', 'start_584', 'start_585', 'start_586', 'start_587', 'start_588', 'start_589', 'start_590', 'start_591', 'start_592', 'start_593', 'start_594', 'start_595', 'start_596', 'start_597', 'start_598', 'start_599', 'start_600', 'start_601', 'start_602', 'start_603', 'start_604', 'start_605', 'start_606', 'start_607', 'start_608', 'start_609', 'start_610', 'start_611', 'start_612', 'start_613', 'start_614', 'start_615', 'start_616', 'start_617', 'start_618', 'start_619', 'start_620', 'start_621', 'start_622', 'start_623', 'start_624']","[""generate delta-neighbors states"", ""reshape input for model"", ""normalize input data""]","inp = L.Input(name=""grid"", shape=(625,nh))
d1 = L.Dense(nn, name='d1', activation=""relu"")(inp)
d2 = L.Dense(nn, name='d2', activation=""relu"")(d1)
preds = L.Dense(1, name='preds', activation=""sigmoid"")(d2)
model = M.Model(inp, preds, name=""ANN"")
model.compile(loss=""binary_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])
net.fit(x_tr, y_tr, batch_size=100, epochs=15)",DL,Tensorflow,ulrich07/quick-neighborhood-fe-mlp-keras,https://www.kaggle.com/ulrich07/quick-neighborhood-fe-mlp-keras
conways-reverse-game-of-life-2020,classification,binary-classification,The task is to predict the starting board configuration of a game based on the stopping board configuration after a specified number of steps.,Tabular,MAE – Mean Absolute Error,"The dataset consists of 50,000 training games and 50,000 test games, each represented by a 25x25 board with 625 cells. Each game has a unique identifier, a delta value indicating the number of steps between the start and stop boards, and features representing the state of the board at both the start and stop points. The starting board's state is recorded after five warmup steps to allow for stabilization, and the stopping board's state is recorded after a random number of steps, delta, which ranges from 1 to 5. The dataset allows for the creation of additional training games if desired.","['start_0', 'start_1', 'start_2', 'start_3', 'start_4', 'start_5', 'start_6', 'start_7', 'start_8', 'start_9', 'start_10', 'start_11', 'start_12', 'start_13', 'start_14', 'start_15', 'start_16', 'start_17', 'start_18', 'start_19', 'start_20', 'start_21', 'start_22', 'start_23', 'start_24', 'start_25', 'start_26', 'start_27', 'start_28', 'start_29', 'start_30', 'start_31', 'start_32', 'start_33', 'start_34', 'start_35', 'start_36', 'start_37', 'start_38', 'start_39', 'start_40', 'start_41', 'start_42', 'start_43', 'start_44', 'start_45', 'start_46', 'start_47', 'start_48', 'start_49', 'start_50', 'start_51', 'start_52', 'start_53', 'start_54', 'start_55', 'start_56', 'start_57', 'start_58', 'start_59', 'start_60', 'start_61', 'start_62', 'start_63', 'start_64', 'start_65', 'start_66', 'start_67', 'start_68', 'start_69', 'start_70', 'start_71', 'start_72', 'start_73', 'start_74', 'start_75', 'start_76', 'start_77', 'start_78', 'start_79', 'start_80', 'start_81', 'start_82', 'start_83', 'start_84', 'start_85', 'start_86', 'start_87', 'start_88', 'start_89', 'start_90', 'start_91', 'start_92', 'start_93', 'start_94', 'start_95', 'start_96', 'start_97', 'start_98', 'start_99', 'start_100', 'start_101', 'start_102', 'start_103', 'start_104', 'start_105', 'start_106', 'start_107', 'start_108', 'start_109', 'start_110', 'start_111', 'start_112', 'start_113', 'start_114', 'start_115', 'start_116', 'start_117', 'start_118', 'start_119', 'start_120', 'start_121', 'start_122', 'start_123', 'start_124', 'start_125', 'start_126', 'start_127', 'start_128', 'start_129', 'start_130', 'start_131', 'start_132', 'start_133', 'start_134', 'start_135', 'start_136', 'start_137', 'start_138', 'start_139', 'start_140', 'start_141', 'start_142', 'start_143', 'start_144', 'start_145', 'start_146', 'start_147', 'start_148', 'start_149', 'start_150', 'start_151', 'start_152', 'start_153', 'start_154', 'start_155', 'start_156', 'start_157', 'start_158', 'start_159', 'start_160', 'start_161', 'start_162', 'start_163', 'start_164', 'start_165', 'start_166', 'start_167', 'start_168', 'start_169', 'start_170', 'start_171', 'start_172', 'start_173', 'start_174', 'start_175', 'start_176', 'start_177', 'start_178', 'start_179', 'start_180', 'start_181', 'start_182', 'start_183', 'start_184', 'start_185', 'start_186', 'start_187', 'start_188', 'start_189', 'start_190', 'start_191', 'start_192', 'start_193', 'start_194', 'start_195', 'start_196', 'start_197', 'start_198', 'start_199', 'start_200', 'start_201', 'start_202', 'start_203', 'start_204', 'start_205', 'start_206', 'start_207', 'start_208', 'start_209', 'start_210', 'start_211', 'start_212', 'start_213', 'start_214', 'start_215', 'start_216', 'start_217', 'start_218', 'start_219', 'start_220', 'start_221', 'start_222', 'start_223', 'start_224', 'start_225', 'start_226', 'start_227', 'start_228', 'start_229', 'start_230', 'start_231', 'start_232', 'start_233', 'start_234', 'start_235', 'start_236', 'start_237', 'start_238', 'start_239', 'start_240', 'start_241', 'start_242', 'start_243', 'start_244', 'start_245', 'start_246', 'start_247', 'start_248', 'start_249', 'start_250', 'start_251', 'start_252', 'start_253', 'start_254', 'start_255', 'start_256', 'start_257', 'start_258', 'start_259', 'start_260', 'start_261', 'start_262', 'start_263', 'start_264', 'start_265', 'start_266', 'start_267', 'start_268', 'start_269', 'start_270', 'start_271', 'start_272', 'start_273', 'start_274', 'start_275', 'start_276', 'start_277', 'start_278', 'start_279', 'start_280', 'start_281', 'start_282', 'start_283', 'start_284', 'start_285', 'start_286', 'start_287', 'start_288', 'start_289', 'start_290', 'start_291', 'start_292', 'start_293', 'start_294', 'start_295', 'start_296', 'start_297', 'start_298', 'start_299', 'start_300', 'start_301', 'start_302', 'start_303', 'start_304', 'start_305', 'start_306', 'start_307', 'start_308', 'start_309', 'start_310', 'start_311', 'start_312', 'start_313', 'start_314', 'start_315', 'start_316', 'start_317', 'start_318', 'start_319', 'start_320', 'start_321', 'start_322', 'start_323', 'start_324', 'start_325', 'start_326', 'start_327', 'start_328', 'start_329', 'start_330', 'start_331', 'start_332', 'start_333', 'start_334', 'start_335', 'start_336', 'start_337', 'start_338', 'start_339', 'start_340', 'start_341', 'start_342', 'start_343', 'start_344', 'start_345', 'start_346', 'start_347', 'start_348', 'start_349', 'start_350', 'start_351', 'start_352', 'start_353', 'start_354', 'start_355', 'start_356', 'start_357', 'start_358', 'start_359', 'start_360', 'start_361', 'start_362', 'start_363', 'start_364', 'start_365', 'start_366', 'start_367', 'start_368', 'start_369', 'start_370', 'start_371', 'start_372', 'start_373', 'start_374', 'start_375', 'start_376', 'start_377', 'start_378', 'start_379', 'start_380', 'start_381', 'start_382', 'start_383', 'start_384', 'start_385', 'start_386', 'start_387', 'start_388', 'start_389', 'start_390', 'start_391', 'start_392', 'start_393', 'start_394', 'start_395', 'start_396', 'start_397', 'start_398', 'start_399', 'start_400', 'start_401', 'start_402', 'start_403', 'start_404', 'start_405', 'start_406', 'start_407', 'start_408', 'start_409', 'start_410', 'start_411', 'start_412', 'start_413', 'start_414', 'start_415', 'start_416', 'start_417', 'start_418', 'start_419', 'start_420', 'start_421', 'start_422', 'start_423', 'start_424', 'start_425', 'start_426', 'start_427', 'start_428', 'start_429', 'start_430', 'start_431', 'start_432', 'start_433', 'start_434', 'start_435', 'start_436', 'start_437', 'start_438', 'start_439', 'start_440', 'start_441', 'start_442', 'start_443', 'start_444', 'start_445', 'start_446', 'start_447', 'start_448', 'start_449', 'start_450', 'start_451', 'start_452', 'start_453', 'start_454', 'start_455', 'start_456', 'start_457', 'start_458', 'start_459', 'start_460', 'start_461', 'start_462', 'start_463', 'start_464', 'start_465', 'start_466', 'start_467', 'start_468', 'start_469', 'start_470', 'start_471', 'start_472', 'start_473', 'start_474', 'start_475', 'start_476', 'start_477', 'start_478', 'start_479', 'start_480', 'start_481', 'start_482', 'start_483', 'start_484', 'start_485', 'start_486', 'start_487', 'start_488', 'start_489', 'start_490', 'start_491', 'start_492', 'start_493', 'start_494', 'start_495', 'start_496', 'start_497', 'start_498', 'start_499', 'start_500', 'start_501', 'start_502', 'start_503', 'start_504', 'start_505', 'start_506', 'start_507', 'start_508', 'start_509', 'start_510', 'start_511', 'start_512', 'start_513', 'start_514', 'start_515', 'start_516', 'start_517', 'start_518', 'start_519', 'start_520', 'start_521', 'start_522', 'start_523', 'start_524', 'start_525', 'start_526', 'start_527', 'start_528', 'start_529', 'start_530', 'start_531', 'start_532', 'start_533', 'start_534', 'start_535', 'start_536', 'start_537', 'start_538', 'start_539', 'start_540', 'start_541', 'start_542', 'start_543', 'start_544', 'start_545', 'start_546', 'start_547', 'start_548', 'start_549', 'start_550', 'start_551', 'start_552', 'start_553', 'start_554', 'start_555', 'start_556', 'start_557', 'start_558', 'start_559', 'start_560', 'start_561', 'start_562', 'start_563', 'start_564', 'start_565', 'start_566', 'start_567', 'start_568', 'start_569', 'start_570', 'start_571', 'start_572', 'start_573', 'start_574', 'start_575', 'start_576', 'start_577', 'start_578', 'start_579', 'start_580', 'start_581', 'start_582', 'start_583', 'start_584', 'start_585', 'start_586', 'start_587', 'start_588', 'start_589', 'start_590', 'start_591', 'start_592', 'start_593', 'start_594', 'start_595', 'start_596', 'start_597', 'start_598', 'start_599', 'start_600', 'start_601', 'start_602', 'start_603', 'start_604', 'start_605', 'start_606', 'start_607', 'start_608', 'start_609', 'start_610', 'start_611', 'start_612', 'start_613', 'start_614', 'start_615', 'start_616', 'start_617', 'start_618', 'start_619', 'start_620', 'start_621', 'start_622', 'start_623']","[""reshape stop features to 25x25x1"", ""reshape start features to 25x25x1"", ""convert features to float32"", ""batch the dataset""]","self.encoder = L.Conv2D(32, kernel_size=(3,3), padding=""SAME"")
self.memory = tf.keras.models.Sequential([
    L.Conv2D(128, kernel_size=(3,3), padding=""SAME""),
    L.BatchNormalization(),
    L.ReLU(),
    L.Conv2D(32, kernel_size=(3,3), padding=""SAME""),
    L.BatchNormalization(),
    L.ReLU(),
])
self.decoder = L.Conv2D(1, kernel_size=(3,3), padding=""SAME"")
model.compile(loss=""bce"", optimizer=tf.keras.optimizers.Adam(), metrics=[""accuracy""])
model.fit(tr_df, epochs=50)",DL,Tensorflow,parmarsuraj99/a-neural-cnn-game-of-life-with-keras,https://www.kaggle.com/parmarsuraj99/a-neural-cnn-game-of-life-with-keras
conways-reverse-game-of-life-2020,classification,binary-classification,The task is to predict the starting board configuration of a game based on the stopping board configuration after a specified number of steps.,Tabular,MAE – Mean Absolute Error,"The dataset consists of 50,000 training games and 50,000 test games, each represented by a 25x25 board with 625 cells. Each game has a unique identifier and includes variables for the number of steps between the start and stop boards, as well as the state of the board at both the start and stop. The starting board is recorded after five warmup steps to allow for stabilization, and the stopping board is recorded after a random number of steps, delta, which ranges from 1 to 5. The initial board is generated with a random density of alive and dead cells.","['start_0', 'start_1', 'start_2', 'start_3', 'start_4', 'start_5', 'start_6', 'start_7', 'start_8', 'start_9', 'start_10', 'start_11', 'start_12', 'start_13', 'start_14', 'start_15', 'start_16', 'start_17', 'start_18', 'start_19', 'start_20', 'start_21', 'start_22', 'start_23', 'start_24', 'start_25', 'start_26', 'start_27', 'start_28', 'start_29', 'start_30', 'start_31', 'start_32', 'start_33', 'start_34', 'start_35', 'start_36', 'start_37', 'start_38', 'start_39', 'start_40', 'start_41', 'start_42', 'start_43', 'start_44', 'start_45', 'start_46', 'start_47', 'start_48', 'start_49', 'start_50', 'start_51', 'start_52', 'start_53', 'start_54', 'start_55', 'start_56', 'start_57', 'start_58', 'start_59', 'start_60', 'start_61', 'start_62', 'start_63', 'start_64', 'start_65', 'start_66', 'start_67', 'start_68', 'start_69', 'start_70', 'start_71', 'start_72', 'start_73', 'start_74', 'start_75', 'start_76', 'start_77', 'start_78', 'start_79', 'start_80', 'start_81', 'start_82', 'start_83', 'start_84', 'start_85', 'start_86', 'start_87', 'start_88', 'start_89', 'start_90', 'start_91', 'start_92', 'start_93', 'start_94', 'start_95', 'start_96', 'start_97', 'start_98', 'start_99', 'start_100', 'start_101', 'start_102', 'start_103', 'start_104', 'start_105', 'start_106', 'start_107', 'start_108', 'start_109', 'start_110', 'start_111', 'start_112', 'start_113', 'start_114', 'start_115', 'start_116', 'start_117', 'start_118', 'start_119', 'start_120', 'start_121', 'start_122', 'start_123', 'start_124', 'start_125', 'start_126', 'start_127', 'start_128', 'start_129', 'start_130', 'start_131', 'start_132', 'start_133', 'start_134', 'start_135', 'start_136', 'start_137', 'start_138', 'start_139', 'start_140', 'start_141', 'start_142', 'start_143', 'start_144', 'start_145', 'start_146', 'start_147', 'start_148', 'start_149', 'start_150', 'start_151', 'start_152', 'start_153', 'start_154', 'start_155', 'start_156', 'start_157', 'start_158', 'start_159', 'start_160', 'start_161', 'start_162', 'start_163', 'start_164', 'start_165', 'start_166', 'start_167', 'start_168', 'start_169', 'start_170', 'start_171', 'start_172', 'start_173', 'start_174', 'start_175', 'start_176', 'start_177', 'start_178', 'start_179', 'start_180', 'start_181', 'start_182', 'start_183', 'start_184', 'start_185', 'start_186', 'start_187', 'start_188', 'start_189', 'start_190', 'start_191', 'start_192', 'start_193', 'start_194', 'start_195', 'start_196', 'start_197', 'start_198', 'start_199', 'start_200', 'start_201', 'start_202', 'start_203', 'start_204', 'start_205', 'start_206', 'start_207', 'start_208', 'start_209', 'start_210', 'start_211', 'start_212', 'start_213', 'start_214', 'start_215', 'start_216', 'start_217', 'start_218', 'start_219', 'start_220', 'start_221', 'start_222', 'start_223', 'start_224', 'start_225', 'start_226', 'start_227', 'start_228', 'start_229', 'start_230', 'start_231', 'start_232', 'start_233', 'start_234', 'start_235', 'start_236', 'start_237', 'start_238', 'start_239', 'start_240', 'start_241', 'start_242', 'start_243', 'start_244', 'start_245', 'start_246', 'start_247', 'start_248', 'start_249', 'start_250', 'start_251', 'start_252', 'start_253', 'start_254', 'start_255', 'start_256', 'start_257', 'start_258', 'start_259', 'start_260', 'start_261', 'start_262', 'start_263', 'start_264', 'start_265', 'start_266', 'start_267', 'start_268', 'start_269', 'start_270', 'start_271', 'start_272', 'start_273', 'start_274', 'start_275', 'start_276', 'start_277', 'start_278', 'start_279', 'start_280', 'start_281', 'start_282', 'start_283', 'start_284', 'start_285', 'start_286', 'start_287', 'start_288', 'start_289', 'start_290', 'start_291', 'start_292', 'start_293', 'start_294', 'start_295', 'start_296', 'start_297', 'start_298', 'start_299', 'start_300', 'start_301', 'start_302', 'start_303', 'start_304', 'start_305', 'start_306', 'start_307', 'start_308', 'start_309', 'start_310', 'start_311', 'start_312', 'start_313', 'start_314', 'start_315', 'start_316', 'start_317', 'start_318', 'start_319', 'start_320', 'start_321', 'start_322', 'start_323', 'start_324', 'start_325', 'start_326', 'start_327', 'start_328', 'start_329', 'start_330', 'start_331', 'start_332', 'start_333', 'start_334', 'start_335', 'start_336', 'start_337', 'start_338', 'start_339', 'start_340', 'start_341', 'start_342', 'start_343', 'start_344', 'start_345', 'start_346', 'start_347', 'start_348', 'start_349', 'start_350', 'start_351', 'start_352', 'start_353', 'start_354', 'start_355', 'start_356', 'start_357', 'start_358', 'start_359', 'start_360', 'start_361', 'start_362', 'start_363', 'start_364', 'start_365', 'start_366', 'start_367', 'start_368', 'start_369', 'start_370', 'start_371', 'start_372', 'start_373', 'start_374', 'start_375', 'start_376', 'start_377', 'start_378', 'start_379', 'start_380', 'start_381', 'start_382', 'start_383', 'start_384', 'start_385', 'start_386', 'start_387', 'start_388', 'start_389', 'start_390', 'start_391', 'start_392', 'start_393', 'start_394', 'start_395', 'start_396', 'start_397', 'start_398', 'start_399', 'start_400', 'start_401', 'start_402', 'start_403', 'start_404', 'start_405', 'start_406', 'start_407', 'start_408', 'start_409', 'start_410', 'start_411', 'start_412', 'start_413', 'start_414', 'start_415', 'start_416', 'start_417', 'start_418', 'start_419', 'start_420', 'start_421', 'start_422', 'start_423', 'start_424', 'start_425', 'start_426', 'start_427', 'start_428', 'start_429', 'start_430', 'start_431', 'start_432', 'start_433', 'start_434', 'start_435', 'start_436', 'start_437', 'start_438', 'start_439', 'start_440', 'start_441', 'start_442', 'start_443', 'start_444', 'start_445', 'start_446', 'start_447', 'start_448', 'start_449', 'start_450', 'start_451', 'start_452', 'start_453', 'start_454', 'start_455', 'start_456', 'start_457', 'start_458', 'start_459', 'start_460', 'start_461', 'start_462', 'start_463', 'start_464', 'start_465', 'start_466', 'start_467', 'start_468', 'start_469', 'start_470', 'start_471', 'start_472', 'start_473', 'start_474', 'start_475', 'start_476', 'start_477', 'start_478', 'start_479', 'start_480', 'start_481', 'start_482', 'start_483', 'start_484', 'start_485', 'start_486', 'start_487', 'start_488', 'start_489', 'start_490', 'start_491', 'start_492', 'start_493', 'start_494', 'start_495', 'start_496', 'start_497', 'start_498', 'start_499', 'start_500', 'start_501', 'start_502', 'start_503', 'start_504', 'start_505', 'start_506', 'start_507', 'start_508', 'start_509', 'start_510', 'start_511', 'start_512', 'start_513', 'start_514', 'start_515', 'start_516', 'start_517', 'start_518', 'start_519', 'start_520', 'start_521', 'start_522', 'start_523', 'start_524', 'start_525', 'start_526', 'start_527', 'start_528', 'start_529', 'start_530', 'start_531', 'start_532', 'start_533', 'start_534', 'start_535', 'start_536', 'start_537', 'start_538', 'start_539', 'start_540', 'start_541', 'start_542', 'start_543', 'start_544', 'start_545', 'start_546', 'start_547', 'start_548', 'start_549', 'start_550', 'start_551', 'start_552', 'start_553', 'start_554', 'start_555', 'start_556', 'start_557', 'start_558', 'start_559', 'start_560', 'start_561', 'start_562', 'start_563', 'start_564', 'start_565', 'start_566', 'start_567', 'start_568', 'start_569', 'start_570', 'start_571', 'start_572', 'start_573', 'start_574', 'start_575', 'start_576', 'start_577', 'start_578', 'start_579', 'start_580', 'start_581', 'start_582', 'start_583', 'start_584', 'start_585', 'start_586', 'start_587', 'start_588', 'start_589', 'start_590', 'start_591', 'start_592', 'start_593', 'start_594', 'start_595', 'start_596', 'start_597', 'start_598', 'start_599', 'start_600', 'start_601', 'start_602', 'start_603', 'start_604', 'start_605', 'start_606', 'start_607', 'start_608', 'start_609', 'start_610', 'start_611', 'start_612', 'start_613', 'start_614', 'start_615', 'start_616', 'start_617', 'start_618', 'start_619', 'start_620', 'start_621', 'start_622', 'start_623', 'start_624']","[""reshape data to 25x25 format"", ""pad the board for boundary conditions"", ""normalize input values to range [0, 1]""]","i = Input((27, 27, 1))
x1 = Conv2D(64, (3, 3), activation='relu', padding='valid')(i)
x2 = Conv2D(64, (3, 3), activation='relu', padding='valid')(i)
x3 = Conv2D(64, (3, 3), activation='sigmoid', padding='valid')(i)
x = multiply([x3, add([x1, x2])])
x = MaxPooling2D((3, 3))(x)
x = Reshape((64, 64, 1))(x)
x = Conv2D(128, (8, 8), activation='relu', padding='valid')(x)
x1 = Conv2D(256, (8, 8), activation='relu', padding='valid')(x)
x2 = Conv2D(256, (8, 8), activation='sigmoid', padding='valid')(x)
x = multiply([x1, x2])
x = MaxPooling2D((2, 2))(x)
x = Reshape((400, 400, 1))(x)
x = Conv2D(1, (1, 1), activation='relu', padding='valid')(x)
x = MaxPooling2D((2, 2), strides=(2, 2))(x)
x = Flatten()(x)
o = Dense(625, activation='sigmoid')(x)
model = Model(inputs=i, outputs=o)
opt = Adam(lr=0.0001)
loss = CC(from_logits=False, reduction=tf.keras.losses.Reduction.SUM)
model.compile(optimizer=opt, loss=loss, metrics=['accuracy', 'mse'])
model.fit(tx, ty, validation_data=(vx, vy), batch_size=256, epochs=300, callbacks=[call_rlrp, call_es], verbose=1)",DL,Tensorflow,seraphwedd18/application-of-gan-for-predicting-initial-state,https://www.kaggle.com/seraphwedd18/application-of-gan-for-predicting-initial-state
conways-reverse-game-of-life-2020,classification,binary-classification,The task is to predict the starting board configuration of a game based on the stopping board configuration and the number of steps taken.,Tabular,MAE – Mean Absolute Error,"The dataset consists of 50,000 training games and 50,000 test games, each represented by a 25x25 board with 625 cells. The training data includes unique identifiers, the number of steps between the start and stop boards, and the state of the board at both the start and stop points. The initial board is generated randomly and evolves over a specified number of steps, with the stopping state recorded for prediction.","['start_0', 'start_1', 'start_2', 'start_3', 'start_4', 'start_5', 'start_6', 'start_7', 'start_8', 'start_9', 'start_10', 'start_11', 'start_12', 'start_13', 'start_14', 'start_15', 'start_16', 'start_17', 'start_18', 'start_19', 'start_20', 'start_21', 'start_22', 'start_23', 'start_24', 'start_25', 'start_26', 'start_27', 'start_28', 'start_29', 'start_30', 'start_31', 'start_32', 'start_33', 'start_34', 'start_35', 'start_36', 'start_37', 'start_38', 'start_39', 'start_40', 'start_41', 'start_42', 'start_43', 'start_44', 'start_45', 'start_46', 'start_47', 'start_48', 'start_49', 'start_50', 'start_51', 'start_52', 'start_53', 'start_54', 'start_55', 'start_56', 'start_57', 'start_58', 'start_59', 'start_60', 'start_61', 'start_62', 'start_63', 'start_64', 'start_65', 'start_66', 'start_67', 'start_68', 'start_69', 'start_70', 'start_71', 'start_72', 'start_73', 'start_74', 'start_75', 'start_76', 'start_77', 'start_78', 'start_79', 'start_80', 'start_81', 'start_82', 'start_83', 'start_84', 'start_85', 'start_86', 'start_87', 'start_88', 'start_89', 'start_90', 'start_91', 'start_92', 'start_93', 'start_94', 'start_95', 'start_96', 'start_97', 'start_98', 'start_99', 'start_100', 'start_101', 'start_102', 'start_103', 'start_104', 'start_105', 'start_106', 'start_107', 'start_108', 'start_109', 'start_110', 'start_111', 'start_112', 'start_113', 'start_114', 'start_115', 'start_116', 'start_117', 'start_118', 'start_119', 'start_120', 'start_121', 'start_122', 'start_123', 'start_124', 'start_125', 'start_126', 'start_127', 'start_128', 'start_129', 'start_130', 'start_131', 'start_132', 'start_133', 'start_134', 'start_135', 'start_136', 'start_137', 'start_138', 'start_139', 'start_140', 'start_141', 'start_142', 'start_143', 'start_144', 'start_145', 'start_146', 'start_147', 'start_148', 'start_149', 'start_150', 'start_151', 'start_152', 'start_153', 'start_154', 'start_155', 'start_156', 'start_157', 'start_158', 'start_159', 'start_160', 'start_161', 'start_162', 'start_163', 'start_164', 'start_165', 'start_166', 'start_167', 'start_168', 'start_169', 'start_170', 'start_171', 'start_172', 'start_173', 'start_174', 'start_175', 'start_176', 'start_177', 'start_178', 'start_179', 'start_180', 'start_181', 'start_182', 'start_183', 'start_184', 'start_185', 'start_186', 'start_187', 'start_188', 'start_189', 'start_190', 'start_191', 'start_192', 'start_193', 'start_194', 'start_195', 'start_196', 'start_197', 'start_198', 'start_199', 'start_200', 'start_201', 'start_202', 'start_203', 'start_204', 'start_205', 'start_206', 'start_207', 'start_208', 'start_209', 'start_210', 'start_211', 'start_212', 'start_213', 'start_214', 'start_215', 'start_216', 'start_217', 'start_218', 'start_219', 'start_220', 'start_221', 'start_222', 'start_223', 'start_224', 'start_225', 'start_226', 'start_227', 'start_228', 'start_229', 'start_230', 'start_231', 'start_232', 'start_233', 'start_234', 'start_235', 'start_236', 'start_237', 'start_238', 'start_239', 'start_240', 'start_241', 'start_242', 'start_243', 'start_244', 'start_245', 'start_246', 'start_247', 'start_248', 'start_249', 'start_250', 'start_251', 'start_252', 'start_253', 'start_254', 'start_255', 'start_256', 'start_257', 'start_258', 'start_259', 'start_260', 'start_261', 'start_262', 'start_263', 'start_264', 'start_265', 'start_266', 'start_267', 'start_268', 'start_269', 'start_270', 'start_271', 'start_272', 'start_273', 'start_274', 'start_275', 'start_276', 'start_277', 'start_278', 'start_279', 'start_280', 'start_281', 'start_282', 'start_283', 'start_284', 'start_285', 'start_286', 'start_287', 'start_288', 'start_289', 'start_290', 'start_291', 'start_292', 'start_293', 'start_294', 'start_295', 'start_296', 'start_297', 'start_298', 'start_299', 'start_300', 'start_301', 'start_302', 'start_303', 'start_304', 'start_305', 'start_306', 'start_307', 'start_308', 'start_309', 'start_310', 'start_311', 'start_312', 'start_313', 'start_314', 'start_315', 'start_316', 'start_317', 'start_318', 'start_319', 'start_320', 'start_321', 'start_322', 'start_323', 'start_324', 'start_325', 'start_326', 'start_327', 'start_328', 'start_329', 'start_330', 'start_331', 'start_332', 'start_333', 'start_334', 'start_335', 'start_336', 'start_337', 'start_338', 'start_339', 'start_340', 'start_341', 'start_342', 'start_343', 'start_344', 'start_345', 'start_346', 'start_347', 'start_348', 'start_349', 'start_350', 'start_351', 'start_352', 'start_353', 'start_354', 'start_355', 'start_356', 'start_357', 'start_358', 'start_359', 'start_360', 'start_361', 'start_362', 'start_363', 'start_364', 'start_365', 'start_366', 'start_367', 'start_368', 'start_369', 'start_370', 'start_371', 'start_372', 'start_373', 'start_374', 'start_375', 'start_376', 'start_377', 'start_378', 'start_379', 'start_380', 'start_381', 'start_382', 'start_383', 'start_384', 'start_385', 'start_386', 'start_387', 'start_388', 'start_389', 'start_390', 'start_391', 'start_392', 'start_393', 'start_394', 'start_395', 'start_396', 'start_397', 'start_398', 'start_399', 'start_400', 'start_401', 'start_402', 'start_403', 'start_404', 'start_405', 'start_406', 'start_407', 'start_408', 'start_409', 'start_410', 'start_411', 'start_412', 'start_413', 'start_414', 'start_415', 'start_416', 'start_417', 'start_418', 'start_419', 'start_420', 'start_421', 'start_422', 'start_423', 'start_424', 'start_425', 'start_426', 'start_427', 'start_428', 'start_429', 'start_430', 'start_431', 'start_432', 'start_433', 'start_434', 'start_435', 'start_436', 'start_437', 'start_438', 'start_439', 'start_440', 'start_441', 'start_442', 'start_443', 'start_444', 'start_445', 'start_446', 'start_447', 'start_448', 'start_449', 'start_450', 'start_451', 'start_452', 'start_453', 'start_454', 'start_455', 'start_456', 'start_457', 'start_458', 'start_459', 'start_460', 'start_461', 'start_462', 'start_463', 'start_464', 'start_465', 'start_466', 'start_467', 'start_468', 'start_469', 'start_470', 'start_471', 'start_472', 'start_473', 'start_474', 'start_475', 'start_476', 'start_477', 'start_478', 'start_479', 'start_480', 'start_481', 'start_482', 'start_483', 'start_484', 'start_485', 'start_486', 'start_487', 'start_488', 'start_489', 'start_490', 'start_491', 'start_492', 'start_493', 'start_494', 'start_495', 'start_496', 'start_497', 'start_498', 'start_499', 'start_500', 'start_501', 'start_502', 'start_503', 'start_504', 'start_505', 'start_506', 'start_507', 'start_508', 'start_509', 'start_510', 'start_511', 'start_512', 'start_513', 'start_514', 'start_515', 'start_516', 'start_517', 'start_518', 'start_519', 'start_520', 'start_521', 'start_522', 'start_523', 'start_524', 'start_525', 'start_526', 'start_527', 'start_528', 'start_529', 'start_530', 'start_531', 'start_532', 'start_533', 'start_534', 'start_535', 'start_536', 'start_537', 'start_538', 'start_539', 'start_540', 'start_541', 'start_542', 'start_543', 'start_544', 'start_545', 'start_546', 'start_547', 'start_548', 'start_549', 'start_550', 'start_551', 'start_552', 'start_553', 'start_554', 'start_555', 'start_556', 'start_557', 'start_558', 'start_559', 'start_560', 'start_561', 'start_562', 'start_563', 'start_564', 'start_565', 'start_566', 'start_567', 'start_568', 'start_569', 'start_570', 'start_571', 'start_572', 'start_573', 'start_574', 'start_575', 'start_576', 'start_577', 'start_578', 'start_579', 'start_580', 'start_581', 'start_582', 'start_583', 'start_584', 'start_585', 'start_586', 'start_587', 'start_588', 'start_589', 'start_590', 'start_591', 'start_592', 'start_593', 'start_594', 'start_595', 'start_596', 'start_597', 'start_598', 'start_599', 'start_600', 'start_601', 'start_602', 'start_603', 'start_604', 'start_605', 'start_606', 'start_607', 'start_608', 'start_609', 'start_610', 'start_611', 'start_612', 'start_613', 'start_614', 'start_615', 'start_616', 'start_617', 'start_618', 'start_619', 'start_620', 'start_621', 'start_622', 'start_623']","[""reshape stop features to 25x25 grid"", ""normalize input values to range [0, 1]"", ""batch the dataset for training""]","model = LifeModel()
model.compile(loss=""bce"", optimizer=tf.keras.optimizers.Adam(), metrics=[""accuracy""])
model.fit(tr_df, epochs=5)",DL,Tensorflow,parmarsuraj99/a-neural-lstm-game-of-life-with-keras,https://www.kaggle.com/parmarsuraj99/a-neural-lstm-game-of-life-with-keras
conways-reverse-game-of-life-2020,classification,binary-classification,"The task is to predict the starting board state of a game based on the stopping board state after a specified number of steps, where each cell can be either alive or dead.",Tabular,MAE – Mean Absolute Error,"The dataset consists of 50,000 training games and 50,000 test games, each represented by a 25x25 board with 625 cells. The training data includes unique identifiers, the number of steps between the start and stop boards, and the state of the board at both the start and stop. The initial board is generated randomly and evolves over a number of steps, with the stopping state recorded for prediction.","['start_0', 'start_1', 'start_2', 'start_3', 'start_4', 'start_5', 'start_6', 'start_7', 'start_8', 'start_9', 'start_10', 'start_11', 'start_12', 'start_13', 'start_14', 'start_15', 'start_16', 'start_17', 'start_18', 'start_19', 'start_20', 'start_21', 'start_22', 'start_23', 'start_24', 'start_25', 'start_26', 'start_27', 'start_28', 'start_29', 'start_30', 'start_31', 'start_32', 'start_33', 'start_34', 'start_35', 'start_36', 'start_37', 'start_38', 'start_39', 'start_40', 'start_41', 'start_42', 'start_43', 'start_44', 'start_45', 'start_46', 'start_47', 'start_48', 'start_49', 'start_50', 'start_51', 'start_52', 'start_53', 'start_54', 'start_55', 'start_56', 'start_57', 'start_58', 'start_59', 'start_60', 'start_61', 'start_62', 'start_63', 'start_64', 'start_65', 'start_66', 'start_67', 'start_68', 'start_69', 'start_70', 'start_71', 'start_72', 'start_73', 'start_74', 'start_75', 'start_76', 'start_77', 'start_78', 'start_79', 'start_80', 'start_81', 'start_82', 'start_83', 'start_84', 'start_85', 'start_86', 'start_87', 'start_88', 'start_89', 'start_90', 'start_91', 'start_92', 'start_93', 'start_94', 'start_95', 'start_96', 'start_97', 'start_98', 'start_99', 'start_100', 'start_101', 'start_102', 'start_103', 'start_104', 'start_105', 'start_106', 'start_107', 'start_108', 'start_109', 'start_110', 'start_111', 'start_112', 'start_113', 'start_114', 'start_115', 'start_116', 'start_117', 'start_118', 'start_119', 'start_120', 'start_121', 'start_122', 'start_123', 'start_124', 'start_125', 'start_126', 'start_127', 'start_128', 'start_129', 'start_130', 'start_131', 'start_132', 'start_133', 'start_134', 'start_135', 'start_136', 'start_137', 'start_138', 'start_139', 'start_140', 'start_141', 'start_142', 'start_143', 'start_144', 'start_145', 'start_146', 'start_147', 'start_148', 'start_149', 'start_150', 'start_151', 'start_152', 'start_153', 'start_154', 'start_155', 'start_156', 'start_157', 'start_158', 'start_159', 'start_160', 'start_161', 'start_162', 'start_163', 'start_164', 'start_165', 'start_166', 'start_167', 'start_168', 'start_169', 'start_170', 'start_171', 'start_172', 'start_173', 'start_174', 'start_175', 'start_176', 'start_177', 'start_178', 'start_179', 'start_180', 'start_181', 'start_182', 'start_183', 'start_184', 'start_185', 'start_186', 'start_187', 'start_188', 'start_189', 'start_190', 'start_191', 'start_192', 'start_193', 'start_194', 'start_195', 'start_196', 'start_197', 'start_198', 'start_199', 'start_200', 'start_201', 'start_202', 'start_203', 'start_204', 'start_205', 'start_206', 'start_207', 'start_208', 'start_209', 'start_210', 'start_211', 'start_212', 'start_213', 'start_214', 'start_215', 'start_216', 'start_217', 'start_218', 'start_219', 'start_220', 'start_221', 'start_222', 'start_223', 'start_224', 'start_225', 'start_226', 'start_227', 'start_228', 'start_229', 'start_230', 'start_231', 'start_232', 'start_233', 'start_234', 'start_235', 'start_236', 'start_237', 'start_238', 'start_239', 'start_240', 'start_241', 'start_242', 'start_243', 'start_244', 'start_245', 'start_246', 'start_247', 'start_248', 'start_249', 'start_250', 'start_251', 'start_252', 'start_253', 'start_254', 'start_255', 'start_256', 'start_257', 'start_258', 'start_259', 'start_260', 'start_261', 'start_262', 'start_263', 'start_264', 'start_265', 'start_266', 'start_267', 'start_268', 'start_269', 'start_270', 'start_271', 'start_272', 'start_273', 'start_274', 'start_275', 'start_276', 'start_277', 'start_278', 'start_279', 'start_280', 'start_281', 'start_282', 'start_283', 'start_284', 'start_285', 'start_286', 'start_287', 'start_288', 'start_289', 'start_290', 'start_291', 'start_292', 'start_293', 'start_294', 'start_295', 'start_296', 'start_297', 'start_298', 'start_299', 'start_300', 'start_301', 'start_302', 'start_303', 'start_304', 'start_305', 'start_306', 'start_307', 'start_308', 'start_309', 'start_310', 'start_311', 'start_312', 'start_313', 'start_314', 'start_315', 'start_316', 'start_317', 'start_318', 'start_319', 'start_320', 'start_321', 'start_322', 'start_323', 'start_324', 'start_325', 'start_326', 'start_327', 'start_328', 'start_329', 'start_330', 'start_331', 'start_332', 'start_333', 'start_334', 'start_335', 'start_336', 'start_337', 'start_338', 'start_339', 'start_340', 'start_341', 'start_342', 'start_343', 'start_344', 'start_345', 'start_346', 'start_347', 'start_348', 'start_349', 'start_350', 'start_351', 'start_352', 'start_353', 'start_354', 'start_355', 'start_356', 'start_357', 'start_358', 'start_359', 'start_360', 'start_361', 'start_362', 'start_363', 'start_364', 'start_365', 'start_366', 'start_367', 'start_368', 'start_369', 'start_370', 'start_371', 'start_372', 'start_373', 'start_374', 'start_375', 'start_376', 'start_377', 'start_378', 'start_379', 'start_380', 'start_381', 'start_382', 'start_383', 'start_384', 'start_385', 'start_386', 'start_387', 'start_388', 'start_389', 'start_390', 'start_391', 'start_392', 'start_393', 'start_394', 'start_395', 'start_396', 'start_397', 'start_398', 'start_399', 'start_400', 'start_401', 'start_402', 'start_403', 'start_404', 'start_405', 'start_406', 'start_407', 'start_408', 'start_409', 'start_410', 'start_411', 'start_412', 'start_413', 'start_414', 'start_415', 'start_416', 'start_417', 'start_418', 'start_419', 'start_420', 'start_421', 'start_422', 'start_423', 'start_424', 'start_425', 'start_426', 'start_427', 'start_428', 'start_429', 'start_430', 'start_431', 'start_432', 'start_433', 'start_434', 'start_435', 'start_436', 'start_437', 'start_438', 'start_439', 'start_440', 'start_441', 'start_442', 'start_443', 'start_444', 'start_445', 'start_446', 'start_447', 'start_448', 'start_449', 'start_450', 'start_451', 'start_452', 'start_453', 'start_454', 'start_455', 'start_456', 'start_457', 'start_458', 'start_459', 'start_460', 'start_461', 'start_462', 'start_463', 'start_464', 'start_465', 'start_466', 'start_467', 'start_468', 'start_469', 'start_470', 'start_471', 'start_472', 'start_473', 'start_474', 'start_475', 'start_476', 'start_477', 'start_478', 'start_479', 'start_480', 'start_481', 'start_482', 'start_483', 'start_484', 'start_485', 'start_486', 'start_487', 'start_488', 'start_489', 'start_490', 'start_491', 'start_492', 'start_493', 'start_494', 'start_495', 'start_496', 'start_497', 'start_498', 'start_499', 'start_500', 'start_501', 'start_502', 'start_503', 'start_504', 'start_505', 'start_506', 'start_507', 'start_508', 'start_509', 'start_510', 'start_511', 'start_512', 'start_513', 'start_514', 'start_515', 'start_516', 'start_517', 'start_518', 'start_519', 'start_520', 'start_521', 'start_522', 'start_523', 'start_524', 'start_525', 'start_526', 'start_527', 'start_528', 'start_529', 'start_530', 'start_531', 'start_532', 'start_533', 'start_534', 'start_535', 'start_536', 'start_537', 'start_538', 'start_539', 'start_540', 'start_541', 'start_542', 'start_543', 'start_544', 'start_545', 'start_546', 'start_547', 'start_548', 'start_549', 'start_550', 'start_551', 'start_552', 'start_553', 'start_554', 'start_555', 'start_556', 'start_557', 'start_558', 'start_559', 'start_560', 'start_561', 'start_562', 'start_563', 'start_564', 'start_565', 'start_566', 'start_567', 'start_568', 'start_569', 'start_570', 'start_571', 'start_572', 'start_573', 'start_574', 'start_575', 'start_576', 'start_577', 'start_578', 'start_579', 'start_580', 'start_581', 'start_582', 'start_583', 'start_584', 'start_585', 'start_586', 'start_587', 'start_588', 'start_589', 'start_590', 'start_591', 'start_592', 'start_593', 'start_594', 'start_595', 'start_596', 'start_597', 'start_598', 'start_599', 'start_600', 'start_601', 'start_602', 'start_603', 'start_604', 'start_605', 'start_606', 'start_607', 'start_608', 'start_609', 'start_610', 'start_611', 'start_612', 'start_613', 'start_614', 'start_615', 'start_616', 'start_617', 'start_618', 'start_619', 'start_620', 'start_621', 'start_622', 'start_623']","[""reshape input data to 3D for model compatibility"", ""split data into training and testing sets"", ""normalize input data using MinMaxScaler""]","model = Sequential()
model.add(Conv2D(filters, kernel_size, padding='same', activation='relu', strides=strides, input_shape=(25, 25, 1)))
model.add(Conv2D(filters, kernel_size, padding='same', activation='relu', strides=strides))
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dense(32))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))",DL,Tensorflow,lakitha/game-of-life-code-keras-cnn-accuracy-84,https://www.kaggle.com/lakitha/game-of-life-code-keras-cnn-accuracy-84
conways-reverse-game-of-life-2020,classification,binary-classification,The task is to predict the starting board configuration of a game based on the stopping board configuration after a specified number of steps.,Tabular,MAE – Mean Absolute Error,"The dataset consists of 50,000 training games and 50,000 test games, each represented as a 25x25 board with 625 cells. Each game has a unique identifier and includes variables for the number of steps between the start and stop boards, as well as the state of the board at both the start and stop points. The starting board is recorded after a warmup period of 5 steps, and the stopping board is recorded after a random number of steps, delta, which ranges from 1 to 5. The dataset allows for the creation of additional training games if desired.","['start_0', 'start_1', 'start_2', 'start_3', 'start_4', 'start_5', 'start_6', 'start_7', 'start_8', 'start_9', 'start_10', 'start_11', 'start_12', 'start_13', 'start_14', 'start_15', 'start_16', 'start_17', 'start_18', 'start_19', 'start_20', 'start_21', 'start_22', 'start_23', 'start_24', 'start_25', 'start_26', 'start_27', 'start_28', 'start_29', 'start_30', 'start_31', 'start_32', 'start_33', 'start_34', 'start_35', 'start_36', 'start_37', 'start_38', 'start_39', 'start_40', 'start_41', 'start_42', 'start_43', 'start_44', 'start_45', 'start_46', 'start_47', 'start_48', 'start_49', 'start_50', 'start_51', 'start_52', 'start_53', 'start_54', 'start_55', 'start_56', 'start_57', 'start_58', 'start_59', 'start_60', 'start_61', 'start_62', 'start_63', 'start_64', 'start_65', 'start_66', 'start_67', 'start_68', 'start_69', 'start_70', 'start_71', 'start_72', 'start_73', 'start_74', 'start_75', 'start_76', 'start_77', 'start_78', 'start_79', 'start_80', 'start_81', 'start_82', 'start_83', 'start_84', 'start_85', 'start_86', 'start_87', 'start_88', 'start_89', 'start_90', 'start_91', 'start_92', 'start_93', 'start_94', 'start_95', 'start_96', 'start_97', 'start_98', 'start_99', 'start_100', 'start_101', 'start_102', 'start_103', 'start_104', 'start_105', 'start_106', 'start_107', 'start_108', 'start_109', 'start_110', 'start_111', 'start_112', 'start_113', 'start_114', 'start_115', 'start_116', 'start_117', 'start_118', 'start_119', 'start_120', 'start_121', 'start_122', 'start_123', 'start_124', 'start_125', 'start_126', 'start_127', 'start_128', 'start_129', 'start_130', 'start_131', 'start_132', 'start_133', 'start_134', 'start_135', 'start_136', 'start_137', 'start_138', 'start_139', 'start_140', 'start_141', 'start_142', 'start_143', 'start_144', 'start_145', 'start_146', 'start_147', 'start_148', 'start_149', 'start_150', 'start_151', 'start_152', 'start_153', 'start_154', 'start_155', 'start_156', 'start_157', 'start_158', 'start_159', 'start_160', 'start_161', 'start_162', 'start_163', 'start_164', 'start_165', 'start_166', 'start_167', 'start_168', 'start_169', 'start_170', 'start_171', 'start_172', 'start_173', 'start_174', 'start_175', 'start_176', 'start_177', 'start_178', 'start_179', 'start_180', 'start_181', 'start_182', 'start_183', 'start_184', 'start_185', 'start_186', 'start_187', 'start_188', 'start_189', 'start_190', 'start_191', 'start_192', 'start_193', 'start_194', 'start_195', 'start_196', 'start_197', 'start_198', 'start_199', 'start_200', 'start_201', 'start_202', 'start_203', 'start_204', 'start_205', 'start_206', 'start_207', 'start_208', 'start_209', 'start_210', 'start_211', 'start_212', 'start_213', 'start_214', 'start_215', 'start_216', 'start_217', 'start_218', 'start_219', 'start_220', 'start_221', 'start_222', 'start_223', 'start_224', 'start_225', 'start_226', 'start_227', 'start_228', 'start_229', 'start_230', 'start_231', 'start_232', 'start_233', 'start_234', 'start_235', 'start_236', 'start_237', 'start_238', 'start_239', 'start_240', 'start_241', 'start_242', 'start_243', 'start_244', 'start_245', 'start_246', 'start_247', 'start_248', 'start_249', 'start_250', 'start_251', 'start_252', 'start_253', 'start_254', 'start_255', 'start_256', 'start_257', 'start_258', 'start_259', 'start_260', 'start_261', 'start_262', 'start_263', 'start_264', 'start_265', 'start_266', 'start_267', 'start_268', 'start_269', 'start_270', 'start_271', 'start_272', 'start_273', 'start_274', 'start_275', 'start_276', 'start_277', 'start_278', 'start_279', 'start_280', 'start_281', 'start_282', 'start_283', 'start_284', 'start_285', 'start_286', 'start_287', 'start_288', 'start_289', 'start_290', 'start_291', 'start_292', 'start_293', 'start_294', 'start_295', 'start_296', 'start_297', 'start_298', 'start_299', 'start_300', 'start_301', 'start_302', 'start_303', 'start_304', 'start_305', 'start_306', 'start_307', 'start_308', 'start_309', 'start_310', 'start_311', 'start_312', 'start_313', 'start_314', 'start_315', 'start_316', 'start_317', 'start_318', 'start_319', 'start_320', 'start_321', 'start_322', 'start_323', 'start_324', 'start_325', 'start_326', 'start_327', 'start_328', 'start_329', 'start_330', 'start_331', 'start_332', 'start_333', 'start_334', 'start_335', 'start_336', 'start_337', 'start_338', 'start_339', 'start_340', 'start_341', 'start_342', 'start_343', 'start_344', 'start_345', 'start_346', 'start_347', 'start_348', 'start_349', 'start_350', 'start_351', 'start_352', 'start_353', 'start_354', 'start_355', 'start_356', 'start_357', 'start_358', 'start_359', 'start_360', 'start_361', 'start_362', 'start_363', 'start_364', 'start_365', 'start_366', 'start_367', 'start_368', 'start_369', 'start_370', 'start_371', 'start_372', 'start_373', 'start_374', 'start_375', 'start_376', 'start_377', 'start_378', 'start_379', 'start_380', 'start_381', 'start_382', 'start_383', 'start_384', 'start_385', 'start_386', 'start_387', 'start_388', 'start_389', 'start_390', 'start_391', 'start_392', 'start_393', 'start_394', 'start_395', 'start_396', 'start_397', 'start_398', 'start_399', 'start_400', 'start_401', 'start_402', 'start_403', 'start_404', 'start_405', 'start_406', 'start_407', 'start_408', 'start_409', 'start_410', 'start_411', 'start_412', 'start_413', 'start_414', 'start_415', 'start_416', 'start_417', 'start_418', 'start_419', 'start_420', 'start_421', 'start_422', 'start_423', 'start_424', 'start_425', 'start_426', 'start_427', 'start_428', 'start_429', 'start_430', 'start_431', 'start_432', 'start_433', 'start_434', 'start_435', 'start_436', 'start_437', 'start_438', 'start_439', 'start_440', 'start_441', 'start_442', 'start_443', 'start_444', 'start_445', 'start_446', 'start_447', 'start_448', 'start_449', 'start_450', 'start_451', 'start_452', 'start_453', 'start_454', 'start_455', 'start_456', 'start_457', 'start_458', 'start_459', 'start_460', 'start_461', 'start_462', 'start_463', 'start_464', 'start_465', 'start_466', 'start_467', 'start_468', 'start_469', 'start_470', 'start_471', 'start_472', 'start_473', 'start_474', 'start_475', 'start_476', 'start_477', 'start_478', 'start_479', 'start_480', 'start_481', 'start_482', 'start_483', 'start_484', 'start_485', 'start_486', 'start_487', 'start_488', 'start_489', 'start_490', 'start_491', 'start_492', 'start_493', 'start_494', 'start_495', 'start_496', 'start_497', 'start_498', 'start_499', 'start_500', 'start_501', 'start_502', 'start_503', 'start_504', 'start_505', 'start_506', 'start_507', 'start_508', 'start_509', 'start_510', 'start_511', 'start_512', 'start_513', 'start_514', 'start_515', 'start_516', 'start_517', 'start_518', 'start_519', 'start_520', 'start_521', 'start_522', 'start_523', 'start_524', 'start_525', 'start_526', 'start_527', 'start_528', 'start_529', 'start_530', 'start_531', 'start_532', 'start_533', 'start_534', 'start_535', 'start_536', 'start_537', 'start_538', 'start_539', 'start_540', 'start_541', 'start_542', 'start_543', 'start_544', 'start_545', 'start_546', 'start_547', 'start_548', 'start_549', 'start_550', 'start_551', 'start_552', 'start_553', 'start_554', 'start_555', 'start_556', 'start_557', 'start_558', 'start_559', 'start_560', 'start_561', 'start_562', 'start_563', 'start_564', 'start_565', 'start_566', 'start_567', 'start_568', 'start_569', 'start_570', 'start_571', 'start_572', 'start_573', 'start_574', 'start_575', 'start_576', 'start_577', 'start_578', 'start_579', 'start_580', 'start_581', 'start_582', 'start_583', 'start_584', 'start_585', 'start_586', 'start_587', 'start_588', 'start_589', 'start_590', 'start_591', 'start_592', 'start_593', 'start_594', 'start_595', 'start_596', 'start_597', 'start_598', 'start_599', 'start_600', 'start_601', 'start_602', 'start_603', 'start_604', 'start_605', 'start_606', 'start_607', 'start_608', 'start_609', 'start_610', 'start_611', 'start_612', 'start_613', 'start_614', 'start_615', 'start_616', 'start_617', 'start_618', 'start_619', 'start_620', 'start_621', 'start_622', 'start_623']","[""reshape data into 25x25 grid format"", ""convert data to tensor format"", ""split data into training and validation sets""]","model = ReverseModel()
optimizer = Lookahead(RAdam(params=model.parameters(), lr=1e-3))
criterion = {""bce"": nn.BCEWithLogitsLoss()}
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.25, patience=2)
runner.train(model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler, loaders=loaders, callbacks=callbacks, logdir=""./logs"", num_epochs=999, main_metric=""loss"", minimize_metric=True, verbose=True)",DL,Pytorch,yakuben/crgl2020-iterative-cnn-approach,https://www.kaggle.com/yakuben/crgl2020-iterative-cnn-approach
conways-reverse-game-of-life-2020,classification,binary-classification,The task is to predict the starting board configuration of a game based on the stopping board configuration after a specified number of steps.,Tabular,MAE – Mean Absolute Error,"The dataset consists of 50,000 training games and 50,000 test games, each represented by a 25x25 grid of cells. Each cell can either be alive or dead, and the data includes unique identifiers, the number of steps between the start and stop boards, and the state of the boards at both starting and stopping points. The starting board is recorded after five warmup steps to allow the cells to stabilize into a more life-like state. The stopping board is recorded after a random number of steps, delta, which ranges from 1 to 5. The dataset is provided in CSV format and is subject to competition rules.","['start_0', 'start_1', 'start_2', 'start_3', 'start_4', 'start_5', 'start_6', 'start_7', 'start_8', 'start_9', 'start_10', 'start_11', 'start_12', 'start_13', 'start_14', 'start_15', 'start_16', 'start_17', 'start_18', 'start_19', 'start_20', 'start_21', 'start_22', 'start_23', 'start_24', 'start_25', 'start_26', 'start_27', 'start_28', 'start_29', 'start_30', 'start_31', 'start_32', 'start_33', 'start_34', 'start_35', 'start_36', 'start_37', 'start_38', 'start_39', 'start_40', 'start_41', 'start_42', 'start_43', 'start_44', 'start_45', 'start_46', 'start_47', 'start_48', 'start_49', 'start_50', 'start_51', 'start_52', 'start_53', 'start_54', 'start_55', 'start_56', 'start_57', 'start_58', 'start_59', 'start_60', 'start_61', 'start_62', 'start_63', 'start_64', 'start_65', 'start_66', 'start_67', 'start_68', 'start_69', 'start_70', 'start_71', 'start_72', 'start_73', 'start_74', 'start_75', 'start_76', 'start_77', 'start_78', 'start_79', 'start_80', 'start_81', 'start_82', 'start_83', 'start_84', 'start_85', 'start_86', 'start_87', 'start_88', 'start_89', 'start_90', 'start_91', 'start_92', 'start_93', 'start_94', 'start_95', 'start_96', 'start_97', 'start_98', 'start_99', 'start_100', 'start_101', 'start_102', 'start_103', 'start_104', 'start_105', 'start_106', 'start_107', 'start_108', 'start_109', 'start_110', 'start_111', 'start_112', 'start_113', 'start_114', 'start_115', 'start_116', 'start_117', 'start_118', 'start_119', 'start_120', 'start_121', 'start_122', 'start_123', 'start_124', 'start_125', 'start_126', 'start_127', 'start_128', 'start_129', 'start_130', 'start_131', 'start_132', 'start_133', 'start_134', 'start_135', 'start_136', 'start_137', 'start_138', 'start_139', 'start_140', 'start_141', 'start_142', 'start_143', 'start_144', 'start_145', 'start_146', 'start_147', 'start_148', 'start_149', 'start_150', 'start_151', 'start_152', 'start_153', 'start_154', 'start_155', 'start_156', 'start_157', 'start_158', 'start_159', 'start_160', 'start_161', 'start_162', 'start_163', 'start_164', 'start_165', 'start_166', 'start_167', 'start_168', 'start_169', 'start_170', 'start_171', 'start_172', 'start_173', 'start_174', 'start_175', 'start_176', 'start_177', 'start_178', 'start_179', 'start_180', 'start_181', 'start_182', 'start_183', 'start_184', 'start_185', 'start_186', 'start_187', 'start_188', 'start_189', 'start_190', 'start_191', 'start_192', 'start_193', 'start_194', 'start_195', 'start_196', 'start_197', 'start_198', 'start_199', 'start_200', 'start_201', 'start_202', 'start_203', 'start_204', 'start_205', 'start_206', 'start_207', 'start_208', 'start_209', 'start_210', 'start_211', 'start_212', 'start_213', 'start_214', 'start_215', 'start_216', 'start_217', 'start_218', 'start_219', 'start_220', 'start_221', 'start_222', 'start_223', 'start_224', 'start_225', 'start_226', 'start_227', 'start_228', 'start_229', 'start_230', 'start_231', 'start_232', 'start_233', 'start_234', 'start_235', 'start_236', 'start_237', 'start_238', 'start_239', 'start_240', 'start_241', 'start_242', 'start_243', 'start_244', 'start_245', 'start_246', 'start_247', 'start_248', 'start_249', 'start_250', 'start_251', 'start_252', 'start_253', 'start_254', 'start_255', 'start_256', 'start_257', 'start_258', 'start_259', 'start_260', 'start_261', 'start_262', 'start_263', 'start_264', 'start_265', 'start_266', 'start_267', 'start_268', 'start_269', 'start_270', 'start_271', 'start_272', 'start_273', 'start_274', 'start_275', 'start_276', 'start_277', 'start_278', 'start_279', 'start_280', 'start_281', 'start_282', 'start_283', 'start_284', 'start_285', 'start_286', 'start_287', 'start_288', 'start_289', 'start_290', 'start_291', 'start_292', 'start_293', 'start_294', 'start_295', 'start_296', 'start_297', 'start_298', 'start_299', 'start_300', 'start_301', 'start_302', 'start_303', 'start_304', 'start_305', 'start_306', 'start_307', 'start_308', 'start_309', 'start_310', 'start_311', 'start_312', 'start_313', 'start_314', 'start_315', 'start_316', 'start_317', 'start_318', 'start_319', 'start_320', 'start_321', 'start_322', 'start_323', 'start_324', 'start_325', 'start_326', 'start_327', 'start_328', 'start_329', 'start_330', 'start_331', 'start_332', 'start_333', 'start_334', 'start_335', 'start_336', 'start_337', 'start_338', 'start_339', 'start_340', 'start_341', 'start_342', 'start_343', 'start_344', 'start_345', 'start_346', 'start_347', 'start_348', 'start_349', 'start_350', 'start_351', 'start_352', 'start_353', 'start_354', 'start_355', 'start_356', 'start_357', 'start_358', 'start_359', 'start_360', 'start_361', 'start_362', 'start_363', 'start_364', 'start_365', 'start_366', 'start_367', 'start_368', 'start_369', 'start_370', 'start_371', 'start_372', 'start_373', 'start_374', 'start_375', 'start_376', 'start_377', 'start_378', 'start_379', 'start_380', 'start_381', 'start_382', 'start_383', 'start_384', 'start_385', 'start_386', 'start_387', 'start_388', 'start_389', 'start_390', 'start_391', 'start_392', 'start_393', 'start_394', 'start_395', 'start_396', 'start_397', 'start_398', 'start_399', 'start_400', 'start_401', 'start_402', 'start_403', 'start_404', 'start_405', 'start_406', 'start_407', 'start_408', 'start_409', 'start_410', 'start_411', 'start_412', 'start_413', 'start_414', 'start_415', 'start_416', 'start_417', 'start_418', 'start_419', 'start_420', 'start_421', 'start_422', 'start_423', 'start_424', 'start_425', 'start_426', 'start_427', 'start_428', 'start_429', 'start_430', 'start_431', 'start_432', 'start_433', 'start_434', 'start_435', 'start_436', 'start_437', 'start_438', 'start_439', 'start_440', 'start_441', 'start_442', 'start_443', 'start_444', 'start_445', 'start_446', 'start_447', 'start_448', 'start_449', 'start_450', 'start_451', 'start_452', 'start_453', 'start_454', 'start_455', 'start_456', 'start_457', 'start_458', 'start_459', 'start_460', 'start_461', 'start_462', 'start_463', 'start_464', 'start_465', 'start_466', 'start_467', 'start_468', 'start_469', 'start_470', 'start_471', 'start_472', 'start_473', 'start_474', 'start_475', 'start_476', 'start_477', 'start_478', 'start_479', 'start_480', 'start_481', 'start_482', 'start_483', 'start_484', 'start_485', 'start_486', 'start_487', 'start_488', 'start_489', 'start_490', 'start_491', 'start_492', 'start_493', 'start_494', 'start_495', 'start_496', 'start_497', 'start_498', 'start_499', 'start_500', 'start_501', 'start_502', 'start_503', 'start_504', 'start_505', 'start_506', 'start_507', 'start_508', 'start_509', 'start_510', 'start_511', 'start_512', 'start_513', 'start_514', 'start_515', 'start_516', 'start_517', 'start_518', 'start_519', 'start_520', 'start_521', 'start_522', 'start_523', 'start_524', 'start_525', 'start_526', 'start_527', 'start_528', 'start_529', 'start_530', 'start_531', 'start_532', 'start_533', 'start_534', 'start_535', 'start_536', 'start_537', 'start_538', 'start_539', 'start_540', 'start_541', 'start_542', 'start_543', 'start_544', 'start_545', 'start_546', 'start_547', 'start_548', 'start_549', 'start_550', 'start_551', 'start_552', 'start_553', 'start_554', 'start_555', 'start_556', 'start_557', 'start_558', 'start_559', 'start_560', 'start_561', 'start_562', 'start_563', 'start_564', 'start_565', 'start_566', 'start_567', 'start_568', 'start_569', 'start_570', 'start_571', 'start_572', 'start_573', 'start_574', 'start_575', 'start_576', 'start_577', 'start_578', 'start_579', 'start_580', 'start_581', 'start_582', 'start_583', 'start_584', 'start_585', 'start_586', 'start_587', 'start_588', 'start_589', 'start_590', 'start_591', 'start_592', 'start_593', 'start_594', 'start_595', 'start_596', 'start_597', 'start_598', 'start_599', 'start_600', 'start_601', 'start_602', 'start_603', 'start_604', 'start_605', 'start_606', 'start_607', 'start_608', 'start_609', 'start_610', 'start_611', 'start_612', 'start_613', 'start_614', 'start_615', 'start_616', 'start_617', 'start_618', 'start_619', 'start_620', 'start_621', 'start_622', 'start_623', 'start_624']","[""reshape the board data into a suitable format for processing"", ""normalize the cell values to binary (0 or 1)""]","cv = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, padding_mode='circular', bias=False)
cv.requires_grad=False
cv.weight = torch.nn.Parameter(
    torch.tensor(
        [[[[ 1., 1., 1.],
           [ 1., 0., 1.],
           [ 1., 1., 1.]]]],
        device=device,
        dtype=torch.float16
    ),
    requires_grad=False,
)

submission.to_csv(OUTPUT_CSV)",DL,Pytorch,ebouteillon/top-10-with-first-genetic-algorithm-on-gpu,https://www.kaggle.com/ebouteillon/top-10-with-first-genetic-algorithm-on-gpu
conways-reverse-game-of-life-2020,classification,binary-classification,"The task is to predict the starting board state of a game based on the stopping board state after a specified number of steps, where each cell can be either alive or dead.",Tabular,MAE – Mean Absolute Error,"The dataset consists of 50,000 training games and 50,000 test games, each represented as a 25x25 board with 625 cells. The training data includes unique identifiers, the number of steps between the start and stop boards, and the state of each cell in both the starting and stopping boards. The starting board is recorded after five warmup steps to allow the cells to stabilize into a more realistic state. The stopping board is recorded after a random number of steps, delta, which ranges from 1 to 5. The dataset allows for the creation of additional training games if desired.","['start_0', 'start_1', 'start_2', 'start_3', 'start_4', 'start_5', 'start_6', 'start_7', 'start_8', 'start_9', 'start_10', 'start_11', 'start_12', 'start_13', 'start_14', 'start_15', 'start_16', 'start_17', 'start_18', 'start_19', 'start_20', 'start_21', 'start_22', 'start_23', 'start_24', 'start_25', 'start_26', 'start_27', 'start_28', 'start_29', 'start_30', 'start_31', 'start_32', 'start_33', 'start_34', 'start_35', 'start_36', 'start_37', 'start_38', 'start_39', 'start_40', 'start_41', 'start_42', 'start_43', 'start_44', 'start_45', 'start_46', 'start_47', 'start_48', 'start_49', 'start_50', 'start_51', 'start_52', 'start_53', 'start_54', 'start_55', 'start_56', 'start_57', 'start_58', 'start_59', 'start_60', 'start_61', 'start_62', 'start_63', 'start_64', 'start_65', 'start_66', 'start_67', 'start_68', 'start_69', 'start_70', 'start_71', 'start_72', 'start_73', 'start_74', 'start_75', 'start_76', 'start_77', 'start_78', 'start_79', 'start_80', 'start_81', 'start_82', 'start_83', 'start_84', 'start_85', 'start_86', 'start_87', 'start_88', 'start_89', 'start_90', 'start_91', 'start_92', 'start_93', 'start_94', 'start_95', 'start_96', 'start_97', 'start_98', 'start_99', 'start_100', 'start_101', 'start_102', 'start_103', 'start_104', 'start_105', 'start_106', 'start_107', 'start_108', 'start_109', 'start_110', 'start_111', 'start_112', 'start_113', 'start_114', 'start_115', 'start_116', 'start_117', 'start_118', 'start_119', 'start_120', 'start_121', 'start_122', 'start_123', 'start_124', 'start_125', 'start_126', 'start_127', 'start_128', 'start_129', 'start_130', 'start_131', 'start_132', 'start_133', 'start_134', 'start_135', 'start_136', 'start_137', 'start_138', 'start_139', 'start_140', 'start_141', 'start_142', 'start_143', 'start_144', 'start_145', 'start_146', 'start_147', 'start_148', 'start_149', 'start_150', 'start_151', 'start_152', 'start_153', 'start_154', 'start_155', 'start_156', 'start_157', 'start_158', 'start_159', 'start_160', 'start_161', 'start_162', 'start_163', 'start_164', 'start_165', 'start_166', 'start_167', 'start_168', 'start_169', 'start_170', 'start_171', 'start_172', 'start_173', 'start_174', 'start_175', 'start_176', 'start_177', 'start_178', 'start_179', 'start_180', 'start_181', 'start_182', 'start_183', 'start_184', 'start_185', 'start_186', 'start_187', 'start_188', 'start_189', 'start_190', 'start_191', 'start_192', 'start_193', 'start_194', 'start_195', 'start_196', 'start_197', 'start_198', 'start_199', 'start_200', 'start_201', 'start_202', 'start_203', 'start_204', 'start_205', 'start_206', 'start_207', 'start_208', 'start_209', 'start_210', 'start_211', 'start_212', 'start_213', 'start_214', 'start_215', 'start_216', 'start_217', 'start_218', 'start_219', 'start_220', 'start_221', 'start_222', 'start_223', 'start_224', 'start_225', 'start_226', 'start_227', 'start_228', 'start_229', 'start_230', 'start_231', 'start_232', 'start_233', 'start_234', 'start_235', 'start_236', 'start_237', 'start_238', 'start_239', 'start_240', 'start_241', 'start_242', 'start_243', 'start_244', 'start_245', 'start_246', 'start_247', 'start_248', 'start_249', 'start_250', 'start_251', 'start_252', 'start_253', 'start_254', 'start_255', 'start_256', 'start_257', 'start_258', 'start_259', 'start_260', 'start_261', 'start_262', 'start_263', 'start_264', 'start_265', 'start_266', 'start_267', 'start_268', 'start_269', 'start_270', 'start_271', 'start_272', 'start_273', 'start_274', 'start_275', 'start_276', 'start_277', 'start_278', 'start_279', 'start_280', 'start_281', 'start_282', 'start_283', 'start_284', 'start_285', 'start_286', 'start_287', 'start_288', 'start_289', 'start_290', 'start_291', 'start_292', 'start_293', 'start_294', 'start_295', 'start_296', 'start_297', 'start_298', 'start_299', 'start_300', 'start_301', 'start_302', 'start_303', 'start_304', 'start_305', 'start_306', 'start_307', 'start_308', 'start_309', 'start_310', 'start_311', 'start_312', 'start_313', 'start_314', 'start_315', 'start_316', 'start_317', 'start_318', 'start_319', 'start_320', 'start_321', 'start_322', 'start_323', 'start_324', 'start_325', 'start_326', 'start_327', 'start_328', 'start_329', 'start_330', 'start_331', 'start_332', 'start_333', 'start_334', 'start_335', 'start_336', 'start_337', 'start_338', 'start_339', 'start_340', 'start_341', 'start_342', 'start_343', 'start_344', 'start_345', 'start_346', 'start_347', 'start_348', 'start_349', 'start_350', 'start_351', 'start_352', 'start_353', 'start_354', 'start_355', 'start_356', 'start_357', 'start_358', 'start_359', 'start_360', 'start_361', 'start_362', 'start_363', 'start_364', 'start_365', 'start_366', 'start_367', 'start_368', 'start_369', 'start_370', 'start_371', 'start_372', 'start_373', 'start_374', 'start_375', 'start_376', 'start_377', 'start_378', 'start_379', 'start_380', 'start_381', 'start_382', 'start_383', 'start_384', 'start_385', 'start_386', 'start_387', 'start_388', 'start_389', 'start_390', 'start_391', 'start_392', 'start_393', 'start_394', 'start_395', 'start_396', 'start_397', 'start_398', 'start_399', 'start_400', 'start_401', 'start_402', 'start_403', 'start_404', 'start_405', 'start_406', 'start_407', 'start_408', 'start_409', 'start_410', 'start_411', 'start_412', 'start_413', 'start_414', 'start_415', 'start_416', 'start_417', 'start_418', 'start_419', 'start_420', 'start_421', 'start_422', 'start_423', 'start_424', 'start_425', 'start_426', 'start_427', 'start_428', 'start_429', 'start_430', 'start_431', 'start_432', 'start_433', 'start_434', 'start_435', 'start_436', 'start_437', 'start_438', 'start_439', 'start_440', 'start_441', 'start_442', 'start_443', 'start_444', 'start_445', 'start_446', 'start_447', 'start_448', 'start_449', 'start_450', 'start_451', 'start_452', 'start_453', 'start_454', 'start_455', 'start_456', 'start_457', 'start_458', 'start_459', 'start_460', 'start_461', 'start_462', 'start_463', 'start_464', 'start_465', 'start_466', 'start_467', 'start_468', 'start_469', 'start_470', 'start_471', 'start_472', 'start_473', 'start_474', 'start_475', 'start_476', 'start_477', 'start_478', 'start_479', 'start_480', 'start_481', 'start_482', 'start_483', 'start_484', 'start_485', 'start_486', 'start_487', 'start_488', 'start_489', 'start_490', 'start_491', 'start_492', 'start_493', 'start_494', 'start_495', 'start_496', 'start_497', 'start_498', 'start_499', 'start_500', 'start_501', 'start_502', 'start_503', 'start_504', 'start_505', 'start_506', 'start_507', 'start_508', 'start_509', 'start_510', 'start_511', 'start_512', 'start_513', 'start_514', 'start_515', 'start_516', 'start_517', 'start_518', 'start_519', 'start_520', 'start_521', 'start_522', 'start_523', 'start_524', 'start_525', 'start_526', 'start_527', 'start_528', 'start_529', 'start_530', 'start_531', 'start_532', 'start_533', 'start_534', 'start_535', 'start_536', 'start_537', 'start_538', 'start_539', 'start_540', 'start_541', 'start_542', 'start_543', 'start_544', 'start_545', 'start_546', 'start_547', 'start_548', 'start_549', 'start_550', 'start_551', 'start_552', 'start_553', 'start_554', 'start_555', 'start_556', 'start_557', 'start_558', 'start_559', 'start_560', 'start_561', 'start_562', 'start_563', 'start_564', 'start_565', 'start_566', 'start_567', 'start_568', 'start_569', 'start_570', 'start_571', 'start_572', 'start_573', 'start_574', 'start_575', 'start_576', 'start_577', 'start_578', 'start_579', 'start_580', 'start_581', 'start_582', 'start_583', 'start_584', 'start_585', 'start_586', 'start_587', 'start_588', 'start_589', 'start_590', 'start_591', 'start_592', 'start_593', 'start_594', 'start_595', 'start_596', 'start_597', 'start_598', 'start_599', 'start_600', 'start_601', 'start_602', 'start_603', 'start_604', 'start_605', 'start_606', 'start_607', 'start_608', 'start_609', 'start_610', 'start_611', 'start_612', 'start_613', 'start_614', 'start_615', 'start_616', 'start_617', 'start_618', 'start_619', 'start_620', 'start_621', 'start_622', 'start_623']","[""normalize cell values to range [0, 1]"", ""reshape data to fit model input requirements""]","model = BestGuessModule()
model.compile(optimizer='adam', loss='binary_crossentropy')
model.fit(train_data, train_labels, epochs=10, batch_size=32)",DL,Pytorch,markuskarmann/3rd-place-solution-part,https://www.kaggle.com/markuskarmann/3rd-place-solution-part
conways-reverse-game-of-life-2020,classification,binary-classification,The task is to predict the starting board state of Conway's Game of Life based on the stopping board state after a specified number of steps.,Tabular,MAE – Mean Absolute Error,"The dataset consists of 50,000 training games and 50,000 test games, each represented as a 25x25 board with 625 cells. Each game has a unique identifier and includes variables for the number of steps between the start and stop boards, as well as the state of each cell in both the starting and stopping boards. The starting board is recorded after five warmup steps, and the stopping board is recorded after a random number of steps between one and five. The initial board is generated with a random density of live cells.","['start_0', 'start_1', 'start_2', 'start_3', 'start_4', 'start_5', 'start_6', 'start_7', 'start_8', 'start_9', 'start_10', 'start_11', 'start_12', 'start_13', 'start_14', 'start_15', 'start_16', 'start_17', 'start_18', 'start_19', 'start_20', 'start_21', 'start_22', 'start_23', 'start_24', 'start_25', 'start_26', 'start_27', 'start_28', 'start_29', 'start_30', 'start_31', 'start_32', 'start_33', 'start_34', 'start_35', 'start_36', 'start_37', 'start_38', 'start_39', 'start_40', 'start_41', 'start_42', 'start_43', 'start_44', 'start_45', 'start_46', 'start_47', 'start_48', 'start_49', 'start_50', 'start_51', 'start_52', 'start_53', 'start_54', 'start_55', 'start_56', 'start_57', 'start_58', 'start_59', 'start_60', 'start_61', 'start_62', 'start_63', 'start_64', 'start_65', 'start_66', 'start_67', 'start_68', 'start_69', 'start_70', 'start_71', 'start_72', 'start_73', 'start_74', 'start_75', 'start_76', 'start_77', 'start_78', 'start_79', 'start_80', 'start_81', 'start_82', 'start_83', 'start_84', 'start_85', 'start_86', 'start_87', 'start_88', 'start_89', 'start_90', 'start_91', 'start_92', 'start_93', 'start_94', 'start_95', 'start_96', 'start_97', 'start_98', 'start_99', 'start_100', 'start_101', 'start_102', 'start_103', 'start_104', 'start_105', 'start_106', 'start_107', 'start_108', 'start_109', 'start_110', 'start_111', 'start_112', 'start_113', 'start_114', 'start_115', 'start_116', 'start_117', 'start_118', 'start_119', 'start_120', 'start_121', 'start_122', 'start_123', 'start_124', 'start_125', 'start_126', 'start_127', 'start_128', 'start_129', 'start_130', 'start_131', 'start_132', 'start_133', 'start_134', 'start_135', 'start_136', 'start_137', 'start_138', 'start_139', 'start_140', 'start_141', 'start_142', 'start_143', 'start_144', 'start_145', 'start_146', 'start_147', 'start_148', 'start_149', 'start_150', 'start_151', 'start_152', 'start_153', 'start_154', 'start_155', 'start_156', 'start_157', 'start_158', 'start_159', 'start_160', 'start_161', 'start_162', 'start_163', 'start_164', 'start_165', 'start_166', 'start_167', 'start_168', 'start_169', 'start_170', 'start_171', 'start_172', 'start_173', 'start_174', 'start_175', 'start_176', 'start_177', 'start_178', 'start_179', 'start_180', 'start_181', 'start_182', 'start_183', 'start_184', 'start_185', 'start_186', 'start_187', 'start_188', 'start_189', 'start_190', 'start_191', 'start_192', 'start_193', 'start_194', 'start_195', 'start_196', 'start_197', 'start_198', 'start_199', 'start_200', 'start_201', 'start_202', 'start_203', 'start_204', 'start_205', 'start_206', 'start_207', 'start_208', 'start_209', 'start_210', 'start_211', 'start_212', 'start_213', 'start_214', 'start_215', 'start_216', 'start_217', 'start_218', 'start_219', 'start_220', 'start_221', 'start_222', 'start_223', 'start_224', 'start_225', 'start_226', 'start_227', 'start_228', 'start_229', 'start_230', 'start_231', 'start_232', 'start_233', 'start_234', 'start_235', 'start_236', 'start_237', 'start_238', 'start_239', 'start_240', 'start_241', 'start_242', 'start_243', 'start_244', 'start_245', 'start_246', 'start_247', 'start_248', 'start_249', 'start_250', 'start_251', 'start_252', 'start_253', 'start_254', 'start_255', 'start_256', 'start_257', 'start_258', 'start_259', 'start_260', 'start_261', 'start_262', 'start_263', 'start_264', 'start_265', 'start_266', 'start_267', 'start_268', 'start_269', 'start_270', 'start_271', 'start_272', 'start_273', 'start_274', 'start_275', 'start_276', 'start_277', 'start_278', 'start_279', 'start_280', 'start_281', 'start_282', 'start_283', 'start_284', 'start_285', 'start_286', 'start_287', 'start_288', 'start_289', 'start_290', 'start_291', 'start_292', 'start_293', 'start_294', 'start_295', 'start_296', 'start_297', 'start_298', 'start_299', 'start_300', 'start_301', 'start_302', 'start_303', 'start_304', 'start_305', 'start_306', 'start_307', 'start_308', 'start_309', 'start_310', 'start_311', 'start_312', 'start_313', 'start_314', 'start_315', 'start_316', 'start_317', 'start_318', 'start_319', 'start_320', 'start_321', 'start_322', 'start_323', 'start_324', 'start_325', 'start_326', 'start_327', 'start_328', 'start_329', 'start_330', 'start_331', 'start_332', 'start_333', 'start_334', 'start_335', 'start_336', 'start_337', 'start_338', 'start_339', 'start_340', 'start_341', 'start_342', 'start_343', 'start_344', 'start_345', 'start_346', 'start_347', 'start_348', 'start_349', 'start_350', 'start_351', 'start_352', 'start_353', 'start_354', 'start_355', 'start_356', 'start_357', 'start_358', 'start_359', 'start_360', 'start_361', 'start_362', 'start_363', 'start_364', 'start_365', 'start_366', 'start_367', 'start_368', 'start_369', 'start_370', 'start_371', 'start_372', 'start_373', 'start_374', 'start_375', 'start_376', 'start_377', 'start_378', 'start_379', 'start_380', 'start_381', 'start_382', 'start_383', 'start_384', 'start_385', 'start_386', 'start_387', 'start_388', 'start_389', 'start_390', 'start_391', 'start_392', 'start_393', 'start_394', 'start_395', 'start_396', 'start_397', 'start_398', 'start_399', 'start_400', 'start_401', 'start_402', 'start_403', 'start_404', 'start_405', 'start_406', 'start_407', 'start_408', 'start_409', 'start_410', 'start_411', 'start_412', 'start_413', 'start_414', 'start_415', 'start_416', 'start_417', 'start_418', 'start_419', 'start_420', 'start_421', 'start_422', 'start_423', 'start_424', 'start_425', 'start_426', 'start_427', 'start_428', 'start_429', 'start_430', 'start_431', 'start_432', 'start_433', 'start_434', 'start_435', 'start_436', 'start_437', 'start_438', 'start_439', 'start_440', 'start_441', 'start_442', 'start_443', 'start_444', 'start_445', 'start_446', 'start_447', 'start_448', 'start_449', 'start_450', 'start_451', 'start_452', 'start_453', 'start_454', 'start_455', 'start_456', 'start_457', 'start_458', 'start_459', 'start_460', 'start_461', 'start_462', 'start_463', 'start_464', 'start_465', 'start_466', 'start_467', 'start_468', 'start_469', 'start_470', 'start_471', 'start_472', 'start_473', 'start_474', 'start_475', 'start_476', 'start_477', 'start_478', 'start_479', 'start_480', 'start_481', 'start_482', 'start_483', 'start_484', 'start_485', 'start_486', 'start_487', 'start_488', 'start_489', 'start_490', 'start_491', 'start_492', 'start_493', 'start_494', 'start_495', 'start_496', 'start_497', 'start_498', 'start_499', 'start_500', 'start_501', 'start_502', 'start_503', 'start_504', 'start_505', 'start_506', 'start_507', 'start_508', 'start_509', 'start_510', 'start_511', 'start_512', 'start_513', 'start_514', 'start_515', 'start_516', 'start_517', 'start_518', 'start_519', 'start_520', 'start_521', 'start_522', 'start_523', 'start_524', 'start_525', 'start_526', 'start_527', 'start_528', 'start_529', 'start_530', 'start_531', 'start_532', 'start_533', 'start_534', 'start_535', 'start_536', 'start_537', 'start_538', 'start_539', 'start_540', 'start_541', 'start_542', 'start_543', 'start_544', 'start_545', 'start_546', 'start_547', 'start_548', 'start_549', 'start_550', 'start_551', 'start_552', 'start_553', 'start_554', 'start_555', 'start_556', 'start_557', 'start_558', 'start_559', 'start_560', 'start_561', 'start_562', 'start_563', 'start_564', 'start_565', 'start_566', 'start_567', 'start_568', 'start_569', 'start_570', 'start_571', 'start_572', 'start_573', 'start_574', 'start_575', 'start_576', 'start_577', 'start_578', 'start_579', 'start_580', 'start_581', 'start_582', 'start_583', 'start_584', 'start_585', 'start_586', 'start_587', 'start_588', 'start_589', 'start_590', 'start_591', 'start_592', 'start_593', 'start_594', 'start_595', 'start_596', 'start_597', 'start_598', 'start_599', 'start_600', 'start_601', 'start_602', 'start_603', 'start_604', 'start_605', 'start_606', 'start_607', 'start_608', 'start_609', 'start_610', 'start_611', 'start_612', 'start_613', 'start_614', 'start_615', 'start_616', 'start_617', 'start_618', 'start_619', 'start_620', 'start_621', 'start_622', 'start_623', 'start_624']","[""reshape data to 25x25 format"", ""normalize cell values to binary (0 or 1)""]","self.conv1   = nn.Conv2d(in_channels=1,     out_channels=128, kernel_size=(3,3), padding=1, padding_mode='circular')
self.conv2   = nn.Conv2d(in_channels=128,   out_channels=16,  kernel_size=(1,1))
self.conv3   = nn.Conv2d(in_channels=16,    out_channels=8,   kernel_size=(1,1))
self.conv4   = nn.Conv2d(in_channels=1+8,   out_channels=1,   kernel_size=(1,1))
optimizer = optim.RMSprop(model.parameters(), lr=0.01, momentum=0.9)
outputs = model(inputs)
loss = model.loss(outputs, expected, inputs)",DL,Pytorch,jamesmcguigan/pytorch-game-of-life-first-attempt,https://www.kaggle.com/jamesmcguigan/pytorch-game-of-life-first-attempt
conways-reverse-game-of-life-2020,classification,binary-classification,"The task is to predict the starting board state of a game based on the stopping board state after a specified number of steps, where each cell can be either alive or dead.",Tabular,MAE – Mean Absolute Error,"The dataset consists of 50,000 training games and 50,000 test games, each represented as a 25x25 board with 625 cells. The training data includes unique identifiers, the number of steps between the start and stop boards, and the state of the board at both the start and stop. The initial board is generated with a random density of alive cells, evolved for a few steps, and the resulting states are recorded for prediction.","['start_0', 'start_1', 'start_2', 'start_3', 'start_4', 'start_5', 'start_6', 'start_7', 'start_8', 'start_9', 'start_10', 'start_11', 'start_12', 'start_13', 'start_14', 'start_15', 'start_16', 'start_17', 'start_18', 'start_19', 'start_20', 'start_21', 'start_22', 'start_23', 'start_24', 'start_25', 'start_26', 'start_27', 'start_28', 'start_29', 'start_30', 'start_31', 'start_32', 'start_33', 'start_34', 'start_35', 'start_36', 'start_37', 'start_38', 'start_39', 'start_40', 'start_41', 'start_42', 'start_43', 'start_44', 'start_45', 'start_46', 'start_47', 'start_48', 'start_49', 'start_50', 'start_51', 'start_52', 'start_53', 'start_54', 'start_55', 'start_56', 'start_57', 'start_58', 'start_59', 'start_60', 'start_61', 'start_62', 'start_63', 'start_64', 'start_65', 'start_66', 'start_67', 'start_68', 'start_69', 'start_70', 'start_71', 'start_72', 'start_73', 'start_74', 'start_75', 'start_76', 'start_77', 'start_78', 'start_79', 'start_80', 'start_81', 'start_82', 'start_83', 'start_84', 'start_85', 'start_86', 'start_87', 'start_88', 'start_89', 'start_90', 'start_91', 'start_92', 'start_93', 'start_94', 'start_95', 'start_96', 'start_97', 'start_98', 'start_99', 'start_100', 'start_101', 'start_102', 'start_103', 'start_104', 'start_105', 'start_106', 'start_107', 'start_108', 'start_109', 'start_110', 'start_111', 'start_112', 'start_113', 'start_114', 'start_115', 'start_116', 'start_117', 'start_118', 'start_119', 'start_120', 'start_121', 'start_122', 'start_123', 'start_124', 'start_125', 'start_126', 'start_127', 'start_128', 'start_129', 'start_130', 'start_131', 'start_132', 'start_133', 'start_134', 'start_135', 'start_136', 'start_137', 'start_138', 'start_139', 'start_140', 'start_141', 'start_142', 'start_143', 'start_144', 'start_145', 'start_146', 'start_147', 'start_148', 'start_149', 'start_150', 'start_151', 'start_152', 'start_153', 'start_154', 'start_155', 'start_156', 'start_157', 'start_158', 'start_159', 'start_160', 'start_161', 'start_162', 'start_163', 'start_164', 'start_165', 'start_166', 'start_167', 'start_168', 'start_169', 'start_170', 'start_171', 'start_172', 'start_173', 'start_174', 'start_175', 'start_176', 'start_177', 'start_178', 'start_179', 'start_180', 'start_181', 'start_182', 'start_183', 'start_184', 'start_185', 'start_186', 'start_187', 'start_188', 'start_189', 'start_190', 'start_191', 'start_192', 'start_193', 'start_194', 'start_195', 'start_196', 'start_197', 'start_198', 'start_199', 'start_200', 'start_201', 'start_202', 'start_203', 'start_204', 'start_205', 'start_206', 'start_207', 'start_208', 'start_209', 'start_210', 'start_211', 'start_212', 'start_213', 'start_214', 'start_215', 'start_216', 'start_217', 'start_218', 'start_219', 'start_220', 'start_221', 'start_222', 'start_223', 'start_224', 'start_225', 'start_226', 'start_227', 'start_228', 'start_229', 'start_230', 'start_231', 'start_232', 'start_233', 'start_234', 'start_235', 'start_236', 'start_237', 'start_238', 'start_239', 'start_240', 'start_241', 'start_242', 'start_243', 'start_244', 'start_245', 'start_246', 'start_247', 'start_248', 'start_249', 'start_250', 'start_251', 'start_252', 'start_253', 'start_254', 'start_255', 'start_256', 'start_257', 'start_258', 'start_259', 'start_260', 'start_261', 'start_262', 'start_263', 'start_264', 'start_265', 'start_266', 'start_267', 'start_268', 'start_269', 'start_270', 'start_271', 'start_272', 'start_273', 'start_274', 'start_275', 'start_276', 'start_277', 'start_278', 'start_279', 'start_280', 'start_281', 'start_282', 'start_283', 'start_284', 'start_285', 'start_286', 'start_287', 'start_288', 'start_289', 'start_290', 'start_291', 'start_292', 'start_293', 'start_294', 'start_295', 'start_296', 'start_297', 'start_298', 'start_299', 'start_300', 'start_301', 'start_302', 'start_303', 'start_304', 'start_305', 'start_306', 'start_307', 'start_308', 'start_309', 'start_310', 'start_311', 'start_312', 'start_313', 'start_314', 'start_315', 'start_316', 'start_317', 'start_318', 'start_319', 'start_320', 'start_321', 'start_322', 'start_323', 'start_324', 'start_325', 'start_326', 'start_327', 'start_328', 'start_329', 'start_330', 'start_331', 'start_332', 'start_333', 'start_334', 'start_335', 'start_336', 'start_337', 'start_338', 'start_339', 'start_340', 'start_341', 'start_342', 'start_343', 'start_344', 'start_345', 'start_346', 'start_347', 'start_348', 'start_349', 'start_350', 'start_351', 'start_352', 'start_353', 'start_354', 'start_355', 'start_356', 'start_357', 'start_358', 'start_359', 'start_360', 'start_361', 'start_362', 'start_363', 'start_364', 'start_365', 'start_366', 'start_367', 'start_368', 'start_369', 'start_370', 'start_371', 'start_372', 'start_373', 'start_374', 'start_375', 'start_376', 'start_377', 'start_378', 'start_379', 'start_380', 'start_381', 'start_382', 'start_383', 'start_384', 'start_385', 'start_386', 'start_387', 'start_388', 'start_389', 'start_390', 'start_391', 'start_392', 'start_393', 'start_394', 'start_395', 'start_396', 'start_397', 'start_398', 'start_399', 'start_400', 'start_401', 'start_402', 'start_403', 'start_404', 'start_405', 'start_406', 'start_407', 'start_408', 'start_409', 'start_410', 'start_411', 'start_412', 'start_413', 'start_414', 'start_415', 'start_416', 'start_417', 'start_418', 'start_419', 'start_420', 'start_421', 'start_422', 'start_423', 'start_424', 'start_425', 'start_426', 'start_427', 'start_428', 'start_429', 'start_430', 'start_431', 'start_432', 'start_433', 'start_434', 'start_435', 'start_436', 'start_437', 'start_438', 'start_439', 'start_440', 'start_441', 'start_442', 'start_443', 'start_444', 'start_445', 'start_446', 'start_447', 'start_448', 'start_449', 'start_450', 'start_451', 'start_452', 'start_453', 'start_454', 'start_455', 'start_456', 'start_457', 'start_458', 'start_459', 'start_460', 'start_461', 'start_462', 'start_463', 'start_464', 'start_465', 'start_466', 'start_467', 'start_468', 'start_469', 'start_470', 'start_471', 'start_472', 'start_473', 'start_474', 'start_475', 'start_476', 'start_477', 'start_478', 'start_479', 'start_480', 'start_481', 'start_482', 'start_483', 'start_484', 'start_485', 'start_486', 'start_487', 'start_488', 'start_489', 'start_490', 'start_491', 'start_492', 'start_493', 'start_494', 'start_495', 'start_496', 'start_497', 'start_498', 'start_499', 'start_500', 'start_501', 'start_502', 'start_503', 'start_504', 'start_505', 'start_506', 'start_507', 'start_508', 'start_509', 'start_510', 'start_511', 'start_512', 'start_513', 'start_514', 'start_515', 'start_516', 'start_517', 'start_518', 'start_519', 'start_520', 'start_521', 'start_522', 'start_523', 'start_524', 'start_525', 'start_526', 'start_527', 'start_528', 'start_529', 'start_530', 'start_531', 'start_532', 'start_533', 'start_534', 'start_535', 'start_536', 'start_537', 'start_538', 'start_539', 'start_540', 'start_541', 'start_542', 'start_543', 'start_544', 'start_545', 'start_546', 'start_547', 'start_548', 'start_549', 'start_550', 'start_551', 'start_552', 'start_553', 'start_554', 'start_555', 'start_556', 'start_557', 'start_558', 'start_559', 'start_560', 'start_561', 'start_562', 'start_563', 'start_564', 'start_565', 'start_566', 'start_567', 'start_568', 'start_569', 'start_570', 'start_571', 'start_572', 'start_573', 'start_574', 'start_575', 'start_576', 'start_577', 'start_578', 'start_579', 'start_580', 'start_581', 'start_582', 'start_583', 'start_584', 'start_585', 'start_586', 'start_587', 'start_588', 'start_589', 'start_590', 'start_591', 'start_592', 'start_593', 'start_594', 'start_595', 'start_596', 'start_597', 'start_598', 'start_599', 'start_600', 'start_601', 'start_602', 'start_603', 'start_604', 'start_605', 'start_606', 'start_607', 'start_608', 'start_609', 'start_610', 'start_611', 'start_612', 'start_613', 'start_614', 'start_615', 'start_616', 'start_617', 'start_618', 'start_619', 'start_620', 'start_621', 'start_622', 'start_623']","[""reshape data to 2D arrays"", ""normalize cell values to binary (0 or 1)""]","model = GameOfLifeHardcodedReLU1_21()
model.load()
model.to(device)
model.train()

# Training loop
for epoch in range(num_epochs):
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()",DL,Pytorch,jamesmcguigan/pytorch-game-of-life-hardcoding-network-weights,https://www.kaggle.com/jamesmcguigan/pytorch-game-of-life-hardcoding-network-weights
Kannada-MNIST,classification,multiclass-classification,The task is to classify gray-scale images of hand-drawn digits in the Kannada script into their respective digit categories from zero to nine.,Tabular,Categorization accuracy,"The dataset consists of gray-scale images of hand-drawn digits in the Kannada script, with each image being 28x28 pixels, resulting in 784 pixel values per image. The training set contains 60,000 images, each labeled with the corresponding digit. The test set is similar but lacks labels. The pixel values range from 0 to 255, indicating the lightness or darkness of each pixel. The dataset is structured in CSV format with pixel values in columns named pixel{x}, where x is an integer from 0 to 783, and the first column is the label.",['label'],"[""normalizing pixel values to a 0-1 scale"", ""splitting the dataset into training and validation sets"", ""reshaping the data to fit the model input requirements""]","model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(64, (3,3), padding='same', input_shape=(28, 28, 1)),
    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.Conv2D(64,  (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.Conv2D(64,  (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Conv2D(128, (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.Conv2D(128, (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.Conv2D(128, (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Conv2D(256, (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.Conv2D(256, (3,3), padding='same'),
    tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5, gamma_initializer=""uniform""),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256),
    tf.keras.layers.LeakyReLU(alpha=0.1),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(10, activation='softmax')
])

optimizer = RMSprop(learning_rate=0.002,
    rho=0.9,
    momentum=0.1,
    epsilon=1e-07,
    centered=True,
    name='RMSprop')
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])
history = model.fit_generator(datagen_train.flow(x_train, y_train, batch_size=batch_size),
                              steps_per_epoch=len(x_train)//batch_size,
                              epochs=epochs,
                              validation_data=(x_val, y_val),
                              validation_steps=50,
                              callbacks=[learning_rate_reduction, es],
                              verbose=2)",DL,Tensorflow,benanakca/kannada-mnist-cnn-tutorial-with-app-top-2,https://www.kaggle.com/benanakca/kannada-mnist-cnn-tutorial-with-app-top-2
Kannada-MNIST,classification,multiclass-classification,The task is to classify gray-scale images of hand-drawn digits in the Kannada script into their respective digit categories from zero to nine.,Tabular,Categorization accuracy,"The dataset consists of gray-scale images of hand-drawn digits in the Kannada script, with each image sized 28x28 pixels. The training set contains 785 columns, where the first column is the label (the digit) and the remaining columns represent pixel values ranging from 0 to 255. The test set has the same structure but lacks the label column. The evaluation metric is categorization accuracy, indicating the percentage of correctly classified images.",['label'],"[""reshape images to 28x28x1"", ""convert labels to categorical format"", ""normalize pixel values to range [0, 1]""]","model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', input_shape=(28, 28, 1)),\n    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=""uniform""),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(64,  (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=""uniform""),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=""uniform""),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=""uniform""),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv2D(256, (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=""uniform""),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(256, (3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=""uniform""),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(loss=""categorical_crossentropy"", optimizer=RMSprop(lr=initial_learningrate), metrics=['accuracy'])\nhistory = model.fit_generator(train_datagen.flow(X_train,Y_train, batch_size=batch_size), steps_per_epoch=100, epochs=epochs, callbacks=[LearningRateScheduler(lr_decay), es], validation_data=valid_datagen.flow(X_valid,Y_valid), validation_steps=50, verbose=2)",DL,Tensorflow,bustam/cnn-in-keras-for-kannada-digits,https://www.kaggle.com/bustam/cnn-in-keras-for-kannada-digits
Kannada-MNIST,classification,multiclass-classification,"The task is to classify gray-scale images of hand-drawn digits in the Kannada script into one of ten categories, representing the digits from zero to nine.",Tabular,Categorization accuracy,"The dataset consists of gray-scale images of hand-drawn digits in the Kannada script, with each image being 28x28 pixels, totaling 784 pixels. The training set contains 785 columns, with the first column as the label indicating the digit and the remaining columns representing pixel values ranging from 0 to 255. The test set mirrors the training set but lacks the label column. The evaluation metric is categorization accuracy, measuring the proportion of correctly classified images.",['label'],"[""normalize pixel values to a range of 0 to 1"", ""reshape images to include a channel dimension"", ""split training data into training and validation sets""]","cnn_model = Sequential()
cnn_model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(28,28,1), activation='relu', padding='same'))
cnn_model.add(BatchNormalization())
cnn_model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))
cnn_model.add(BatchNormalization())
cnn_model.add(MaxPooling2D(pool_size=(2, 2)))
cnn_model.add(Dropout(0.2))
cnn_model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))
cnn_model.add(BatchNormalization())
cnn_model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))
cnn_model.add(BatchNormalization())
cnn_model.add(MaxPooling2D(pool_size=(2, 2)))
cnn_model.add(Dropout(0.2))
cnn_model.add(Flatten())
cnn_model.add(Dense(units=128, activation='relu'))
cnn_model.add(Dropout(0.2))
cnn_model.add(Dense(units=10, activation='softmax'))
cnn_model.compile(loss ='sparse_categorical_crossentropy', optimizer='adam' ,metrics=['accuracy'])
history = cnn_model.fit(X_train, y_train, batch_size=512, epochs=50, verbose=1, validation_data=(X_validate, y_validate))",DL,Tensorflow,faressayah/fashion-mnist-classification-using-cnns,https://www.kaggle.com/faressayah/fashion-mnist-classification-using-cnns
Kannada-MNIST,classification,multiclass-classification,The task is to classify gray-scale images of hand-drawn digits in the Kannada script into their respective digit categories from zero to nine.,Image,Categorization accuracy,"The dataset consists of gray-scale images of hand-drawn digits in the Kannada script, with each image being 28x28 pixels, totaling 784 pixels. The training set contains 785 columns, where the first column is the label (the digit) and the remaining columns represent pixel values ranging from 0 to 255. The test set mirrors the training set but lacks the label column. The evaluation metric is categorization accuracy, indicating the percentage of correctly classified images.",['label'],"[""reshape images to add a channel dimension"", ""normalize pixel values to the range 0-1""]","model = Sequential()
model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', input_shape=in_shape))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
model.add(Dropout(0.5))
model.add(Dense(n_classes, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=0)",DL,Tensorflow,faressayah/tensorflow-2-tutorial-get-started-in-deep-learning,https://www.kaggle.com/faressayah/tensorflow-2-tutorial-get-started-in-deep-learning
Kannada-MNIST,classification,multiclass-classification,The goal is to classify hand-drawn digits written in the Kannada script into their respective categories from zero to nine.,Tabular,Categorization accuracy,"The dataset consists of gray-scale images of hand-drawn digits in the Kannada script, with each image being 28x28 pixels. The training set contains 785 columns, where the first column is the label (the digit) and the remaining columns represent pixel values ranging from 0 to 255. The test set has the same structure but lacks the label column. The evaluation metric is categorization accuracy, which measures the proportion of correctly classified images.",['label'],"[""remove label column from training data"", ""remove id column from test data"", ""normalize pixel values to range 0-1"", ""reshape images to 28x28x1 format""]","model.add(Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 1), input_shape=input_shape, padding=""same"", activation='relu', kernel_initializer=my_init))
model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))
model.add(Dropout(0.25))
model.add(Conv2D(filters=128, kernel_size=(5, 5), strides=(1, 1), padding=""same"", activation='relu', kernel_initializer=my_init))
model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))
model.add(Dropout(0.25))
model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding=""same"", activation='relu', kernel_initializer=my_init))
model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(256, kernel_initializer=my_init))
model.add(Activation(my_activ))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(128, kernel_initializer=my_init))
model.add(Activation(my_activ))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(nb_classes, activation='softmax'))
model.compile(optimizer=my_optimiser, loss='categorical_crossentropy', metrics=['accuracy'])
history = nnet_model.fit_generator(datagen.flow(X_train_f,y_train_f, batch_size=my_batch_size), epochs=n_epochs, verbose=my_verbose, steps_per_epoch=X_train.shape[0] // my_batch_size, validation_data=(X_val_f,y_val_f), callbacks=[learning_rate_reduction])",DL,Tensorflow,datahobbit/cnn-for-digit-recognition-in-the-kannada-script,https://www.kaggle.com/datahobbit/cnn-for-digit-recognition-in-the-kannada-script
Kannada-MNIST,classification,multiclass-classification,"The task is to classify gray-scale images of hand-drawn digits in the Kannada script into one of ten categories, corresponding to the digits zero through nine.",Tabular,Categorization accuracy,"The dataset consists of gray-scale images of hand-drawn digits in the Kannada script, with each image being 28x28 pixels. The training set contains 785 columns, where the first column is the label (the digit) and the remaining columns represent pixel values. Each pixel value ranges from 0 to 255, indicating the lightness or darkness of the pixel. The test set is similar but lacks the label column. The evaluation metric is categorization accuracy, which measures the proportion of correctly classified images.",['label'],"[""reshape images to 28x28 pixels"", ""convert pixel values to tensors"", ""apply random horizontal flip for training images"", ""normalize pixel values""]","model = CNN()
model = model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(params=model.parameters(), lr=0.001, momentum=0.9)
epochs = 10
model.train()
for epoch in range(epochs):
    epoch_loss = 0
    for data, label in train_loader:
        data = data.to(device)
        label = label.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()
        epoch_loss += (loss.item())/len(train_loader)
    print('Epoch : {},  train loss : {}'.format(epoch+1,epoch_loss))
    model.eval()
    with torch.no_grad():
        epoch_val_loss = 0
        for data, label in val_loader:
            data = data.to(device)
            label = label.to(device)
            output = model(data)
            val_loss = criterion(output,label)
            epoch_val_loss += val_loss/ len(val_loader)
        print('Epoch : {} , val_loss : {}'.format(epoch+1,epoch_val_loss))",DL,Pytorch,akshitsharma1/kannada-mnist-0-97-easy-detailed-cnn-tutorial,https://www.kaggle.com/akshitsharma1/kannada-mnist-0-97-easy-detailed-cnn-tutorial
Kannada-MNIST,classification,multiclass-classification,The task is to classify gray-scale images of hand-drawn digits in the Kannada script into their respective digit categories from zero to nine.,Tabular,Categorization accuracy,"The dataset consists of gray-scale images of hand-drawn digits in the Kannada script, with each image being 28x28 pixels, resulting in 784 pixel values per image. The training set contains 785 columns, where the first column is the label (the digit) and the remaining columns represent the pixel values. The test set has the same structure but lacks the label column. The evaluation metric is categorization accuracy, which measures the proportion of correctly classified images.",['label'],"[""reshape pixel values into 28x28 images"", ""split training data into training and validation sets"", ""create dataset objects for training and validation""]","class ConvNet1(nn.Module):
    def __init__(self):
        super(ConvNet1, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(32)
        )
        self.layer2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(32)
        )
        self.layer3 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=1, stride=2),
            nn.ReLU(),
            nn.BatchNorm2d(64)        
        )
        self.layer4 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(64)
        )
        self.layer5 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(64)
        )
        self.layer6 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=1, stride=2),
            nn.ReLU(),
            nn.BatchNorm2d(128),
        )
        
        self.drop_out = nn.Dropout(0.2)
        self.relu = nn.ReLU()

        self.fc1 = nn.Linear(4608, 128)
        self.fc2 = nn.Linear(128, 10)
        self.bn1d = nn.BatchNorm1d(128)
        self.output = nn.Softmax(dim=1)
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = self.drop_out(self.layer3(out))
        out = self.layer4(out)
        out = self.layer5(out)
        out = self.drop_out(self.layer6(out))
        out = out.view(out.shape[0], -1)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.bn1d(out)
        out = self.drop_out(out)
        out = self.fc2(out)
        out = self.output(out)
        return out

learner1 = Learner(data, 
                  conv_net_1, 
                  metrics=accuracy, 
                  model_dir=MODEL_DIR,
                  opt_func=Adam,
                  loss_func=nn.CrossEntropyLoss()
                 )
learner1.fit_one_cycle(50, 
                      slice(1e-03),
                      callbacks=[SaveModelCallback(learner1, 
                                                   every='improvement', 
                                                   monitor='accuracy', 
                                                   name='best_model_1')]
                     )",DL,Pytorch,abhinand05/k-mnist-ensemble-of-4-models-lb-0-985-starterkit,https://www.kaggle.com/abhinand05/k-mnist-ensemble-of-4-models-lb-0-985-starterkit
Kannada-MNIST,classification,multiclass-classification,"The task is to classify gray-scale images of hand-drawn digits in the Kannada script into one of ten categories, corresponding to the digits zero through nine.",Tabular,Categorization accuracy,"The dataset consists of gray-scale images of hand-drawn digits in the Kannada script, with each image being 28x28 pixels. The training set contains 785 columns, where the first column is the label (the digit) and the remaining columns represent pixel values ranging from 0 to 255. The test set has the same structure but lacks the label column. The evaluation metric is categorization accuracy, which measures the proportion of correctly classified images.",['label'],"[""split the dataset into training and validation sets"", ""normalize pixel values to a range of 0 to 1"", ""apply data augmentation techniques to the training set""]","model = Net(10)
model = model.to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=n_epochs // 4, gamma=0.1)",DL,Pytorch,hocop1/manifold-mixup-using-pytorch,https://www.kaggle.com/hocop1/manifold-mixup-using-pytorch
Kannada-MNIST,classification,multiclass-classification,The task is to classify gray-scale images of hand-drawn digits in the Kannada script into their respective digit categories from zero to nine.,Tabular,Categorization accuracy,"The dataset consists of gray-scale images of hand-drawn digits in the Kannada script, with each image being 28x28 pixels. The training set contains 785 columns, where the first column is the label (the digit) and the remaining columns represent pixel values ranging from 0 to 255. The test set mirrors the training set but lacks the label column. The evaluation metric is categorization accuracy, which measures the proportion of correctly classified images.",['label'],"[""normalize pixel values by dividing by 255"", ""split the dataset into training and testing sets""]","model = CNNModel()
error = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        train = Variable(images.view(-1, 1, 28, 28))
        labels = Variable(labels)
        optimizer.zero_grad()
        outputs = model(train)
        loss = error(outputs, labels)
        loss.backward()
        optimizer.step()",DL,Pytorch,subinium/shap-deep-explainer-pytorch-ver,https://www.kaggle.com/subinium/shap-deep-explainer-pytorch-ver
Kannada-MNIST,classification,multiclass-classification,The task is to classify gray-scale images of hand-drawn digits in the Kannada script into their respective digit categories from zero to nine.,Image,Categorization accuracy,"The dataset consists of gray-scale images of hand-drawn digits in the Kannada script, with each image being 28x28 pixels. The training set contains 785 columns, where the first column is the label (the digit) and the remaining columns represent pixel values ranging from 0 to 255. The test set is similar but lacks the label column. The evaluation metric is categorization accuracy, which measures the percentage of correctly classified images.",['label'],"[""normalize pixel values to range 0-1"", ""apply random rotation to images"", ""apply random cropping to images"", ""convert images to tensor format"", ""normalize images using calculated mean and std""]","model = MLP(INPUT_DIM, OUTPUT_DIM)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())
model.train()
for (x, y) in iterator:
    y_pred, _ = model(x)
    loss = criterion(y_pred, y)
    loss.backward()
    optimizer.step()",DL,Pytorch,darthmanav/multilayer-perceptron-fine-tuning-pca-t-sne-mnist,https://www.kaggle.com/darthmanav/multilayer-perceptron-fine-tuning-pca-t-sne-mnist
sf-crime,classification,multiclass-classification,The task is to predict the category of crime incidents based on various features derived from the SFPD Crime Incident Reporting system.,Tabular,Multi-class Log Loss,"This dataset contains incidents derived from the SFPD Crime Incident Reporting system, covering data from January 1, 2003, to May 13, 2015. The training and test sets rotate weekly, with odd weeks designated for testing and even weeks for training. Key fields include the timestamp of the crime, the category of the crime (target variable), detailed descriptions, day of the week, police district, resolution status, approximate address, and geographical coordinates (longitude and latitude).",['Category'],"[""extract year from Dates"", ""extract month from Dates"", ""extract day from Dates"", ""convert Dates to time in minutes"", ""one-hot encode DayOfWeek"", ""one-hot encode PdDistrict"", ""drop unnecessary columns from training data"", ""drop Id and Address from test data""]","model = tf.keras.models.Sequential([
                                      tf.keras.layers.Dense(100, input_shape=(X.shape[1],)),
                                      tf.keras.layers.Dense(80, activation='relu'),
                                      tf.keras.layers.Dense(64, activation='relu'),
                                      tf.keras.layers.Dense(64, activation='relu'),
                                      tf.keras.layers.Dense(64,activation='relu'),
                                      tf.keras.layers.Dense(64, activation='relu'),
                                      tf.keras.layers.Dense(39, activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(X_train, y_train, batch_size=64,epochs=10,verbose=2,validation_data=(X_test,y_test))",DL,Tensorflow,shivangi21/crime-tesorflow,https://www.kaggle.com/shivangi21/crime-tesorflow
sf-crime,classification,multiclass-classification,"The task is to predict the category of crime incidents based on various features such as time, location, and day of the week.",Tabular,Multi-class Log Loss,"This dataset contains incidents derived from the SFPD Crime Incident Reporting system, covering data from January 1, 2003, to May 13, 2015. The training and test sets rotate weekly, with specific weeks designated for each. Key fields include the timestamp of the crime, the category of the crime (target variable), detailed descriptions, day of the week, police district, resolution status, approximate address, and geographical coordinates (longitude and latitude).",['Category'],"[""drop unnecessary columns"", ""replace day of the week with numerical values"", ""extract year, month, day, hours, and minutes from the date"", ""one-hot encode police district""]","model=Sequential()
model.add(Dense(128,input_shape=(X.shape[1],)))
model.add(Dense(128,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(39,activation='softmax'))
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
train=model.fit(X_train,y_train,batch_size=32,epochs=10,verbose=2,validation_data=(X_test,y_test))",DL,Tensorflow,nishita17/san-francisco-crime-classification,https://www.kaggle.com/nishita17/san-francisco-crime-classification
sf-crime,classification,multiclass-classification,"The task is to predict the category of crime incidents based on various features such as time, location, and description.",Tabular,Multi-class Log Loss,"This dataset contains incidents derived from the SFPD Crime Incident Reporting system, covering data from January 1, 2003, to May 13, 2015. The training and test sets rotate weekly, with odd weeks designated for testing and even weeks for training. Key fields include the timestamp of the crime, the category of the crime (target variable), detailed descriptions, day of the week, police district, resolution status, approximate address, and geographical coordinates (longitude and latitude).",['Category'],"[""convert categorical columns to numerical representations"", ""drop unnecessary columns""]","model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_shape,)))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32)",DL,Tensorflow,nitishputrevu/sfc-classification-pipeline-model-rf-regressor,https://www.kaggle.com/nitishputrevu/sfc-classification-pipeline-model-rf-regressor
sf-crime,classification,multiclass-classification,"The task is to predict the category of crime incidents based on various features such as time, day of the week, and police district.",Tabular,Multi-class Log Loss,"This dataset contains incidents derived from the SFPD Crime Incident Reporting system, covering data from January 1, 2003, to May 13, 2015. The training and test sets rotate weekly, with odd weeks for testing and even weeks for training. Key fields include the timestamp of the crime, the category of the crime (target variable), detailed descriptions, day of the week, police district, resolution of the incident, approximate address, and geographical coordinates (longitude and latitude).",['Category'],"[""extract year from Dates"", ""extract month from Dates"", ""extract day from Dates"", ""convert time to minutes since midnight"", ""drop Dates column"", ""one-hot encode DayOfWeek"", ""one-hot encode PdDistrict"", ""drop Descript, Resolution, and Address columns"", ""convert target variable Category to one-hot encoding"", ""split data into features and target variable"", ""convert feature data to float type"", ""split data into training and testing sets""]","model=Sequential()
model.add(Dense(100,input_shape=(X.shape[1],)))
model.add(Dense(80,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(39,activation='softmax'))
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
train=model.fit(X_train,y_train,batch_size=64,epochs=16,verbose=2,validation_data=(X_test,y_test))",DL,Tensorflow,sakshi0100/sanfrancisco-crime-keras,https://www.kaggle.com/sakshi0100/sanfrancisco-crime-keras
sf-crime,classification,multiclass-classification,"The task is to predict the category of crime incidents based on various features such as time, location, and day of the week.",Tabular,Multi-class Log Loss,"This dataset contains incidents derived from the SFPD Crime Incident Reporting system, covering data from January 1, 2003, to May 13, 2015. The training and test sets rotate weekly, with specific weeks designated for each. Key fields include the timestamp of the crime, the category of the crime (target variable), detailed descriptions, day of the week, police district, resolution status, approximate address, and geographical coordinates (longitude and latitude).",['Category'],"[""drop unnecessary columns"", ""replace day names with numerical values"", ""one-hot encode police district"", ""extract year, month, day, hours, and minutes from date"", ""convert data types to float""]","model = Sequential()
model.add(Dense(100, input_shape=(X.shape[1],)))
model.add(Activation('relu'))
model.add(Dense(100))
model.add(Activation('relu'))
model.add(Dense(80))
model.add(Activation('relu'))
model.add(Dense(60))
model.add(Activation('relu'))
model.add(Dense(39))
model.add(Activation('softmax'))
model.compile(optimizer ='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])
r = model.fit(X_train,y_train, batch_size = 32, epochs = 20, verbose = 2, validation_data=(X_test,y_test))",DL,Tensorflow,arko007/neural-networks-sf-crime-classification,https://www.kaggle.com/arko007/neural-networks-sf-crime-classification
shelter-animal-outcomes,classification,multiclass-classification,"The task is to predict the outcome of animals as they leave the Austin Animal Center, with possible outcomes including Adoption, Died, Euthanasia, Return to owner, and Transfer.",Tabular,Multi-class Log Loss,"The dataset consists of records from the Austin Animal Center from October 1st, 2013 to March 2016, detailing the outcomes of animals as they leave the center. Each animal is assigned a unique Animal ID during intake. The dataset includes a training set and a test set, with the training set used to develop the model and the test set used for evaluation. The outcomes to be predicted are Adoption, Died, Euthanasia, Return to owner, and Transfer.",['OutcomeType'],"[""drop irrelevant columns"", ""map DateTime to year, month, day"", ""map SexuponOutcome to intactness and gender"", ""map AgeuponOutcome to age in days"", ""map Breed to two breed columns"", ""map Color to two color columns"", ""one-hot encode categorical features"", ""scale features using standard scaler"", ""split data into training and validation sets""]","layer = tf.layers.dense(inputs=layer, units=1024, activation=tf.nn.relu)
layer = tf.layers.dense(inputs=layer, units=512, activation=tf.nn.relu)
layer = tf.layers.dense(inputs=layer, units=256, activation=tf.nn.relu)
layer = tf.layers.dense(inputs=layer, units=128, activation=tf.nn.relu)
logits = tf.layers.dense(inputs=layer, units=params['num_classes'])
classifier.train(input_fn=train_input_fn)",DL,Tensorflow,zhoulingyan0228/shelter-animal-outcome-prediction-w-nn,https://www.kaggle.com/zhoulingyan0228/shelter-animal-outcome-prediction-w-nn
shelter-animal-outcomes,classification,multiclass-classification,"The task is to predict the outcome of animals as they leave the Austin Animal Center, with possible outcomes including Adoption, Died, Euthanasia, Return to owner, and Transfer.",Tabular,Multi-class Log Loss,"The dataset consists of records from the Austin Animal Center from October 1st, 2013 to March 2016, detailing the outcomes of animals as they leave the center. Each animal is assigned a unique Animal ID during intake. The training and test datasets are randomly split, and the files include train.csv for training, test.csv for testing, and sample_submission.csv for the submission format.","['Adoption', 'Died', 'Euthanasia', 'Return_to_owner', 'Transfer']","[""fill missing names with 'No Name'""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(5, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32)",DL,Tensorflow,scollins/scollins-shelteranimaloutcomes,https://www.kaggle.com/scollins/scollins-shelteranimaloutcomes
shelter-animal-outcomes,classification,multiclass-classification,"Predict the outcome of shelter animals as they leave the Animal Center, with outcomes including Adoption, Died, Euthanasia, Return to owner, and Transfer.",Tabular,Multi-class Log Loss,"The dataset consists of records from the Austin Animal Center from October 1, 2013, to March 2016, detailing the outcomes of animals as they leave the center. Each animal is assigned a unique Animal ID during intake. The training and test datasets are randomly split, and the outcomes to predict include Adoption, Died, Euthanasia, Return to owner, and Transfer.",['OutcomeType'],"[""drop irrelevant columns"", ""fill missing values with 'NA' for categorical and 0 for numerical"", ""label encode categorical variables"", ""convert all variables to categorical type"", ""split the dataset back into training and test sets"", ""encode the target variable"", ""perform train-validation split""]","model = ShelterOutcomeModel(embedding_sizes, 1)
to_device(model, device)

optim = get_optimizer(model, lr = 0.001, wd = 0.0)
for i in range(epochs): 
    loss = train_model(model, optim, train_dl)
    print(""training loss: "", loss)
    val_loss(model, valid_dl)",DL,Pytorch,vishakbharadwaj/using-embbeings-for-categorical-variables,https://www.kaggle.com/vishakbharadwaj/using-embbeings-for-categorical-variables
shelter-animal-outcomes,classification,multiclass-classification,"The task is to predict the outcome of animals as they leave the Austin Animal Center, with possible outcomes including Adoption, Died, Euthanasia, Return to owner, and Transfer.",Tabular,Multi-class Log Loss,"The dataset consists of records from the Austin Animal Center from October 1st, 2013 to March, 2016, detailing the outcomes of animals as they leave the center. Each animal is assigned a unique Animal ID during intake. The training and test datasets are randomly split, and the files include train.csv for training, test.csv for testing, and sample_submission.csv for the submission format.",['OutcomeType'],"[""drop irrelevant columns"", ""fill missing values with 'Unknown'"", ""convert categorical columns to category type"", ""encode categorical columns as integer codes"", ""split data into training and validation sets""]","model = ShelterOutcomeModel(emb_szs, n_cont).to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
epochs = 100
for epoch in range(epochs):
    model.train()
    for x_cat, x_cont, y in train_dl:
        x_cat, x_cont, y = x_cat.to(device), x_cont.to(device), y.to(device).long()
        preds = model(x_cat, x_cont)
        loss = loss_fn(preds, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()",DL,Pytorch,guptamols/categorical-embedding,https://www.kaggle.com/guptamols/categorical-embedding
integer-sequence-learning,regression,continuous-regression,The task is to predict the next integer in a sequence based on the provided integer sequences.,Tabular,Classification accuracy,"This dataset contains the majority of the integer sequences from the OEIS. It is split into a training set, where you are given the full sequence, and a test set, where we have removed the last number from the sequence. The task is to predict this removed integer. The training set contains full sequences, while the test set is missing the last number in each sequence. A sample submission file is also provided in the correct format.",['Last'],"[""normalize the integer sequences""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=10, batch_size=32)",DL,Tensorflow,jhy1993/time-series-prediction,https://www.kaggle.com/jhy1993/time-series-prediction
integer-sequence-learning,regression,continuous-regression,The task is to predict the next integer in a sequence based on the provided integer sequences from the OEIS.,Tabular,Classification accuracy,"This dataset contains the majority of the integer sequences from the OEIS. It is split into a training set, where you are given the full sequence, and a test set, where we have removed the last number from the sequence. The task is to predict this removed integer. The training set contains full sequences, while the test set is missing the last number in each sequence. A sample submission file is also provided in the correct format.",['Last'],"[""convert sequences to numerical format"", ""normalize sequence lengths""]","model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(input_shape,)))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(train_data, train_labels, epochs=10, batch_size=32)",DL,Tensorflow,steviekbd/newse,https://www.kaggle.com/steviekbd/newse
integer-sequence-learning,regression,continuous-regression,The task is to predict the last integer in a sequence based on the preceding integers.,Tabular,Classification accuracy,"This dataset contains integer sequences from the OEIS, split into a training set with full sequences and a test set missing the last number. The training set is in train.csv, while test.csv contains the sequences without the last integer. A sample submission file is provided in sample_submission.csv.",['Id'],"[""drop missing values"", ""convert sequences from string to numpy array""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(train_data, train_label, epochs=10, batch_size=32)",DL,Tensorflow,zhangzhiqian/mytry-1,https://www.kaggle.com/zhangzhiqian/mytry-1
ghouls-goblins-and-ghosts-boo,classification,multiclass-classification,"The task is to classify creatures into one of three types: Ghost, Goblin, or Ghoul based on their features.",Tabular,Categorization accuracy,"The dataset consists of a training set and a test set. The training set includes features such as bone length, rotting flesh percentage, hair length, and whether the creature has a soul, along with a target variable indicating the type of creature. The test set is used for making predictions. Each creature is identified by an id, and the dominant color is also recorded.",['type'],"[""normalize bone_length and hair_length between 0 and 1"", ""create polynomial features for interactions between bone_length, hair_length, and has_soul"", ""apply kernel PCA to reduce dimensionality""]","net = tflearn.input_data(shape=[None, X_train.shape[1]]); net = tflearn.fully_connected(net, 1024, activation='relu', weights_init='xavier', regularizer='L2'); net = tflearn.dropout(net, 0.8); net = tflearn.fully_connected(net, y_train.shape[1], activation='softmax'); net = tflearn.regression(net); model = tflearn.DNN(net, tensorboard_verbose=0); model.fit(X_train, y_train, validation_set=0.2, n_epoch=30);",DL,Tensorflow,daavoo/tensorflow-1vs1,https://www.kaggle.com/daavoo/tensorflow-1vs1
ghouls-goblins-and-ghosts-boo,classification,multiclass-classification,"The task is to classify creatures into types based on their features such as bone length, rotting flesh, hair length, and color.",Tabular,Categorization accuracy,"The dataset consists of a training set and a test set, with features including id, bone length, rotting flesh percentage, hair length, soul percentage, and dominant color. The target variable is the type of creature, which can be 'Ghost', 'Goblin', or 'Ghoul'.",['type'],"[""drop the id column from train and test sets"", ""create new features by multiplying existing features"", ""one-hot encode the color feature"", ""label encode the target variable""]","import tensorflow as tf
from tensorflow.contrib import learn
x=tf.contrib.learn.infer_real_valued_columns_from_input(X_Train1)
tf_clf_dnn = learn.DNNClassifier(hidden_units=[16], n_classes=3, feature_columns=x, activation_fn=tf.sigmoid)
tf_clf_dnn.fit(X_Train1, y_Train1, max_steps=5000)",DL,Tensorflow,gauravjoshi1986/ghostbuster-data,https://www.kaggle.com/gauravjoshi1986/ghostbuster-data
ghouls-goblins-and-ghosts-boo,classification,multiclass-classification,"The task is to classify creatures into types based on their features such as bone length, rotting flesh percentage, hair length, and soul percentage.",Tabular,Categorization accuracy,"The dataset consists of a training set and a test set. The training set includes features such as id, bone length, rotting flesh percentage, hair length, soul percentage, and color, with the target variable being the type of creature which can be 'Ghost', 'Goblin', or 'Ghoul'. The test set is used for making predictions and includes similar features without the target variable.",['type'],"[""one-hot encode the color feature"", ""drop the original color column"", ""split the dataset into features and target variable"", ""split the training data into training and validation sets""]","model = Sequential()
model.add(Dense(64,input_shape = (X.shape[1],)))
model.add(Dense(32,activation = 'relu'))
model.add(Dense(32,activation = 'relu'))
model.add(Dense(32,activation = 'relu'))
model.add(Dense(16,activation = 'relu'))
model.add(Dense(16,activation = 'relu'))
model.add(Dense(8,activation = 'relu'))
model.add(Dense(3,activation = 'softmax'))
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
train=model.fit(x=X_train,y=y_train,batch_size=16,epochs=38,verbose=2,validation_data=(X_test,y_test))",DL,Tensorflow,chromerai/ghouls-goblins-neural-net,https://www.kaggle.com/chromerai/ghouls-goblins-neural-net
ghouls-goblins-and-ghosts-boo,classification,multiclass-classification,"The task is to classify creatures into types based on their features, specifically identifying whether they are a Ghost, Goblin, or Ghoul.",Tabular,Categorization accuracy,"The dataset consists of a training set and a test set, with features including id, bone_length, rotting_flesh, hair_length, has_soul, color, and the target variable type which indicates the creature type. The bone_length, hair_length, and has_soul features are normalized between 0 and 1, while rotting_flesh is a percentage. The color feature includes categories such as white, black, clear, blue, green, and blood.",['type'],"[""drop the 'id' column from training and test sets"", ""log transform numerical columns"", ""label encode the target variable"", ""apply one-hot encoding to categorical features"", ""scale numerical features using standard scaling""]","model.fit(X_train, y_train)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(3, activation='softmax'))",DL,Tensorflow,mwilla/ghouls-goblins-and-ghosts-boo,https://www.kaggle.com/mwilla/ghouls-goblins-and-ghosts-boo
ghouls-goblins-and-ghosts-boo,classification,multiclass-classification,"The goal is to classify creatures into types such as Ghost, Goblin, and Ghoul based on features like bone length, rotting flesh, hair length, and color.",Tabular,Categorization accuracy,"The dataset consists of a training set and a test set, with features including id, bone_length, rotting_flesh, hair_length, has_soul, and color. The target variable is type, which indicates the creature's classification as either Ghost, Goblin, or Ghoul.",['type'],"[""normalize bone_length and hair_length"", ""encode color using one-hot encoding"", ""create interaction features like hair_soul and hair_bone""]","model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(x_train.shape[1],)))
model.add(Dense(32, activation='relu'))
model.add(Dense(3, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=50, batch_size=32)",DL,Tensorflow,senasudemir/ghost-type-prediction,https://www.kaggle.com/senasudemir/ghost-type-prediction
spooky-author-identification,classification,multiclass-classification,"The objective is to predict the author of a given text from three possible authors: Edgar Allan Poe, H.P. Lovecraft, and Mary Shelley, based on the text's content.",Tabular,Multi-class Log Loss,"The dataset contains text from works of fiction written by Edgar Allan Poe, H.P. Lovecraft, and Mary Shelley. It is prepared by chunking larger texts into sentences. The training set includes labeled sentences with a unique identifier, the text itself, and the corresponding author. The dataset consists of three files: train.csv for training, test.csv for testing, and sample_submission.csv for submission format.",['author'],"[""split data into training and validation sets"", ""encode labels into integers"", ""convert integer labels to binary class matrices"", ""tokenize text data"", ""convert text to sequences of integers"", ""pad sequences to a uniform length"", ""create a matrix of word vectors using GloVe embeddings""]","model = Sequential()
model.add(Embedding(len(word_index) + 1, 
                    dim_glove,
                    weights = [word_vectorization_matrix],
                    input_length = maxlen_,
                    trainable = False))
model.add(SpatialDropout1D(0.3))
model.add(LSTM(dim_glove, dropout = 0.3, recurrent_dropout = 0.3))
model.add(Dense(512, activation = 'relu'))
model.add(Dropout(0.8))
model.add(Dense(256, activation = 'relu'))
model.add(Dropout(0.8))
model.add(Dense(3, activation = 'softmax'))

model.compile(loss = multiclass_log_loss, 
              optimizer = tf.keras.optimizers.Adam(learning_rate = initial_learning_rate))

history = model.fit(x_train_pad, 
                    y = y_train_matrix, 
                    batch_size = 256, 
                    epochs = 100, 
                    verbose = 0, 
                    validation_data = (x_valid_pad, y_valid_matrix), 
                    callbacks = [earlystop, learning_rate_scheduler])",DL,Tensorflow,sugataghosh/spooky-author-identification-glove-lstm,https://www.kaggle.com/sugataghosh/spooky-author-identification-glove-lstm
spooky-author-identification,classification,multiclass-classification,"The task is to classify sentences from works of fiction into one of three authors: Edgar Allan Poe, HP Lovecraft, or Mary Shelley, based on the text provided.",Tabular,Multi-class Log Loss,"The competition dataset contains text from works of fiction written by Edgar Allan Poe, HP Lovecraft, and Mary Shelley. The data was prepared by chunking larger texts into sentences using a sentence tokenizer. The dataset includes a training set, a test set, and a sample submission file. Each entry has a unique identifier, the text of the sentence, and the corresponding author label.",['author'],"[""load the training dataset"", ""load the test dataset"", ""drop rows with specific author"", ""encode author labels using label encoding"", ""split the training data into training and validation sets""]","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)",DL,Tensorflow,shaz13/spooky-author-identification-bert,https://www.kaggle.com/shaz13/spooky-author-identification-bert
spooky-author-identification,classification,multiclass-classification,The task is to identify the author of sentences from works of fiction by predicting the probability of each author for given text.,Tabular,Multi-class Log Loss,"The dataset contains text from works of fiction written by Edgar Allan Poe, HP Lovecraft, and Mary Shelley. It is prepared by chunking larger texts into sentences. The training set includes a unique identifier for each sentence, the text itself, and the corresponding author. The test set is used for evaluation, and a sample submission file is provided.",['author'],"[""remove punctuation from text"", ""tokenize sentences into words"", ""count total words per sentence"", ""calculate sentence length without punctuation"", ""calculate total text length"", ""count punctuation marks per sentence"", ""calculate punctuation per character ratio"", ""calculate unique word ratio"", ""calculate average word length"", ""perform sentiment analysis on text""]","feature_word_count = tf.feature_column.numeric_column(""word_count"")
feature_text_length = tf.feature_column.numeric_column(""text_length"")
feature_punctuation_per_char = tf.feature_column.numeric_column(""punctuation_per_char"")
feature_unique_ratio = tf.feature_column.numeric_column(""unique_ratio"")
feature_avg_word_length = tf.feature_column.numeric_column(""avg_word_length"")
feature_sentiment = tf.feature_column.numeric_column(""sentiment"")

base_columns = [
    feature_word_count, feature_text_length, feature_punctuation_per_char, feature_unique_ratio, feature_avg_word_length, feature_sentiment
]

linear_model = tf.estimator.LinearClassifier(
    model_dir=model_dir, 
    feature_columns=base_columns,
    n_classes=len(authors),
    label_vocabulary=authors)

linear_model.train(input_fn=train_fn, steps=train_steps)",DL,Tensorflow,srkirkland/author-identification-with-tensorflow,https://www.kaggle.com/srkirkland/author-identification-with-tensorflow
spooky-author-identification,classification,multiclass-classification,"The task is to identify the author of sentences from works of fiction by Edgar Allan Poe, HP Lovecraft, and Mary Shelley based on their writing style.",Tabular,Multi-class Log Loss,"The dataset contains text from works of fiction written by Edgar Allan Poe, HP Lovecraft, and Mary Shelley. It is prepared by chunking larger texts into sentences. The training set includes a unique identifier for each sentence, the text of the sentence, and the corresponding author. The test set is used to evaluate the model's performance in predicting the authors of unseen sentences.",['author'],"[""remove special characters from text"", ""tokenize text into sequences"", ""pad sequences to a fixed length"", ""convert author labels to categorical format""]","inputs = layers.Input(shape=(MAXLEN,))
embedding_layer = TokenAndPositionEmbedding(MAXLEN, VOCABLEN, embed_dim)
x = embedding_layer(inputs)
transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
x = transformer_block(x)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.3)(x)
x = layers.Dense(32, activation=""relu"")(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(NCHANNEL, activation=""softmax"")(x)

model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(Adam(lr=5e-5), ""categorical_crossentropy"", metrics=[""accuracy""])
history = model.fit(X_train, y_train, batch_size=BATCHSIZE, epochs=EPOCHS, validation_data=(X_val, y_val), callbacks=[es])",DL,Tensorflow,nicapotato/spooky-authors-keras-transformers,https://www.kaggle.com/nicapotato/spooky-authors-keras-transformers
spooky-author-identification,classification,multiclass-classification,"The task is to identify the author of sentences from works of fiction by Edgar Allan Poe, HP Lovecraft, and Mary Shelley based on provided text.",Tabular,Multi-class Log Loss,"The dataset contains text from works of fiction written by Edgar Allan Poe, HP Lovecraft, and Mary Shelley. It is prepared by chunking larger texts into sentences. The training set includes a unique identifier for each sentence, the text itself, and the corresponding author. The test set is used to evaluate the model's performance in predicting the authors of unseen sentences.",['author'],"[""tokenize text using BERT tokenizer"", ""encode text into tokens, masks, and segment flags"", ""convert author labels to categorical format""]","input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=""input_word_ids"")
input_mask = Input(shape=(max_len,), dtype=tf.int32, name=""input_mask"")
segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=""segment_ids"")
_, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])
clf_output = sequence_output[:, 0, :]
out = Dense(out_channels, activation='softmax')(clf_output)
model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)
model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
train_history = model.fit(train_input, train_labels, validation_split=0.2, epochs=EPOCHS, callbacks=[checkpoint, es], batch_size=BATCH_SIZE)",DL,Tensorflow,nicapotato/spooky-simple-bert,https://www.kaggle.com/nicapotato/spooky-simple-bert
spooky-author-identification,classification,multiclass-classification,"The task is to classify sentences from works of fiction into one of three authors: Edgar Allan Poe, HP Lovecraft, or Mary Shelley, based on the text content.",Tabular,Multi-class Log Loss,"The competition dataset contains text from works of fiction written by Edgar Allan Poe, HP Lovecraft, and Mary Shelley. The data was prepared by chunking larger texts into sentences using a sentence tokenizer. The dataset includes a training set, a test set, and a sample submission file. Each entry has a unique identifier, the text of the sentence, and the corresponding author.",['author'],"[""clean text by removing stopwords"", ""remove punctuation"", ""lemmatize words"", ""create n-grams from sentences""]","decoder = RNN(voc_len, hidden_size, voc_len, n_layers)
decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss()

for epoch in range(1, n_epochs + 1):
    loss = train(inp,tar)       
    loss_avg += loss

    if epoch % print_every == 0:
        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))

    if epoch % plot_every == 0:
        all_losses.append(loss_avg / plot_every)
        loss_avg = 0",DL,Pytorch,ab971631/beginners-guide-to-text-generation-pytorch,https://www.kaggle.com/ab971631/beginners-guide-to-text-generation-pytorch
spooky-author-identification,classification,multiclass-classification,"The task is to identify the author of sentences from works of fiction by Edgar Allan Poe, HP Lovecraft, and Mary Shelley based on provided text.",Tabular,Multi-class Log Loss,"The dataset contains text from works of fiction written by Edgar Allan Poe, HP Lovecraft, and Mary Shelley. It has been prepared by chunking larger texts into sentences. The training set includes a unique identifier for each sentence, the text of the sentence, and the corresponding author. The test set is structured similarly but does not include author labels. The dataset consists of three files: train.csv, test.csv, and sample_submission.csv, with a total size of 1.9 MB.",['author'],"[""convert text to lowercase"", ""fill missing values with placeholder"", ""tokenize sentences"", ""pad sequences to uniform length"", ""encode target labels using label encoding""]","model = BiLSTMGRUAttention()
model.cuda()
loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')
optimizer = torch.optim.Adam(model.parameters())
train_oofpreds, test_oofpreds = MultiClassTrainingInference(model)",DL,Pytorch,shaz13/revised-base-models-in-pytorch-notebook,https://www.kaggle.com/shaz13/revised-base-models-in-pytorch-notebook
spooky-author-identification,classification,multiclass-classification,"The task is to identify the author of sentences from works of fiction by Edgar Allan Poe, HP Lovecraft, and Mary Shelley based on provided text.",Tabular,Multi-class Log Loss,"The dataset contains text from works of fiction written by Edgar Allan Poe, HP Lovecraft, and Mary Shelley. It is prepared by chunking larger texts into sentences. The training set includes a unique identifier for each sentence, the text of the sentence, and the corresponding author. The test set contains sentences for which the authors need to be predicted.",['author'],"[""remove leading, trailing and in-between whitespaces"", ""replace contractions with their full forms"", ""remove non-alphabetical characters""]","model = BERT_Classifier(CONFIG['num_classes']).to(CONFIG['device'])
optimizer = AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])
scheduler = CosineAnnealingLR(optimizer, T_max=10000, eta_min=2e-5)
loss = nn.CrossEntropyLoss().to(CONFIG['device'])
epochs = CONFIG['epochs']
",DL,Pytorch,raj26000/pytorch-author-identification-using-deberta-v3,https://www.kaggle.com/raj26000/pytorch-author-identification-using-deberta-v3
spooky-author-identification,classification,multiclass-classification,"The task is to identify the author of sentences from works of fiction by predicting the probability of each sentence belonging to one of three authors: Edgar Allan Poe, HP Lovecraft, or Mary Shelley.",Tabular,Multi-class Log Loss,"The dataset contains text from works of fiction written by Edgar Allan Poe, HP Lovecraft, and Mary Shelley. It is prepared by chunking larger texts into sentences. The training set includes unique identifiers, text, and the corresponding author for each sentence. The test set is used for evaluation, and a sample submission file is provided to guide the format of predictions.",['author'],"[""unzip the dataset"", ""load the training data"", ""encode author labels"", ""tokenize the text using BERT tokenizer"", ""create attention masks"", ""split the data into training, validation, and test sets"", ""create DataLoader for training and validation sets"", ""load pre-trained BERT model""]","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_), output_attentions=False, output_hidden_states=False)
model.to(device)
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)
model.train()
for epoch in range(epochs):
    for batch in train_dataloader:
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()",DL,Pytorch,sukanto/spooky-author,https://www.kaggle.com/sukanto/spooky-author
spooky-author-identification,classification,multiclass-classification,"The task is to classify sentences from works of fiction into one of three authors: Edgar Allan Poe, HP Lovecraft, or Mary Shelley based on their writing style.",Tabular,Multi-class Log Loss,"The dataset contains text from works of fiction written by Edgar Allan Poe, HP Lovecraft, and Mary Shelley. It is prepared by chunking larger texts into sentences. The training set includes a unique identifier for each sentence, the text of the sentence, and the corresponding author. The test set is used for evaluation, and a sample submission file is provided to guide the format of submissions.",['author'],"[""tokenize sentences using DistilBERT tokenizer"", ""calculate word counts for each excerpt"", ""count stopwords in each excerpt"", ""create training and validation datasets"", ""pad or trim excerpts to a maximum length""]","model = DistilBertAuthorClassifier()
model = model.to(device)
optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=False)
loss_fn = nn.CrossEntropyLoss().to(device)
model.train()
for d in data_loader:
    input_ids = d['input_ids'].to(device)
    attention_mask = d['attention_mask'].to(device)
    targets = d['targets'].to(device)
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    loss = loss_fn(outputs, targets)
    loss.backward()
    optimizer.step()",DL,Pytorch,odedgolden/spooky-authors-analysis,https://www.kaggle.com/odedgolden/spooky-authors-analysis
sentiment-analysis-on-movie-reviews,classification,multiclass-classification,"The task is to predict the sentiment of phrases from the Rotten Tomatoes dataset, classifying them into five sentiment categories: negative, somewhat negative, neutral, somewhat positive, and positive.",Tabular,Classification accuracy,"The dataset consists of tab-separated files containing phrases from the Rotten Tomatoes dataset. It includes a training set with phrases and their associated sentiment labels, as well as a test set with phrases only. Each phrase is identified by a PhraseId, and sentences are identified by a SentenceId. The training and test sets have been shuffled, and repeated phrases are included only once.",['Sentiment'],"[""remove html content"", ""remove non-alphabetic characters"", ""tokenize the sentences"", ""lemmatize each word"", ""convert target labels to one-hot encoding"", ""split data into training and validation sets"", ""initialize tokenizer"", ""convert texts to sequences"", ""pad sequences to equalize lengths""]","model=Sequential()
model.add(Embedding(len(list(unique_words)),300,input_length=len_max))
model.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))
model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))
model.add(Dense(100,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes,activation='softmax'))
model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])
history=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=6, batch_size=256, verbose=1, callbacks=callback)",DL,Tensorflow,chiranjeevbit/movie-review-prediction,https://www.kaggle.com/chiranjeevbit/movie-review-prediction
sentiment-analysis-on-movie-reviews,classification,multiclass-classification,"The task is to classify phrases from movie reviews into one of five sentiment categories: negative, somewhat negative, neutral, somewhat positive, and positive.",Tabular,Classification accuracy,"The dataset consists of tab-separated files containing phrases extracted from movie reviews. The training set includes 156,060 instances with associated sentiment labels, while the test set contains 66,292 phrases without labels. Each phrase is identified by a PhraseId, and the dataset is designed for benchmarking sentiment classification tasks.",['Sentiment'],"[""remove null or empty values"", ""tokenize phrases"", ""pad sequences to a maximum length"", ""convert sentiment labels to categorical format"", ""split data into training and validation sets""]","input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')
Sentiments = Dense(units=len(data_train.Sentiment_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='Sentiment')(pooled_output)
model = Model(inputs=inputs, outputs=outputs, name='BERT_MultiClass')
model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])
history = model.fit(x={'input_ids': x_train['input_ids']}, y={'Sentiment': y_train}, validation_data=({'input_ids': x_val['input_ids']},{'Sentiment': y_val}), batch_size=64, epochs=2, verbose=1)",DL,Tensorflow,georgesaavedra/best-sentiment-classifier-transformers,https://www.kaggle.com/georgesaavedra/best-sentiment-classifier-transformers
sentiment-analysis-on-movie-reviews,classification,multiclass-classification,"The task is to predict the sentiment of phrases from movie reviews, categorizing them into five sentiment labels ranging from negative to positive.",Tabular,Classification accuracy,"The dataset consists of tab-separated files containing phrases extracted from movie reviews in the Rotten Tomatoes dataset. It includes a training set with phrases and their corresponding sentiment labels, as well as a test set with phrases only. Each phrase is associated with a unique PhraseId, and the dataset has been shuffled to maintain the integrity of the evaluation process.",['Sentiment'],"[""convert text to lowercase"", ""remove punctuation"", ""remove extra spaces"", ""remove single characters""]","input_ids = tf.keras.Input(shape = (max_len, ), dtype = 'int32')
attention_masks = tf.keras.Input(shape = (max_len,), dtype = 'int32')
outputs = model_([input_ids, attention_masks])
final_output = tf.keras.layers.Dense(num_labels, activation = 'softmax')(cls_output)
model = tf.keras.Model(inputs = [input_ids, attention_masks], outputs = final_output)
model.compile(optimizer = Adam(lr = 1e-5), loss = 'categorical_crossentropy', metrics = ['accuracy'])
model.fit([input_ids_train, attention_mask_train], train_y, validation_split=0.2, epochs = 3, batch_size = 16, shuffle = True, verbose=2)",DL,Tensorflow,mitramir5/transformers-with-tensorflow,https://www.kaggle.com/mitramir5/transformers-with-tensorflow
sentiment-analysis-on-movie-reviews,classification,multiclass-classification,"The task is to predict the sentiment of phrases extracted from movie reviews, with labels ranging from negative to positive.",Tabular,Classification accuracy,"The dataset consists of tab-separated files containing phrases from the Rotten Tomatoes dataset. It includes a training set with phrases and their associated sentiment labels, and a test set with phrases only. Each phrase is identified by a PhraseId, and sentences are identified by a SentenceId. The sentiment labels are categorized as negative, somewhat negative, neutral, somewhat positive, and positive.",['Sentiment'],"[""drop unnecessary columns"", ""clean text by removing stopwords and special characters"", ""one-hot encode phrases"", ""pad sequences to a fixed length""]","model = Sequential()
inputs = keras.Input(shape=(None,), dtype=""int32"")
model.add(inputs)
model.add(Embedding(50000, 128))
model.add(Bidirectional(LSTM(64, return_sequences=True)))
model.add(Bidirectional(LSTM(64)))
model.add(Dense(5, activation=""sigmoid"")
model.compile(""adam"", ""sparse_categorical_crossentropy"", metrics=[""accuracy""])
model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_val, y_val))",DL,Tensorflow,shyambhu/sentiment-classification-using-lstm,https://www.kaggle.com/shyambhu/sentiment-classification-using-lstm
sentiment-analysis-on-movie-reviews,classification,multiclass-classification,"The task is to predict the sentiment of phrases extracted from movie reviews, with labels ranging from negative to positive.",Tabular,Classification accuracy,"The dataset consists of tab-separated files containing phrases from the Rotten Tomatoes dataset. It includes a training set with phrases and their associated sentiment labels, as well as a test set with phrases only. Each phrase is identified by a PhraseId, and sentences are identified by a SentenceId. The sentiment labels are categorized as negative, somewhat negative, neutral, somewhat positive, and positive.",['Sentiment'],"[""remove duplicate rows"", ""tokenize phrases using Bert tokenizer"", ""create one-hot encoded labels"", ""shuffle dataset"", ""split dataset into training and validation sets""]","input_ids = tf.keras.layers.Input(shape=(512,), name='input_ids', dtype='int32')
mask = tf.keras.layers.Input(shape=(512,), name='attention_mask', dtype='int32')
embeddings = bert.bert(input_ids, attention_mask=mask)[1]
x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)
y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)
model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)
loss = tf.keras.losses.CategoricalCrossentropy()
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[acc])
history = model.fit(train_ds, validation_data=val_ds, epochs=1)",DL,Tensorflow,amirkonjkav/sentiment-analysis-of-movie-reviews-transformers,https://www.kaggle.com/amirkonjkav/sentiment-analysis-of-movie-reviews-transformers
sentiment-analysis-on-movie-reviews,classification,multiclass-classification,"The task is to predict the sentiment of phrases extracted from movie reviews, with labels ranging from negative to positive.",Tabular,Classification accuracy,"The dataset consists of tab-separated files containing phrases from the Rotten Tomatoes dataset. It includes a training set with phrases and their associated sentiment labels, and a test set with phrases only. Each phrase is identified by a PhraseId, and the dataset preserves the train/test split for benchmarking purposes. Sentiment labels range from 0 (negative) to 4 (positive).",['Sentiment'],"[""tokenize the phrases"", ""numericalize the tokenized phrases""]","transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)
custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)
learner = Learner(databunch, custom_transformer_model, opt_func = CustomAdamW, metrics=[accuracy, error_rate])
learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))",DL,Pytorch,maroberti/fastai-with-transformers-bert-roberta,https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta
sentiment-analysis-on-movie-reviews,classification,multiclass-classification,"The task is to predict the sentiment of phrases from movie reviews, categorized into five sentiment labels ranging from negative to positive.",Tabular,Classification accuracy,"The dataset consists of tab-separated files containing phrases extracted from movie reviews in the Rotten Tomatoes dataset. It includes a training set with phrases and their corresponding sentiment labels, as well as a test set with phrases only. Each phrase is associated with a unique PhraseId, and the dataset is structured to facilitate sentiment analysis by providing a clear mapping of phrases to their sentiments.",['Sentiment'],"[""load data from tab-separated files"", ""tokenize phrases using a pre-trained tokenizer"", ""encode phrases into input IDs and attention masks"", ""create training and validation data loaders""]","model = BertReviewClassifier()
model.compile(optimizer=transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=False), loss=nn.CrossEntropyLoss(), metrics=['accuracy'])
history = model.fit(train_data_loader, validation_data=val_data_loader, epochs=5)",DL,Pytorch,odedgolden/sentiment-analysis-with-dimensionality-reduction,https://www.kaggle.com/odedgolden/sentiment-analysis-with-dimensionality-reduction
sentiment-analysis-on-movie-reviews,classification,multiclass-classification,"The task is to predict the sentiment of phrases from the Rotten Tomatoes dataset, with labels ranging from negative to positive on a scale of 0 to 4.",Tabular,Classification accuracy,"The dataset consists of tab-separated files containing phrases extracted from movie reviews. The training set includes phrases with their corresponding sentiment labels, while the test set contains only phrases for which sentiment labels must be predicted. Each phrase is associated with a unique PhraseId and belongs to a SentenceId, allowing tracking of phrases within sentences. The sentiment labels are categorized as 0 for negative, 1 for somewhat negative, 2 for neutral, 3 for somewhat positive, and 4 for positive.",['Sentiment'],"[""tokenize phrases into words"", ""remove non-ASCII characters"", ""convert words to lowercase"", ""remove punctuation"", ""remove numbers"", ""lemmatize verbs"", ""create a vocabulary mapping words to integers"", ""pad sequences to ensure equal length""]","net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=lr)
net.train()",DL,Pytorch,oragula/sentiment-analysis-rotten-tomato-movie-reviews,https://www.kaggle.com/oragula/sentiment-analysis-rotten-tomato-movie-reviews
sentiment-analysis-on-movie-reviews,classification,multiclass-classification,"The task is to predict the sentiment of phrases from movie reviews, classifying them into five sentiment categories ranging from negative to positive.",Tabular,Classification accuracy,"The dataset consists of tab-separated files containing phrases extracted from movie reviews. It includes a training set with phrases and their corresponding sentiment labels, as well as a test set with phrases only. Each phrase is associated with a unique PhraseId, and the dataset is designed to benchmark sentiment classification performance.",['Sentiment'],"[""tokenize phrases using a pre-trained tokenizer"", ""create a custom dataset class for loading data"", ""split the training dataset into training and validation sets"", ""create data loaders for training, validation, and testing""]","model = Model()
model.to(device)
optimizer = AdamW(model.parameters(), lr=2e-5)
criteron = nn.CrossEntropyLoss()
for epoch in range(3):
    model.train()
    for input_ids, attention_mask, target in train_dataloader:
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        target = target.to(device)
        optimizer.zero_grad()
        y_pred = model(input_ids, attention_mask)
        loss = criteron(y_pred, target)
        loss.backward()
        optimizer.step()",DL,Pytorch,kickitlikeshika/bert-for-sentiment-analysis-5th-place-solution,https://www.kaggle.com/kickitlikeshika/bert-for-sentiment-analysis-5th-place-solution
sentiment-analysis-on-movie-reviews,classification,multiclass-classification,"The task is to predict the sentiment of phrases from movie reviews, categorized into five sentiment labels ranging from negative to positive.",Tabular,Classification accuracy,"The dataset consists of tab-separated files containing phrases extracted from movie reviews in the Rotten Tomatoes dataset. It includes a training set with phrases and their corresponding sentiment labels, as well as a test set with phrases only. Each phrase is associated with a unique PhraseId and belongs to a specific sentence identified by a SentenceId. The sentiment labels are classified into five categories: 0 for negative, 1 for somewhat negative, 2 for neutral, 3 for somewhat positive, and 4 for positive.",['Sentiment'],"[""tokenize phrases"", ""convert phrases to lowercase"", ""one-hot encode sentiment labels""]","model = Model(len(sentiment.ordinals))
optimizer = torch.optim.Adam(model.parameters())
loss_functions = torch.nn.CrossEntropyLoss()

model.train() # Setting the model to Train

for epoch in range(15):
    losses = []
    for inputs , outputs in tqdm(trainloader):
        optimizer.zero_grad()
        results = model(inputs)
        loss = loss_functions(results , outputs)
        losses.append(loss.item())
        loss.backward()
        optimizer.step()
    print(""Loss {0}"".format(torch.tensor(losses).mean()))
    

results_buffer = []
actual_buffer = []
with torch.no_grad():
    model.eval()
    for inputs, outputs in testloader:
        results = model(inputs).argmax(dim = 1).numpy()
        results_buffer.append(results)
        actual_buffer.append(outputs)",DL,Pytorch,terminate9298/pytorch-turorials-for-neural-network-cnn-rnn-gan,https://www.kaggle.com/terminate9298/pytorch-turorials-for-neural-network-cnn-rnn-gan
bigquery-geotab-intersection-congestion,regression,quantile-regression,"The task is to predict the 20th, 50th, and 80th percentiles for total time stopped at intersections and the distance to the first stop for commercial vehicles.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks, grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. Predictions are required for three different quantiles of two metrics: total time stopped at an intersection and distance to the first stop. The training set includes an optional output metric for additional modeling.","['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']","[""median-impute missing values"", ""normalize numerical features"", ""one-hot encode categorical variables""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(6))
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, batch_size=32)",DL,Tensorflow,sirtorry/bigquery-ml-template-intersection-congestion,https://www.kaggle.com/sirtorry/bigquery-ml-template-intersection-congestion
bigquery-geotab-intersection-congestion,regression,quantile-regression,"The task is to predict the 20th, 50th, and 80th percentiles for total time stopped at intersections and the distance to the first stop for commercial vehicles.",Tabular,RMSE – Root Mean Squared Error,"The dataset contains aggregated trip logging metrics from commercial vehicles, grouped by intersection, month, hour of day, direction, and weekend status. It includes features such as latitude, longitude, street names, and various time metrics. The training set includes additional metrics for model building, while the test set requires predictions for specific quantiles of two metrics.","['TotalTimeStopped', 'DistanceToFirstStop']","[""drop unnecessary columns"", ""map street directions to numerical values"", ""encode street types using a predefined mapping"", ""standardize latitude and longitude by city"", ""create new features for total time stopped and distance to first stop based on percentiles"", ""one-hot encode categorical variables""]","x = keras.layers.Input(shape=[X_train.shape[1]])
fc1 = keras.layers.Dense(units=45)(x)
act1 = keras.layers.PReLU()(fc1)
bn1 = keras.layers.BatchNormalization()(act1)
dp1 = keras.layers.Dropout(0.15)(bn1)
concat1 = keras.layers.Concatenate()([x, dp1])
fc2 = keras.layers.Dense(units=60)(concat1)
act2 = keras.layers.PReLU()(fc2)
bn2 = keras.layers.BatchNormalization()(act2)
dp2 = keras.layers.Dropout(0.2)(bn2)
concat2 = keras.layers.Concatenate()([concat1, dp2])
fc3 = keras.layers.Dense(units=40)(concat2)
act3 = keras.layers.PReLU()(fc3)
bn3 = keras.layers.BatchNormalization()(act3)
dp3 = keras.layers.Dropout(0.2)(bn3)
concat3 = keras.layers.Concatenate()([concat2, dp3])
output = keras.layers.Dense(units=2, activation='softmax')(concat2)
model = keras.models.Model(inputs=[x], outputs=[output])
model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-7), loss='mse', metrics=[rmse])
model.fit(X_train, y_train, epochs=200, callbacks=[er], validation_data=[X_val, y_val], batch_size=batch_size)",DL,Tensorflow,bgmello/how-one-percentile-affect-the-others,https://www.kaggle.com/bgmello/how-one-percentile-affect-the-others
bigquery-geotab-intersection-congestion,regression,multi-output-regression,The task is to predict six target outcomes related to vehicle wait times and stop distances at intersections based on aggregated trip logging metrics.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks, grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. Predictions are required for three different quantiles of two metrics: total time stopped at an intersection and distance to the first stop, covering the 20th, 50th, and 80th percentiles. The training set includes an optional output metric for additional modeling.","['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']","[""median-impute missing values"", ""one-hot encode categorical variables"", ""normalize numerical features""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(6, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=100, batch_size=32)",DL,Tensorflow,sanikamal/bqml-predict-wait-times,https://www.kaggle.com/sanikamal/bqml-predict-wait-times
bigquery-geotab-intersection-congestion,regression,multi-output-regression,The task is to predict six continuous variables representing different quantiles of total time stopped and distance to the first stop for vehicles at intersections.,Tabular,RMSE – Root Mean Squared Error,"The dataset contains aggregated trip logging metrics from commercial vehicles, grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. Predictions are required for three quantiles (20th, 50th, and 80th percentiles) of total time stopped and distance to the first stop at intersections. The training set includes an additional metric, TimeFromFirstStop, which is not present in the test set.","['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']","[""fill NaN values with -999"", ""encode categorical features using label encoding"", ""create new features based on existing data"", ""drop unnecessary columns""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(6, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)",DL,Tensorflow,ragnar123/eda-feature-engineer-and-baseline-lgbm,https://www.kaggle.com/ragnar123/eda-feature-engineer-and-baseline-lgbm
bigquery-geotab-intersection-congestion,regression,multi-output-regression,"The task is to predict six target outcomes related to vehicle congestion at intersections, specifically the 20th, 50th, and 80th percentiles for total time stopped and distance to the first stop.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of aggregated trip logging metrics from commercial vehicles, including data grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. The training set includes metrics for total time stopped and distance to the first stop at intersections in major US cities, with additional optional metrics available for model building.","['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']","[""drop rows with missing values"", ""create a new feature for same street exact"", ""combine IntersectionId and City to create a unique Intersection feature"", ""apply label encoding to the Intersection feature"", ""one-hot encode the City feature""]","model = tf.keras.Sequential()\nmodel.add(layers.Dense(128, activation='relu', input_shape=(input_shape,)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(6))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.fit(X_train, y_train, epochs=100, batch_size=32)",DL,Tensorflow,kefahaied/bigquery-geotab-intersection-congestion,https://www.kaggle.com/kefahaied/bigquery-geotab-intersection-congestion
bigquery-geotab-intersection-congestion,regression,quantile-regression,The task is to predict six target outcomes related to vehicle wait times and stop distances at intersections based on aggregated trip logging metrics.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks, grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. Predictions are required for three different quantiles of two metrics: total time stopped at an intersection and distance to the first stop. The training set includes an optional output metric for additional modeling.","['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']","[""fill missing values with 'MISSING' for street names"", ""remove rare months by adjusting month values"", ""create cross-features based on street names and intersection data"", ""calculate distance from city center based on latitude and longitude"", ""group hours into categories""]","model = Net(lin_input_size=lin_input_size, embs_input_size=N_BINS, embs_input_cats=embs_input_cats, lin_output_size=128, embs_output_size=64).to(device)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.00005, momentum=0.9)
model.train()
for epoch in range(max_epochs):
    # training loop
    model.eval()
    # validation loop
",DL,Pytorch,davideanastasia/hybrid-pytorch-hashing-embedding-linear,https://www.kaggle.com/davideanastasia/hybrid-pytorch-hashing-embedding-linear
new-york-city-taxi-fare-prediction,regression,continuous-regression,"The task is to predict the fare amount for taxi rides based on various features such as pickup and dropoff locations, time of day, and passenger count.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set with approximately 55 million rows and a test set with about 10,000 rows. The training set includes features such as pickup and dropoff longitude and latitude, pickup datetime, and passenger count, with the target variable being the fare amount. The test set contains the same features but lacks the fare amount, which needs to be predicted. A sample submission file is also provided, which predicts a constant fare amount based on the mean fare from the training set.",['fare_amount'],"[""remove outliers based on geographical coordinates"", ""remove negative fare amounts"", ""remove fares greater than 250"", ""remove rides that start and end at the same location"", ""create time-based features from pickup datetime"", ""calculate differences in latitude and longitude between pickup and dropoff locations"", ""calculate Euclidean and Manhattan distances between pickup and dropoff locations"", ""scale numerical features using Min-Max scaling"", ""one-hot encode categorical features""]","estimator = tf.estimator.DNNLinearCombinedRegressor(linear_feature_columns=wide_columns,dnn_feature_columns=deep_columns,dnn_hidden_units=hidden_units,dnn_optimizer=optimizer,config=run_config)
tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)",DL,Tensorflow,dimitreoliveira/tensorflow-dnn-coursera-ml-course-tutorial,https://www.kaggle.com/dimitreoliveira/tensorflow-dnn-coursera-ml-course-tutorial
new-york-city-taxi-fare-prediction,regression,continuous-regression,"Predict the fare amount for taxi rides in New York City based on pickup and dropoff locations, passenger count, and time features.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set with approximately 55 million rows and a test set with about 10,000 rows. The training set includes features such as pickup and dropoff coordinates, passenger count, and the target variable fare_amount, which represents the cost of the taxi ride. The test set contains the same features but lacks the fare_amount, which needs to be predicted. Additionally, there is a sample submission file that demonstrates the required format for predictions.",['fare_amount'],"[""drop unnecessary features"", ""remove duplicates"", ""filter out absurd fare values"", ""extract time features from pickup_datetime"", ""compute haversine distance between pickup and dropoff locations"", ""scale features using MinMaxScaler""]","model.compile(optimizer='adam', loss='mean_squared_error');
model.fit(X_train, y_train, epochs=100, batch_size=32)",DL,Tensorflow,victorbnnt/new-york-city-taxi-fare-prediction,https://www.kaggle.com/victorbnnt/new-york-city-taxi-fare-prediction
new-york-city-taxi-fare-prediction,regression,continuous-regression,"The task is to predict the fare amount for taxi rides based on various input features such as pickup and dropoff locations, time of day, and passenger count.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set with approximately 55 million rows and a test set with about 10,000 rows. The training set includes features such as pickup and dropoff coordinates, pickup datetime, and passenger count, with the target variable being the fare amount. The test set contains the same features but lacks the fare amount, which needs to be predicted. A sample submission file is also provided, which predicts a constant fare amount based on the mean fare from the training set.",['fare_amount'],"[""convert pickup_datetime to timestamp"", ""filter out rows with negative fare_amount"", ""remove rows with invalid latitude and longitude values"", ""drop rows with zero passenger_count""]","lyr_latlng_input = lyr.Input((4,))
lyr_haversine_bearing = lyr.Lambda(K_haversine_bearing, name='haversine')(lyr_latlng_input)
lyr_cat_input = lyr.Input((6,))
lyr_cat_embeddings = lyr.Embedding(464, 8)(lyr_cat_input)
lyr_cat_flatten = lyr.Flatten()(lyr_cat_embeddings)
lyr_num_input = lyr.Input((1,))
lyr_concat = lyr.concatenate([lyr_latlng_input, lyr_haversine_bearing, lyr_cat_flatten, lyr_num_input])
lyr_dense1 = lyr.Dense(512, activation=act.selu)(lyr_concat)
lyr_dense2 = lyr.Dense(128, activation=act.selu)(lyr_dense1)
lyr_dense3 = lyr.Dense(32, activation=act.selu)(lyr_dense2)
lyr_out = lyr.Dense(1, activation=act.selu)(lyr_dense3)
model = Model([lyr_latlng_input, lyr_cat_input, lyr_num_input], lyr_out)
model.compile(loss='mse', optimizer='nadam')
model.fit_generator(prep_data_gen(fast_csv_sampler('../input/train.csv', 512, chunk_size=16)), steps_per_epoch=128, epochs=16, workers=2, use_multiprocessing=True)",DL,Tensorflow,donniedarko/deep-haversine,https://www.kaggle.com/donniedarko/deep-haversine
new-york-city-taxi-fare-prediction,regression,continuous-regression,"The task is to predict the fare amount for taxi rides based on various input features such as pickup and dropoff locations, passenger count, and pickup time.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set with approximately 55 million rows and a test set with about 10,000 rows. The training set includes features such as pickup and dropoff coordinates, passenger count, and the target variable fare_amount, which represents the cost of the taxi ride. The test set contains the same features but lacks the fare_amount, which needs to be predicted. Additionally, there is a sample submission file that demonstrates the required format for predictions.",['fare_amount'],"[""replace zero values with NaN in coordinates"", ""remove rows with NaN values in coordinates"", ""convert pickup_datetime to datetime format"", ""extract year, month, day, hour from pickup_datetime"", ""filter out invalid passenger counts"", ""filter out outliers in fare_amount"", ""calculate trip distance using Haversine formula"", ""apply log transformation to fare_amount and trip distance"", ""scale features using MinMaxScaler""]","model_lstm = Sequential()
model_lstm.add(LSTM(256, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))
model_lstm.add(Dense(1))
model_lstm.compile(loss='mse', optimizer=opt)
lstm_history = model_lstm.fit(X_train_series, y_train, validation_split=0.2, epochs=epochs, verbose=2,  batch_size=batch)",DL,Tensorflow,o1ommo1o/datamining-assignment-2,https://www.kaggle.com/o1ommo1o/datamining-assignment-2
new-york-city-taxi-fare-prediction,regression,continuous-regression,"Predict the fare amount for taxi rides in New York City based on pickup and dropoff locations, passenger count, and time features.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set with approximately 55 million rows and a test set with about 10,000 rows. The training set includes features such as pickup and dropoff coordinates, passenger count, and the target variable fare_amount, which represents the cost of the taxi ride. The test set contains the same features but lacks the fare_amount, which needs to be predicted. Additionally, there is a sample submission file that demonstrates the required format for predictions.",['fare_amount'],"[""drop unnecessary features"", ""remove duplicates"", ""filter out absurd fare values"", ""extract time features from pickup_datetime"", ""compute haversine distance between pickup and dropoff locations"", ""scale features using MinMaxScaler""]","model.compile(optimizer='adam', loss='mean_squared_error'); model.fit(X_train, y_train, epochs=10, batch_size=32)",DL,Tensorflow,monthepp/new-york-city-taxi-fare-prediction,https://www.kaggle.com/monthepp/new-york-city-taxi-fare-prediction
new-york-city-taxi-fare-prediction,regression,continuous-regression,"The task is to predict the fare amount for taxi rides based on various input features such as pickup and dropoff locations, timestamps, and passenger count.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set with approximately 55 million rows and a test set with about 10,000 rows. The training set includes features such as pickup and dropoff coordinates, pickup datetime, and passenger count, with the target variable being the fare amount. The test set contains the same features but lacks the fare amount, which needs to be predicted. A sample submission file is also provided, which predicts a constant fare amount based on the mean fare from the training set.",['fare_amount'],"[""convert pickup_datetime to datetime format"", ""extract date parts from pickup_datetime"", ""calculate haversine distance between pickup and dropoff coordinates"", ""drop rows with missing values"", ""log-transform fare_amount for normalization"", ""scale numerical features using standard scaler""]","m = MixedInputModel(emb_szs=emb_szs, n_cont=len(df.columns)-len(catf), emb_drop=0.04, out_sz=1, szs=[1000,500,250], drops=[0.001,0.01,0.01], y_range=y_range).to(device)
opt = optim.Adam(m.parameters(), 1e-2)
lr_cosine = lr_scheduler.CosineAnnealingLR(opt, 1000)
lr, tloss, vloss = fit(model=m, train_dl=traindl, val_dl=valdl, loss_fn=F.mse_loss, opt=opt, scheduler=lr_cosine, epochs=epoch_n)",DL,Pytorch,nicapotato/pytorch-tabular-solution-francesco-pochetti,https://www.kaggle.com/nicapotato/pytorch-tabular-solution-francesco-pochetti
new-york-city-taxi-fare-prediction,regression,continuous-regression,"The task is to predict the fare amount for taxi rides based on various input features such as pickup and dropoff locations, passenger count, and pickup time.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set with approximately 55 million rows and a test set with about 10,000 rows. The training set includes input features such as pickup and dropoff coordinates, passenger count, and the target variable fare_amount, which represents the cost of the taxi ride. The test set contains the same input features but lacks the fare_amount, which needs to be predicted. Additionally, there is a sample submission file that demonstrates the required format for predictions.",['fare_amount'],"[""parse pickup_datetime as datetime"", ""calculate distance using haversine formula"", ""extract hour from pickup_datetime"", ""create AM/PM indicator"", ""extract weekday and day of week from pickup_datetime"", ""convert categorical columns to category type"", ""encode categorical variables as integer codes""]","model = TabularModel(embedding_size, conts.shape[1], 1, [200, 100], p=0.4)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
for i in range(epochs):
    y_pred = model(cat_train, con_train)
    loss = torch.sqrt(criterion(y_pred, y_train))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()",DL,Pytorch,elvinagammed/tabular-pytorch-90,https://www.kaggle.com/elvinagammed/tabular-pytorch-90
new-york-city-taxi-fare-prediction,regression,continuous-regression,"The task is to predict the fare amount for taxi rides based on pickup and dropoff coordinates, timestamps, and passenger count.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set with approximately 55 million rows and a test set with about 10,000 rows. The training set includes features such as pickup and dropoff coordinates, pickup datetime, and passenger count, with the target variable being the fare amount. The test set contains the same features but lacks the fare amount, which needs to be predicted. A sample submission file is also provided, which predicts a constant fare amount based on the mean fare from the training set.",['fare_amount'],"[""drop rows with missing values"", ""filter coordinates to valid ranges"", ""rotate coordinates for analysis"", ""normalize coordinates for model input""]","model = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, output_size)
)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()
model.train()
for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()",DL,Pytorch,rom1m201363/road-grid-and-backpropagated-dijkstra-with-pytorch,https://www.kaggle.com/rom1m201363/road-grid-and-backpropagated-dijkstra-with-pytorch
new-york-city-taxi-fare-prediction,regression,continuous-regression,"The task is to predict the fare amount for taxi rides based on various input features such as pickup and dropoff locations, passenger count, and pickup time.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set with approximately 55 million rows and a test set with about 10,000 rows. The training set includes features such as pickup and dropoff coordinates, passenger count, and the target variable fare_amount, which represents the cost of the taxi ride. The test set contains the same features but lacks the fare_amount, which needs to be predicted. Additionally, there is a sample submission file that demonstrates the required format for predictions.",['fare_amount'],"[""convert pickup_datetime to datetime"", ""calculate distance using haversine formula"", ""extract hour from pickup_datetime"", ""create AM/PM indicator"", ""extract weekday from pickup_datetime"", ""convert categorical variables to numeric""]","model = TabularModel(embedding_size, conts.shape[1], 1, [200, 100], p=0.4)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
for i in range(epochs):
    y_pred = model(cat_train, con_train)
    loss = torch.sqrt(criterion(y_pred, y_train))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()",DL,Pytorch,elvinagammed/nyc-fare-pytorch-tabular-model,https://www.kaggle.com/elvinagammed/nyc-fare-pytorch-tabular-model
new-york-city-taxi-fare-prediction,regression,continuous-regression,"The task is to predict the fare amount for taxi rides based on pickup and dropoff locations, passenger count, and pickup time.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of a training set with approximately 55 million rows and a test set with about 10,000 rows. The training set includes features such as pickup and dropoff longitude and latitude, passenger count, and the target variable fare_amount, which represents the cost of the taxi ride. The test set contains the same features but lacks the fare_amount, which needs to be predicted. Additionally, there is a sample submission file that demonstrates the required format for predictions.",['fare_amount'],"[""remove null data from the dataframe"", ""add new features for traveling distance"", ""remove bizarre traveling distances""]","model = nn.Sequential(nn.Linear(2, 10),
                     nn.Linear(10, 5),
                      nn.Linear(5, 1))
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
for epoch in range(90):
    y_pred = model(X)
    loss = criterion(y_pred, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
for epoch in range(700):
    y_pred = model(X)
    loss = criterion(y_pred, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()",DL,Pytorch,dienhoa/nyc-taxi-fare-neural-network-with-pytorch,https://www.kaggle.com/dienhoa/nyc-taxi-fare-neural-network-with-pytorch
pubg-finish-placement-prediction,regression,continuous-regression,Predict players' finishing placement as a percentage based on their final game stats in PUBG.,Tabular,MAE – Mean Absolute Error,"The dataset contains anonymized PUBG game statistics for players in various match types, including solos, duos, and squads. Each row represents a player's post-game stats, including metrics such as kills, assists, damage dealt, and match duration. The target variable is winPlacePerc, which indicates the player's placement as a percentage, where 1 represents first place and 0 represents last place. The dataset includes training and test sets, with a total size of approximately 965.66 MB.",['winPlacePerc'],"[""reduce memory usage by optimizing data types"", ""drop irrelevant features based on feature importance"", ""fix missing ranks in player statistics"", ""add player-level features by combining existing features"", ""create group-level features from player data"", ""normalize features based on match duration"", ""remove outliers based on predefined thresholds""]","hidden1 = tf.layers.dense(X, 100, name='hidden1', kernel_initializer=tf.variance_scaling_initializer())
hidden1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9, name='hidden1_bn')
hidden1 = tf.nn.elu(hidden1, name='hidden1_activation')
hidden2 = tf.layers.dense(hidden1, 60, name='hidden2', kernel_initializer=tf.variance_scaling_initializer())
hidden2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9, name='hidden2_bn')
hidden2 = tf.nn.elu(hidden2, name='hidden2_activation')
hidden3 = tf.layers.dense(hidden2, 60, name='hidden3', kernel_initializer=tf.variance_scaling_initializer())
hidden3 = tf.layers.batch_normalization(hidden3, training=training, momentum=0.9, name='hidden3_bn')
hidden3 = tf.nn.elu(hidden3, name='hidden3_activation')
output = tf.layers.dense(hidden3, 1, name='output', kernel_initializer=tf.variance_scaling_initializer())
loss_mse = tf.reduce_sum(tf.square(diff_vector)*group_sizes)/sum_group_sizes
training_op = optimizer.minimize(loss_hybrid)
",DL,Tensorflow,batalov/hardest-way-to-get-a-t-shirt-4th-place-solution,https://www.kaggle.com/batalov/hardest-way-to-get-a-t-shirt-4th-place-solution
pubg-finish-placement-prediction,regression,continuous-regression,Predict players' finishing placement as a percentage based on their game statistics in PUBG.,Tabular,MAE – Mean Absolute Error,"The dataset contains anonymized PUBG game statistics for players in various match types, including solos, duos, and squads. Each row represents a player's post-game stats, including metrics such as kills, damage dealt, and match duration. The target variable is winPlacePerc, which indicates the player's placement as a percentage, where 1 is first place and 0 is last place. The dataset includes training and test sets, with a total size of 965.66 MB.",['winPlacePerc'],"[""reduce memory footprint of the dataframe"", ""clean null values from the dataframe"", ""scale features using MinMaxScaler""]","model = Sequential()
model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation = swish))
model.add(Dense(round((X_train.shape[1]/3)*2), kernel_initializer='normal', activation = swish))
model.add(Dense(1, kernel_initializer='normal'))
model.compile(loss='mae', optimizer='adam', metrics=['mae'])
model.fit(x=X_train, y=y_train, batch_size=96, epochs=1, verbose=0, validation_split=0.2, shuffle=True)",DL,Tensorflow,mgiraygokirmak/keras-tensorflow-approach,https://www.kaggle.com/mgiraygokirmak/keras-tensorflow-approach
pubg-finish-placement-prediction,regression,continuous-regression,Predict players' finishing placement as a percentage based on their final game stats in PUBG.,Tabular,MAE – Mean Absolute Error,"The dataset contains anonymized PUBG game statistics for players in various match types, including solos, duos, and squads. Each row represents a player's post-game stats, including metrics such as kills, assists, healing items used, and match duration. The target variable is winPlacePerc, which indicates the player's placement in the match on a scale from 0 (last place) to 1 (first place).",['winPlacePerc'],"[""select relevant features"", ""normalize numerical features""]","my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001)
my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
linear_regressor = tf.estimator.LinearClassifier(feature_columns=feature_columns, optimizer=my_optimizer)
_ = linear_regressor.train(input_fn = lambda:my_input_fn(my_feature, targets), steps=5000)",DL,Tensorflow,joocheol/q20181124,https://www.kaggle.com/joocheol/q20181124
pubg-finish-placement-prediction,regression,continuous-regression,Predict players' finishing placement as a percentage based on their final game stats in PUBG.,Tabular,MAE – Mean Absolute Error,"The dataset contains anonymized PUBG game statistics for players in various match types, including solos, duos, and squads. Each row represents a player's post-game stats, including metrics such as kills, assists, damage dealt, and match duration. The target variable is winPlacePerc, which indicates the player's placement in the match on a scale from 0 (last place) to 1 (first place). The dataset includes training and test sets, with features that capture player performance and match context.",['winPlacePerc'],"[""calculate total distance traveled by summing swim, walk, and ride distances"", ""replace match type strings with numerical values"", ""reduce memory usage by optimizing data types"", ""group by matchId to calculate mean features"", ""group by matchId and groupId to calculate mean and size features""]","model = MLP(layer_size=[train_data_df.shape[1], 32, 32, 32, 1], regularization=1, output_shrink=0.1, output_range=[0,1], loss_type=""hardmse"")
model.train(scaler.transform(features.values), targets.values, iteration_log=20000, rate_init=0.08, rate_decay=0.8, epoch_train=epoch_train, epoch_decay=1)",DL,Tensorflow,tarunpaparaju/pubg-placement-prediction-nn-regression,https://www.kaggle.com/tarunpaparaju/pubg-placement-prediction-nn-regression
pubg-finish-placement-prediction,regression,continuous-regression,Predict players' finishing placement as a percentage based on their game statistics in PUBG.,Tabular,MAE – Mean Absolute Error,"The dataset contains anonymized statistics from PUBG matches, where each row represents a player's post-game stats. Players can be part of teams, and their placement is ranked based on how many other teams are still alive when they are eliminated. The data includes various features such as the number of kills, damage dealt, and match duration, among others. The target variable is winPlacePerc, which indicates the player's placement as a percentage from 0 (last place) to 1 (first place).",['winPlacePerc'],"[""drop rows with illegal matches"", ""drop unnecessary columns"", ""normalize features""]","model = tf.keras.Sequential([
    tf.keras.layers.Dense(300, activation=""relu"", name=""Input_layer""),
    tf.keras.layers.Dense(100, activation=""relu""),
    tf.keras.layers.Dense(100, activation=""relu""),
    tf.keras.layers.Dense(80, activation=""relu""),
    tf.keras.layers.Dense(40, activation=""relu""),
    tf.keras.layers.Dense(30, activation=""relu""),
    tf.keras.layers.Dense(20, activation=""relu""),
    tf.keras.layers.Dense(1, activation=""sigmoid"")
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss='mse',
    metrics=['MSE']
)
history  = model.fit(X_train,y_train,epochs = 200)",DL,Tensorflow,pontiacboy/pubg-competition,https://www.kaggle.com/pontiacboy/pubg-competition
pubg-finish-placement-prediction,regression,continuous-regression,Predict players' finishing placement as a percentage based on their final game stats in PUBG.,Tabular,MAE – Mean Absolute Error,"The dataset contains anonymized PUBG game statistics for players in various match types, including solos, duos, and squads. Each row represents a player's post-game stats, including metrics such as kills, damage dealt, and match duration. The target variable is winPlacePerc, which indicates the player's placement as a percentage, where 1 represents first place and 0 represents last place. The dataset includes training and test sets, with specific columns detailing player performance and match characteristics.",['winPlacePerc'],"[""merge submission with test set columns"", ""group by matchId and groupId"", ""rank players within each match"", ""calculate adjusted placement percentage"", ""handle edge cases for maxPlace""]",model.fit(),DL,Pytorch,ceshine/a-simple-post-processing-trick-lb-0237-0204,https://www.kaggle.com/ceshine/a-simple-post-processing-trick-lb-0237-0204
pubg-finish-placement-prediction,regression,continuous-regression,Predict players' finishing placement as a percentage based on their final game stats in PUBG.,Tabular,MAE – Mean Absolute Error,"The dataset contains anonymized PUBG game statistics for players in various match types, including solos, duos, and squads. Each row represents a player's post-game stats, including metrics such as kills, damage dealt, and distance traveled. The target variable is winPlacePerc, which indicates the player's placement as a percentage, where 1 is first place and 0 is last place. The dataset includes training and test sets, with features that describe player performance and match conditions.",['winPlacePerc'],"[""reduce memory usage by optimizing data types"", ""create new features based on player statistics"", ""fill missing values with zero"", ""scale features to a range of -1 to 1""]","self.model = nn.Sequential(
            weight_norm(nn.Linear(num_features, 128)),
            nn.ReLU(),
            weight_norm(nn.Linear(128, 128)),
            nn.ReLU(),
            weight_norm(nn.Linear(128, 128)),
            nn.ReLU(),
            weight_norm(nn.Linear(128, 128)),
            nn.ReLU(),
            weight_norm(nn.Linear(128, 1)),
        )

model.compile(optimizer=optimizer, loss='mean_absolute_error')
model.fit(train_loader, epochs=20)",DL,Pytorch,ceshine/pytorch-baseline-model,https://www.kaggle.com/ceshine/pytorch-baseline-model
pubg-finish-placement-prediction,regression,continuous-regression,Predict players' finishing placement as a percentage based on their final game stats in PUBG.,Tabular,MAE – Mean Absolute Error,"The dataset contains anonymized PUBG game statistics for players in various match types, including solos, duos, and squads. Each row represents a player's post-game stats, including metrics such as kills, damage dealt, and match duration. The target variable is winPlacePerc, which indicates the player's placement as a percentage, where 1 is first place and 0 is last place. The dataset includes training and test sets, with specific columns detailing player performance and match characteristics.",['winPlacePerc'],"[""merge submission with test set columns"", ""group by matchId and groupId"", ""rank players within each match"", ""calculate adjusted placement percentage"", ""handle edge cases for maxPlace"", ""align winPlacePerc with maxPlace""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='mean_absolute_error')
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)",DL,Pytorch,matthewa313/ensembling-post-processing,https://www.kaggle.com/matthewa313/ensembling-post-processing
pubg-finish-placement-prediction,regression,continuous-regression,Predict players' finishing placement as a percentage based on their final game stats in PUBG.,Tabular,MAE – Mean Absolute Error,"The dataset contains anonymized PUBG game statistics for players in various match types, including solos, duos, and squads. Each row represents a player's post-game stats, including metrics such as kills, damage dealt, and match duration. The target variable is winPlacePerc, which indicates the player's placement as a percentage, where 1 represents first place and 0 represents last place. The dataset includes training and test sets, with a total size of 965.66 MB.",['winPlacePerc'],"[""normalize features"", ""encode categorical variables""]","model = MLPModel(len(features)).to(DEVICE)
model.load_state_dict(torch.load(MODEL, map_location='cpu'))
model.compile(optimizer='adam', loss='mean_absolute_error')
model.fit(x_train, y_train, epochs=100, batch_size=32)",DL,Pytorch,ceshine/pubg-pytorch-model-feature-importance,https://www.kaggle.com/ceshine/pubg-pytorch-model-feature-importance
pubg-finish-placement-prediction,regression,continuous-regression,Predict players' finishing placement as a percentage based on their final game stats in PUBG.,Tabular,MAE – Mean Absolute Error,"The dataset contains anonymized PUBG game statistics for players in various match types, including solos, duos, and squads. Each row represents a player's post-game stats, including metrics such as kills, damage dealt, and match duration. The target variable is winPlacePerc, which indicates the player's placement as a percentage, where 1 represents first place and 0 represents last place. The dataset includes training and test sets, with specific columns detailing player performance and match characteristics.",['winPlacePerc'],"[""merge submission with test set columns"", ""group by matchId and groupId"", ""rank players within each match"", ""calculate adjusted placement percentage"", ""handle edge cases for maxPlace"", ""align winPlacePerc with maxPlace""]",model.fit(),DL,Pytorch,goldenbat/ensembling-post-processing,https://www.kaggle.com/goldenbat/ensembling-post-processing
bigquery-geotab-intersection-congestion,regression,quantile-regression,"The task is to predict the 20th, 50th, and 80th percentiles for total time stopped at intersections and the distance to the first stop for commercial vehicles based on aggregated trip logging metrics.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks, grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. Predictions are required for three different quantiles of two metrics: total time stopped at an intersection and distance to the first stop. The training set includes an optional output metric for additional modeling insights.","['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']","[""median-impute missing values"", ""normalize numerical features"", ""one-hot encode categorical variables""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(6, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)",DL,Tensorflow,sirtorry/bigquery-ml-template-intersection-congestion,https://www.kaggle.com/sirtorry/bigquery-ml-template-intersection-congestion
bigquery-geotab-intersection-congestion,regression,quantile-regression,"The task is to predict the 20th, 50th, and 80th percentiles for total time stopped at intersections and the distance to the first stop for vehicles.",Tabular,RMSE – Root Mean Squared Error,"The dataset contains aggregated trip logging metrics from commercial vehicles, grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. It includes features such as latitude, longitude, street names, and various metrics related to vehicle stops at intersections. The training set includes additional metrics that can aid in model building, while the test set requires predictions for specific quantiles of the metrics.","['TotalTimeStopped', 'DistanceToFirstStop']","[""drop unnecessary columns"", ""map directions to numerical values"", ""encode street types"", ""standardize latitude and longitude"", ""create new features for total time stopped and distance to first stop"", ""one-hot encode city feature""]","x = keras.layers.Input(shape=[X_train.shape[1]])
fc1 = keras.layers.Dense(units=45)(x)
act1 = keras.layers.PReLU()(fc1)
bn1 = keras.layers.BatchNormalization()(act1)
dp1 = keras.layers.Dropout(0.15)(bn1)
concat1 = keras.layers.Concatenate()([x, dp1])
fc2 = keras.layers.Dense(units=60)(concat1)
act2 = keras.layers.PReLU()(fc2)
bn2 = keras.layers.BatchNormalization()(act2)
dp2 = keras.layers.Dropout(0.2)(bn2)
concat2 = keras.layers.Concatenate()([concat1, dp2])
fc3 = keras.layers.Dense(units=40)(concat2)
act3 = keras.layers.PReLU()(fc3)
bn3 = keras.layers.BatchNormalization()(act3)
dp3 = keras.layers.Dropout(0.2)(bn3)
concat3 = keras.layers.Concatenate()([concat2, dp3])
output = keras.layers.Dense(units=2, activation='softmax')(concat2)
model = keras.models.Model(inputs=[x], outputs=[output])
model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-7), loss='mse', metrics=[rmse])
model.fit(X_train, y_train, epochs=200, callbacks=[er], validation_data=[X_val, y_val], batch_size=batch_size)",DL,Tensorflow,bgmello/how-one-percentile-affect-the-others,https://www.kaggle.com/bgmello/how-one-percentile-affect-the-others
bigquery-geotab-intersection-congestion,regression,multi-output-regression,The task is to predict six target outcomes related to vehicle wait times and stop distances at intersections based on aggregated trip logging metrics.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks, grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. Predictions are required for three different quantiles of two metrics: total time stopped at an intersection and distance to the first stop, covering the 20th, 50th, and 80th percentiles. The training set includes an optional additional output metric for model building.","['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']","[""median-impute missing values"", ""one-hot encode categorical variables"", ""normalize numerical features""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(6, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=50, batch_size=32)",DL,Tensorflow,sanikamal/bqml-predict-wait-times,https://www.kaggle.com/sanikamal/bqml-predict-wait-times
bigquery-geotab-intersection-congestion,regression,multi-output-regression,The task is to predict six continuous variables representing different quantiles of total time stopped and distance to the first stop for vehicles at intersections.,Tabular,RMSE – Root Mean Squared Error,"The dataset contains aggregated trip logging metrics from commercial vehicles, grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. Predictions are required for three quantiles (20th, 50th, and 80th percentiles) of total time stopped and distance to the first stop at intersections. The training set includes an additional metric, TimeFromFirstStop, which is not present in the test set.","['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']","[""fill NaN values with -999"", ""drop unnecessary columns"", ""encode categorical variables using label encoding"", ""create new features based on existing data"", ""aggregate features by mean and count""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(6, activation='linear'))
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=100, batch_size=32)",DL,Tensorflow,ragnar123/eda-feature-engineer-and-baseline-lgbm,https://www.kaggle.com/ragnar123/eda-feature-engineer-and-baseline-lgbm
bigquery-geotab-intersection-congestion,regression,multi-output-regression,"The task is to predict six target outcomes related to vehicle congestion at intersections, specifically the 20th, 50th, and 80th percentiles for total time stopped and distance to the first stop.",Tabular,RMSE – Root Mean Squared Error,"The dataset consists of aggregated trip logging metrics from commercial vehicles, including intersection wait times and stopping distances. It is grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. The training set includes an additional output metric that can be useful for model building. Predictions are required for six different metrics, each corresponding to specific percentiles of the total time stopped and distance to the first stop.","['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']","[""drop rows with missing values"", ""create a new feature indicating if the entry and exit streets are the same"", ""combine intersection ID and city to create a unique intersection feature"", ""apply label encoding to the intersection feature"", ""one-hot encode the city feature""]","model = tf.keras.Sequential()\nmodel.add(layers.Dense(128, activation='relu', input_shape=(input_shape,)))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(6))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.fit(X_train, y_train, epochs=100, batch_size=32)",DL,Tensorflow,kefahaied/bigquery-geotab-intersection-congestion,https://www.kaggle.com/kefahaied/bigquery-geotab-intersection-congestion
bigquery-geotab-intersection-congestion,regression,quantile-regression,The task is to predict six target outcomes related to vehicle wait times and stop distances at intersections based on aggregated trip logging metrics.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of aggregated trip logging metrics from commercial vehicles, such as semi-trucks, grouped by intersection, month, hour of day, direction driven, and whether the day was a weekend. Predictions are required for three different quantiles of two metrics: total time stopped at an intersection and distance to the first stop. The training set includes an optional output metric for additional modeling.","['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']","[""fill missing values with 'MISSING' for street names"", ""map street names to numerical values"", ""calculate distance from city center"", ""remove rare months"", ""create cross-features based on street names and headings"", ""one-hot encode categorical features"", ""scale numerical features""]","model = Net(lin_input_size=lin_input_size, embs_input_size=N_BINS, embs_input_cats=embs_input_cats, lin_output_size=128, embs_output_size=64).to(device)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.00005, momentum=0.9)
model.train()
for epoch in range(max_epochs):
    # training loop
    model.eval()
    # validation loop",DL,Pytorch,davideanastasia/hybrid-pytorch-hashing-embedding-linear,https://www.kaggle.com/davideanastasia/hybrid-pytorch-hashing-embedding-linear
scrabble-player-rating,regression,continuous-regression,The task is to predict the human player ratings before a game against a bot based on gameplay data and metadata from previous games.,Tabular,RMSE – Root Mean Squared Error,"The dataset contains approximately 73,000 Scrabble games played by three bots against human opponents on Woogles.io. It includes metadata about the games, gameplay data for each turn, and final scores and ratings for players before the games. The data is split into training and testing sets, with the training set containing ratings for 1031 players and the test set containing ratings for 443 players. The dataset consists of multiple CSV files, including games, turns, and player ratings, which provide comprehensive information for predicting player ratings.",['rating'],"[""fill missing values in turn_type with 'Play'"", ""drop rows with missing values in turns_df"", ""create new features for move length and reuse"", ""aggregate gameplay data by game_id and nickname"", ""drop unnecessary features from games_df"", ""scale numerical features using StandardScaler""]","model = keras.Sequential([keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),keras.layers.Dense(64, activation='relu'),keras.layers.Dense(1)])
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, validation_split=0.2)",DL,Tensorflow,zahrizhalali/scrabble-player-rating,https://www.kaggle.com/zahrizhalali/scrabble-player-rating
scrabble-player-rating,regression,continuous-regression,Predict the human player ratings before a given game against a bot based on gameplay data.,Tabular,RMSE – Root Mean Squared Error,"The dataset includes information about approximately 73,000 Scrabble games played by three bots on Woogles.io. It contains metadata about the games, gameplay data about turns played by each player, and final scores and ratings from before a given game was played. The data is split into training and testing sets, with the training set containing ratings for 1031 players and the test set containing ratings for 443 players. The task is to predict the rating of human players in the test set before the game was played.",['rating'],"[""merging datasets on 'game_id'"", ""filling missing values for 'turn_type'"", ""dropping unused columns"", ""one-hot encoding categorical variables"", ""stratified sampling of data"", ""scaling features""]","model = Sequential([
    Dense(128, activation='relu', input_dim=x_train.shape[1]),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse', metrics=['mse'])
history = model.fit(x_train, y_train,validation_data=(x_val, y_val),epochs=80, batch_size=64, verbose=0)",DL,Tensorflow,icomert/player-rating-deep-learning,https://www.kaggle.com/icomert/player-rating-deep-learning
scrabble-player-rating,regression,continuous-regression,The task is to predict the human player ratings before a game against a bot based on gameplay data.,Tabular,RMSE – Root Mean Squared Error,"The dataset consists of approximately 73,000 Scrabble games played by three bots on Woogles.io against human opponents. It includes metadata about the games, gameplay data for each turn, and ratings for players before the games. The training set contains scores and ratings for 1,031 players, while the test set includes 443 players. The data is structured in multiple CSV files, including game metadata, gameplay turns, and player ratings.",['rating'],"[""standardize features using StandardScaler"", ""split data into training and validation sets""]","model = Sequential([Dense(16, activation='relu', kernel_regularizer=l2(0.01)),Dense(8, activation='relu', kernel_regularizer=l2(0.01)),Dense(1, activation='linear')])
model.compile(optimizer=Adam(0.1), loss=mean_squared_error)
model.fit(x_train, y_train, epochs = 50)",DL,Tensorflow,kavinsubramani/scrabble-player-rating-and-it-works,https://www.kaggle.com/kavinsubramani/scrabble-player-rating-and-it-works
scrabble-player-rating,regression,continuous-regression,Predict the human player ratings before a game against a bot based on gameplay data.,Tabular,RMSE – Root Mean Squared Error,"The dataset contains approximately 73,000 Scrabble games played by three bots on Woogles.io against human opponents. It includes metadata about the games, gameplay data for each turn, and final scores and ratings for players before the games. The data is split into training and test sets, with the training set containing ratings for 1031 players and the test set for 443 players. The goal is to predict the ratings of human players in the test set based on their gameplay data.",['rating'],"[""remove bot players from training and test sets"", ""merge player and bot data on game_id"", ""normalize score and rating columns"", ""split training data into training and validation sets"", ""drop unnecessary columns for model training"", ""convert dataframes to tensors""]","model = torch.nn.Linear(3, 1)
error = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=1e-2)
model.fit()",DL,Pytorch,elillaarcher/linear-regressor-with-pytorch,https://www.kaggle.com/elillaarcher/linear-regressor-with-pytorch
kobe-bryant-shot-selection,classification,binary-classification,The task is to predict the probability that Kobe Bryant made a field goal based on the circumstances of each shot attempt.,Tabular,Log Loss,"This dataset contains the details of every field goal attempted by Kobe Bryant during his career, including attributes such as shot type, distance, and location. The dataset has missing values for the shot_made_flag, which indicates whether the shot was successful. The goal is to predict these missing values using the available features while ensuring that training is done only on prior events to avoid leakage.",['shot_made_flag'],"[""drop unnecessary columns"", ""filter out rows with missing shot_made_flag"", ""convert categorical features to integers"", ""scale the dataset"", ""shuffle the dataset""]","model = keras.models.Sequential([keras.layers.Dense(32),keras.layers.Dropout(0.5),keras.layers.LeakyReLU(),keras.layers.Dense(2, activation='softmax'),])
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])
history = model.fit(train_data, train_labels, epochs=200, validation_data=(valid_data, valid_labels), callbacks=[my_cb])",DL,Tensorflow,chitramdasgupta/kobe-bryant-shot-analysis,https://www.kaggle.com/chitramdasgupta/kobe-bryant-shot-analysis
kobe-bryant-shot-selection,classification,binary-classification,The task is to predict the probability that Kobe Bryant made a field goal based on the location and circumstances of each shot attempted during his career.,Tabular,Log Loss,"This dataset contains the location and circumstances of every field goal attempted by Kobe Bryant during his 20-year career. The dataset includes attributes such as action type, combined shot type, game event ID, game ID, shot distance, and whether the shot was made (shot_made_flag). A portion of the shot_made_flags has been removed to create a test set for which predictions are required. The dataset is structured to avoid leakage by ensuring that training only occurs on events prior to the shots being predicted.",['shot_made_flag'],"[""convert columns to appropriate data types"", ""sample data to reduce size"", ""drop rows with missing values"", ""remove irrelevant columns"", ""create new features from existing ones"", ""bin continuous variables"", ""one-hot encode categorical variables"", ""scale features using standardization""]","classifier = Sequential()
classifier.add(Dense(units = 500, kernel_initializer = 'uniform', activation = 'relu', input_dim = 200))
classifier.add(Dense(units = 250, kernel_initializer = 'uniform', activation = 'relu'))
classifier.add(Dense(units = 125, kernel_initializer = 'uniform', activation = 'relu'))
classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
classifier.fit(X_train_scaled, y_train, batch_size = 10, epochs = 5)",DL,Tensorflow,aralnorman/neural-networks-tryouts-on-kobe,https://www.kaggle.com/aralnorman/neural-networks-tryouts-on-kobe
kobe-bryant-shot-selection,classification,binary-classification,Predict the probability that Kobe Bryant made a field goal based on shot data.,Tabular,Log Loss,"This dataset contains the location and circumstances of every field goal attempted by Kobe Bryant during his 20-year career. The task is to predict whether the basket went in, indicated by the shot_made_flag. The dataset includes attributes such as action_type, combined_shot_type, game_event_id, game_id, lat, loc_x, loc_y, lon, minutes_remaining, period, playoffs, season, seconds_remaining, shot_distance, shot_type, shot_zone_area, shot_zone_basic, shot_zone_range, team_id, team_name, game_date, matchup, opponent, and shot_id. There are 5000 missing shot_made_flags representing the test set for which predictions are required.",['shot_made_flag'],"[""remove missing values"", ""normalize shot distance"", ""one-hot encode categorical variables""]","model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32)",DL,Tensorflow,marozet/kobe-shot-exploration-with-tableau,https://www.kaggle.com/marozet/kobe-shot-exploration-with-tableau
kobe-bryant-shot-selection,classification,binary-classification,Predict the probability that Kobe Bryant made a field goal based on shot location and circumstances.,Tabular,Log Loss,"This dataset contains the location and circumstances of every field goal attempted by Kobe Bryant during his 20-year career. The task is to predict whether the basket went in, indicated by the shot_made_flag. The dataset has 5000 missing shot_made_flags, which represent the test set shots for which predictions must be submitted. The dataset includes various attributes such as action_type, combined_shot_type, game_event_id, game_id, loc_x, loc_y, minutes_remaining, period, playoffs, season, seconds_remaining, shot_distance, shot_type, shot_zone_area, shot_zone_basic, shot_zone_range, team_id, team_name, game_date, matchup, opponent, and shot_id.",['shot_made_flag'],"[""drop unnecessary columns"", ""extract seconds from period end"", ""create binary feature for last 5 seconds in period"", ""create binary feature for home play"", ""extract year and month from game date"", ""replace rare action types with 'Other'"", ""one-hot encode categorical variables""]","feature_columns = [tf.contrib.layers.real_valued_column("""", dimension=161)]
dnnc = tf.contrib.learn.DNNClassifier(
  feature_columns=feature_columns,
  hidden_units=[],
  n_classes=2)
dnnc.fit(x=X_train, y=Y_train, steps=1)",DL,Tensorflow,kevins/notebook940019c959,https://www.kaggle.com/kevins/notebook940019c959
kobe-bryant-shot-selection,classification,binary-classification,The task is to predict the probability that Kobe Bryant made a field goal based on the circumstances of each shot attempt.,Tabular,Log Loss,"This dataset contains the location and circumstances of every field goal attempted by Kobe Bryant during his 20-year career. The dataset includes attributes such as action type, shot distance, and shot zone, among others. A portion of the shot_made_flags has been removed to create a test set for which predictions are required. The dataset is structured to avoid leakage by ensuring that training only occurs on events prior to the shots being predicted.",['shot_made_flag'],"[""load data from CSV"", ""split data into training and prediction sets"", ""reshape target variable"", ""drop target variable from features""]","model = Sequential()
model.add(Dense(80, input_dim=input_dim, kernel_initializer=kernel_init, activation=activation))
model.add(Dropout(drop))
model.add(Dense(50, kernel_initializer=kernel_init, activation=activation))
model.add(Dropout(drop))
model.add(Dense(1, kernel_initializer=kernel_init, activation='sigmoid'))
model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
history = model.fit(X, Y, validation_split=0.2, verbose=1, epochs=100, batch_size=16, callbacks=[es, reduce_lr])",DL,Tensorflow,myxue4869/kobe-grid-search-hyperparameter-tuning,https://www.kaggle.com/myxue4869/kobe-grid-search-hyperparameter-tuning
